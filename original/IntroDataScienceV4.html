<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_l3gnvclgwx98-1>li:before{content:"\0025cb   "}ol.lst-kix_vgnmkrp5lbgx-7.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-7 0}.lst-kix_l3gnvclgwx98-0>li:before{content:"\0025cf   "}.lst-kix_l3gnvclgwx98-2>li:before{content:"\0025a0   "}.lst-kix_l3gnvclgwx98-3>li:before{content:"\0025cf   "}ul.lst-kix_srsp18wge5o6-1{list-style-type:none}ul.lst-kix_srsp18wge5o6-2{list-style-type:none}ul.lst-kix_srsp18wge5o6-0{list-style-type:none}.lst-kix_778s5wjh1op-5>li{counter-increment:lst-ctn-kix_778s5wjh1op-5}ul.lst-kix_srsp18wge5o6-5{list-style-type:none}ul.lst-kix_srsp18wge5o6-6{list-style-type:none}ul.lst-kix_srsp18wge5o6-3{list-style-type:none}ul.lst-kix_srsp18wge5o6-4{list-style-type:none}ul.lst-kix_h8jpq87mkbro-6{list-style-type:none}ul.lst-kix_h8jpq87mkbro-7{list-style-type:none}ul.lst-kix_srsp18wge5o6-7{list-style-type:none}ul.lst-kix_h8jpq87mkbro-8{list-style-type:none}ul.lst-kix_srsp18wge5o6-8{list-style-type:none}.lst-kix_l3gnvclgwx98-8>li:before{content:"\0025a0   "}ul.lst-kix_h8jpq87mkbro-2{list-style-type:none}ul.lst-kix_h8jpq87mkbro-3{list-style-type:none}ul.lst-kix_h8jpq87mkbro-4{list-style-type:none}.lst-kix_psog4mq2dzvp-8>li:before{content:"\0025a0   "}.lst-kix_l3gnvclgwx98-7>li:before{content:"\0025cb   "}ul.lst-kix_h8jpq87mkbro-5{list-style-type:none}.lst-kix_l3gnvclgwx98-4>li:before{content:"\0025cb   "}ul.lst-kix_h8jpq87mkbro-0{list-style-type:none}.lst-kix_psog4mq2dzvp-5>li:before{content:"\0025a0   "}.lst-kix_psog4mq2dzvp-7>li:before{content:"\0025cb   "}.lst-kix_l3gnvclgwx98-6>li:before{content:"\0025cf   "}.lst-kix_a1uldluhyw5c-2>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-2}ul.lst-kix_h8jpq87mkbro-1{list-style-type:none}.lst-kix_kkxcn2m2pfez-5>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-5}.lst-kix_io5u6hn27c2s-6>li:before{content:"\0025cf   "}.lst-kix_psog4mq2dzvp-6>li:before{content:"\0025cf   "}.lst-kix_l3gnvclgwx98-5>li:before{content:"\0025a0   "}.lst-kix_io5u6hn27c2s-4>li:before{content:"\0025cb   "}.lst-kix_io5u6hn27c2s-5>li:before{content:"\0025a0   "}.lst-kix_psog4mq2dzvp-1>li:before{content:"\0025cb   "}.lst-kix_psog4mq2dzvp-3>li:before{content:"\0025cf   "}ol.lst-kix_31p751u2s1-5.start{counter-reset:lst-ctn-kix_31p751u2s1-5 0}.lst-kix_io5u6hn27c2s-3>li:before{content:"\0025cf   "}.lst-kix_psog4mq2dzvp-0>li:before{content:"\0025cf   "}.lst-kix_psog4mq2dzvp-4>li:before{content:"\0025cb   "}.lst-kix_ciwimf4uiie7-0>li:before{content:"\0025cf   "}.lst-kix_io5u6hn27c2s-0>li:before{content:"\0025cf   "}.lst-kix_io5u6hn27c2s-1>li:before{content:"\0025cb   "}.lst-kix_ciwimf4uiie7-1>li:before{content:"\0025cb   "}.lst-kix_io5u6hn27c2s-2>li:before{content:"\0025a0   "}.lst-kix_psog4mq2dzvp-2>li:before{content:"\0025a0   "}.lst-kix_v74vdz8bmwxy-3>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-3}ol.lst-kix_wfbxhn4b3sc5-2.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-2 0}ul.lst-kix_r1iuztannmt0-3{list-style-type:none}ul.lst-kix_r1iuztannmt0-2{list-style-type:none}ul.lst-kix_r1iuztannmt0-1{list-style-type:none}ul.lst-kix_r1iuztannmt0-0{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-6.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-6 0}.lst-kix_vgnmkrp5lbgx-7>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-7,lower-latin) ". "}ul.lst-kix_r1iuztannmt0-8{list-style-type:none}.lst-kix_31p751u2s1-1>li{counter-increment:lst-ctn-kix_31p751u2s1-1}ul.lst-kix_r1iuztannmt0-7{list-style-type:none}ul.lst-kix_r1iuztannmt0-6{list-style-type:none}ul.lst-kix_r1iuztannmt0-5{list-style-type:none}.lst-kix_vgnmkrp5lbgx-4>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-4,lower-latin) ". "}.lst-kix_vgnmkrp5lbgx-8>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-8,lower-roman) ". "}ul.lst-kix_r1iuztannmt0-4{list-style-type:none}.lst-kix_vgnmkrp5lbgx-5>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-5,lower-roman) ". "}.lst-kix_vgnmkrp5lbgx-6>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-6,decimal) ". "}ul.lst-kix_g8didx5tqsy8-4{list-style-type:none}ul.lst-kix_g8didx5tqsy8-3{list-style-type:none}ul.lst-kix_g8didx5tqsy8-6{list-style-type:none}ul.lst-kix_f9h15pl7l8ma-7{list-style-type:none}ul.lst-kix_g8didx5tqsy8-5{list-style-type:none}ul.lst-kix_f9h15pl7l8ma-8{list-style-type:none}ol.lst-kix_j08enc9fhspg-1.start{counter-reset:lst-ctn-kix_j08enc9fhspg-1 0}ul.lst-kix_g8didx5tqsy8-0{list-style-type:none}.lst-kix_io5u6hn27c2s-7>li:before{content:"\0025cb   "}ul.lst-kix_g8didx5tqsy8-2{list-style-type:none}.lst-kix_vgnmkrp5lbgx-0>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-0,lower-latin) ". "}ul.lst-kix_g8didx5tqsy8-1{list-style-type:none}ol.lst-kix_31p751u2s1-0.start{counter-reset:lst-ctn-kix_31p751u2s1-0 0}ul.lst-kix_f9h15pl7l8ma-1{list-style-type:none}.lst-kix_vgnmkrp5lbgx-3>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-3,decimal) ". "}.lst-kix_io5u6hn27c2s-8>li:before{content:"\0025a0   "}ul.lst-kix_f9h15pl7l8ma-2{list-style-type:none}ul.lst-kix_f9h15pl7l8ma-0{list-style-type:none}.lst-kix_rf8s22pxlb23-2>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-2}ul.lst-kix_f9h15pl7l8ma-5{list-style-type:none}.lst-kix_vgnmkrp5lbgx-1>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-1,lower-latin) ". "}ul.lst-kix_f9h15pl7l8ma-6{list-style-type:none}ul.lst-kix_f9h15pl7l8ma-3{list-style-type:none}.lst-kix_vgnmkrp5lbgx-2>li:before{content:"" counter(lst-ctn-kix_vgnmkrp5lbgx-2,lower-roman) ". "}ul.lst-kix_f9h15pl7l8ma-4{list-style-type:none}ul.lst-kix_gfuvlenv4k2k-0{list-style-type:none}ul.lst-kix_gfuvlenv4k2k-1{list-style-type:none}ul.lst-kix_gfuvlenv4k2k-2{list-style-type:none}ul.lst-kix_gfuvlenv4k2k-3{list-style-type:none}ul.lst-kix_gfuvlenv4k2k-4{list-style-type:none}.lst-kix_2w83y07jwups-5>li:before{content:"\0025a0   "}ul.lst-kix_gfuvlenv4k2k-5{list-style-type:none}ul.lst-kix_gfuvlenv4k2k-6{list-style-type:none}.lst-kix_q8jk3tahpxqe-3>li:before{content:"\0025cf   "}.lst-kix_2w83y07jwups-6>li:before{content:"\0025cf   "}ul.lst-kix_gfuvlenv4k2k-7{list-style-type:none}ul.lst-kix_gfuvlenv4k2k-8{list-style-type:none}.lst-kix_q8jk3tahpxqe-2>li:before{content:"\0025a0   "}.lst-kix_2w83y07jwups-7>li:before{content:"\0025cb   "}.lst-kix_q8jk3tahpxqe-0>li:before{content:"\0025cf   "}ul.lst-kix_g8didx5tqsy8-8{list-style-type:none}ul.lst-kix_g8didx5tqsy8-7{list-style-type:none}ol.lst-kix_wfbxhn4b3sc5-7.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-7 0}.lst-kix_q8jk3tahpxqe-1>li:before{content:"\0025cb   "}.lst-kix_2w83y07jwups-8>li:before{content:"\0025a0   "}ul.lst-kix_i3mfsufx5qs4-8{list-style-type:none}ul.lst-kix_i3mfsufx5qs4-7{list-style-type:none}.lst-kix_h8jpq87mkbro-1>li:before{content:"\0025cb   "}ul.lst-kix_i3mfsufx5qs4-6{list-style-type:none}.lst-kix_rf8s22pxlb23-5>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-5,lower-roman) ". "}.lst-kix_rf8s22pxlb23-7>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-7,lower-latin) ". "}ul.lst-kix_i3mfsufx5qs4-5{list-style-type:none}.lst-kix_h8jpq87mkbro-3>li:before{content:"\0025cf   "}.lst-kix_u1fdsix2vcok-0>li:before{content:"\0025cf   "}ul.lst-kix_i3mfsufx5qs4-0{list-style-type:none}.lst-kix_rf8s22pxlb23-1>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-1,lower-latin) ". "}ul.lst-kix_i3mfsufx5qs4-4{list-style-type:none}ul.lst-kix_i3mfsufx5qs4-3{list-style-type:none}.lst-kix_q8jk3tahpxqe-5>li:before{content:"\0025a0   "}ul.lst-kix_i3mfsufx5qs4-2{list-style-type:none}.lst-kix_2w83y07jwups-4>li:before{content:"\0025cb   "}.lst-kix_u1fdsix2vcok-2>li:before{content:"\0025a0   "}ul.lst-kix_i3mfsufx5qs4-1{list-style-type:none}.lst-kix_dy505sosxomp-6>li{counter-increment:lst-ctn-kix_dy505sosxomp-6}ol.lst-kix_rf8s22pxlb23-8.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-8 0}ol.lst-kix_rvej8okh7pu0-7.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-7 0}.lst-kix_q8jk3tahpxqe-7>li:before{content:"\0025cb   "}.lst-kix_u1fdsix2vcok-4>li:before{content:"\0025cb   "}.lst-kix_u1fdsix2vcok-8>li:before{content:"\0025a0   "}.lst-kix_2w83y07jwups-2>li:before{content:"\0025a0   "}.lst-kix_wfbxhn4b3sc5-3>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-3}.lst-kix_u1fdsix2vcok-6>li:before{content:"\0025cf   "}.lst-kix_2w83y07jwups-0>li:before{content:"\0025cf   "}ol.lst-kix_778s5wjh1op-1.start{counter-reset:lst-ctn-kix_778s5wjh1op-1 0}ul.lst-kix_7j2lgrfbtajv-4{list-style-type:none}.lst-kix_pm404dkm1bwn-3>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-3}ul.lst-kix_7j2lgrfbtajv-5{list-style-type:none}.lst-kix_o0jblmcky2dx-4>li:before{content:"\0025cb   "}ul.lst-kix_rwyosv65luta-1{list-style-type:none}ul.lst-kix_7j2lgrfbtajv-6{list-style-type:none}ul.lst-kix_rwyosv65luta-0{list-style-type:none}ul.lst-kix_7j2lgrfbtajv-7{list-style-type:none}.lst-kix_uru4kcognknp-1>li:before{content:"\0025cb   "}ul.lst-kix_rwyosv65luta-3{list-style-type:none}ul.lst-kix_7j2lgrfbtajv-8{list-style-type:none}ul.lst-kix_rwyosv65luta-2{list-style-type:none}.lst-kix_vgnmkrp5lbgx-2>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-2}ul.lst-kix_rwyosv65luta-5{list-style-type:none}ul.lst-kix_estyhardezh6-8{list-style-type:none}ul.lst-kix_rwyosv65luta-4{list-style-type:none}ul.lst-kix_rwyosv65luta-7{list-style-type:none}ul.lst-kix_estyhardezh6-6{list-style-type:none}ul.lst-kix_rwyosv65luta-6{list-style-type:none}ul.lst-kix_estyhardezh6-7{list-style-type:none}.lst-kix_o0jblmcky2dx-8>li:before{content:"\0025a0   "}ul.lst-kix_estyhardezh6-4{list-style-type:none}ul.lst-kix_rwyosv65luta-8{list-style-type:none}ul.lst-kix_estyhardezh6-5{list-style-type:none}ul.lst-kix_estyhardezh6-2{list-style-type:none}ol.lst-kix_j08enc9fhspg-6.start{counter-reset:lst-ctn-kix_j08enc9fhspg-6 0}ul.lst-kix_estyhardezh6-3{list-style-type:none}.lst-kix_o0jblmcky2dx-6>li:before{content:"\0025cf   "}ul.lst-kix_estyhardezh6-0{list-style-type:none}ul.lst-kix_estyhardezh6-1{list-style-type:none}ol.lst-kix_v74vdz8bmwxy-8.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-8 0}.lst-kix_h8jpq87mkbro-7>li:before{content:"\0025cb   "}.lst-kix_dy505sosxomp-5>li{counter-increment:lst-ctn-kix_dy505sosxomp-5}.lst-kix_h8jpq87mkbro-5>li:before{content:"\0025a0   "}.lst-kix_rf8s22pxlb23-3>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-3,decimal) ". "}ul.lst-kix_7j2lgrfbtajv-0{list-style-type:none}ul.lst-kix_7j2lgrfbtajv-1{list-style-type:none}ul.lst-kix_7j2lgrfbtajv-2{list-style-type:none}ul.lst-kix_7j2lgrfbtajv-3{list-style-type:none}.lst-kix_rvej8okh7pu0-6>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-6,decimal) ". "}.lst-kix_rvej8okh7pu0-4>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-4,lower-latin) ". "}.lst-kix_rvej8okh7pu0-8>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-8,lower-roman) ". "}.lst-kix_estyhardezh6-3>li:before{content:"\0025cf   "}.lst-kix_estyhardezh6-7>li:before{content:"\0025cb   "}.lst-kix_ciwimf4uiie7-8>li:before{content:"\0025a0   "}.lst-kix_pm404dkm1bwn-2>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-2}ul.lst-kix_b7fcviwll54m-7{list-style-type:none}ul.lst-kix_b7fcviwll54m-8{list-style-type:none}.lst-kix_estyhardezh6-5>li:before{content:"\0025a0   "}ul.lst-kix_b7fcviwll54m-5{list-style-type:none}ul.lst-kix_b7fcviwll54m-6{list-style-type:none}.lst-kix_ciwimf4uiie7-4>li:before{content:"\0025cb   "}.lst-kix_ciwimf4uiie7-2>li:before{content:"\0025a0   "}.lst-kix_ciwimf4uiie7-6>li:before{content:"\0025cf   "}.lst-kix_estyhardezh6-1>li:before{content:"\0025cb   "}.lst-kix_a1uldluhyw5c-2>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-2,lower-roman) ". "}.lst-kix_a1uldluhyw5c-4>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-4,lower-latin) ". "}.lst-kix_a1uldluhyw5c-6>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-6,decimal) ". "}ol.lst-kix_a1uldluhyw5c-1.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-1 0}.lst-kix_rvej8okh7pu0-0>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-0,upper-latin) ". "}.lst-kix_vgnmkrp5lbgx-3>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-3}.lst-kix_rvej8okh7pu0-2>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-2,lower-roman) ". "}.lst-kix_a1uldluhyw5c-8>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-8,lower-roman) ". "}.lst-kix_a1uldluhyw5c-1>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-1,lower-latin) ". "}ol.lst-kix_778s5wjh1op-2.start{counter-reset:lst-ctn-kix_778s5wjh1op-2 0}.lst-kix_kkxcn2m2pfez-1>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-1,lower-latin) ". "}.lst-kix_rwyosv65luta-1>li:before{content:"\0025cb   "}.lst-kix_rwyosv65luta-5>li:before{content:"\0025a0   "}.lst-kix_rwyosv65luta-2>li:before{content:"\0025a0   "}.lst-kix_rwyosv65luta-6>li:before{content:"\0025cf   "}.lst-kix_kkxcn2m2pfez-2>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-2,lower-roman) ". "}.lst-kix_kkxcn2m2pfez-5>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-5,lower-roman) ". "}ol.lst-kix_rvej8okh7pu0-2.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-2 0}.lst-kix_rvej8okh7pu0-3>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-3}.lst-kix_b7fcviwll54m-0>li:before{content:"\0025cf   "}.lst-kix_aoktuh8ieiw2-1>li:before{content:"\0025cb   "}.lst-kix_aoktuh8ieiw2-2>li:before{content:"\0025a0   "}.lst-kix_uru4kcognknp-4>li:before{content:"\0025cb   "}ol.lst-kix_rf8s22pxlb23-2.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-2 0}.lst-kix_o0jblmcky2dx-1>li:before{content:"\0025cb   "}.lst-kix_uru4kcognknp-3>li:before{content:"\0025cf   "}.lst-kix_o0jblmcky2dx-2>li:before{content:"\0025a0   "}ol.lst-kix_rvej8okh7pu0-3{list-style-type:none}ol.lst-kix_rvej8okh7pu0-2{list-style-type:none}.lst-kix_b7fcviwll54m-8>li:before{content:"\0025a0   "}ol.lst-kix_rvej8okh7pu0-5{list-style-type:none}ol.lst-kix_rvej8okh7pu0-4{list-style-type:none}.lst-kix_dy505sosxomp-6>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-6,decimal) ". "}ol.lst-kix_rvej8okh7pu0-1{list-style-type:none}ol.lst-kix_rvej8okh7pu0-0{list-style-type:none}.lst-kix_bfo654b132ys-3>li:before{content:"\0025cf   "}.lst-kix_dy505sosxomp-7>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-7,lower-latin) ". "}.lst-kix_kkxcn2m2pfez-6>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-6}.lst-kix_aoktuh8ieiw2-5>li:before{content:"\0025a0   "}ol.lst-kix_rf8s22pxlb23-7.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-7 0}.lst-kix_bfo654b132ys-2>li:before{content:"\0025a0   "}.lst-kix_b7fcviwll54m-1>li:before{content:"\0025cb   "}.lst-kix_bfo654b132ys-7>li:before{content:"\0025cb   "}.lst-kix_uru4kcognknp-7>li:before{content:"\0025cb   "}ol.lst-kix_rvej8okh7pu0-3.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-3 0}.lst-kix_aoktuh8ieiw2-6>li:before{content:"\0025cf   "}.lst-kix_kkxcn2m2pfez-6>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-6,decimal) ". "}.lst-kix_uru4kcognknp-8>li:before{content:"\0025a0   "}.lst-kix_b7fcviwll54m-4>li:before{content:"\0025cb   "}ol.lst-kix_rvej8okh7pu0-7{list-style-type:none}ol.lst-kix_rvej8okh7pu0-6{list-style-type:none}.lst-kix_bfo654b132ys-6>li:before{content:"\0025cf   "}.lst-kix_b7fcviwll54m-5>li:before{content:"\0025a0   "}ol.lst-kix_rvej8okh7pu0-8{list-style-type:none}.lst-kix_hxbttwgh00gt-7>li:before{content:"\0025cb   "}.lst-kix_j08enc9fhspg-1>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-1,lower-latin) ". "}.lst-kix_rf8s22pxlb23-6>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-6,decimal) ". "}.lst-kix_h8jpq87mkbro-0>li:before{content:"\0025cf   "}.lst-kix_u1fdsix2vcok-1>li:before{content:"\0025cb   "}.lst-kix_vgnmkrp5lbgx-1>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-1}.lst-kix_rvej8okh7pu0-1>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-1}ol.lst-kix_v74vdz8bmwxy-3.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-3 0}.lst-kix_qjf83hxt4fh9-6>li:before{content:"\0025cf   "}.lst-kix_q8jk3tahpxqe-6>li:before{content:"\0025cf   "}.lst-kix_7j2lgrfbtajv-1>li:before{content:"\0025cb   "}ol.lst-kix_kkxcn2m2pfez-1.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-1 0}ul.lst-kix_62mkmw7g756i-1{list-style-type:none}.lst-kix_2w83y07jwups-1>li:before{content:"\0025cb   "}ul.lst-kix_62mkmw7g756i-0{list-style-type:none}ul.lst-kix_62mkmw7g756i-3{list-style-type:none}ul.lst-kix_62mkmw7g756i-2{list-style-type:none}ul.lst-kix_62mkmw7g756i-5{list-style-type:none}.lst-kix_j08enc9fhspg-5>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-5,lower-roman) ". "}ul.lst-kix_62mkmw7g756i-4{list-style-type:none}.lst-kix_pm404dkm1bwn-4>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-4}ul.lst-kix_62mkmw7g756i-7{list-style-type:none}ul.lst-kix_62mkmw7g756i-6{list-style-type:none}.lst-kix_u1fdsix2vcok-5>li:before{content:"\0025a0   "}ul.lst-kix_62mkmw7g756i-8{list-style-type:none}.lst-kix_qjf83hxt4fh9-2>li:before{content:"\0025a0   "}.lst-kix_o0jblmcky2dx-5>li:before{content:"\0025a0   "}.lst-kix_dy505sosxomp-3>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-3,decimal) ". "}.lst-kix_uru4kcognknp-0>li:before{content:"\0025cf   "}ol.lst-kix_rf8s22pxlb23-3.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-3 0}.lst-kix_h8jpq87mkbro-8>li:before{content:"\0025a0   "}.lst-kix_31p751u2s1-8>li{counter-increment:lst-ctn-kix_31p751u2s1-8}.lst-kix_hxbttwgh00gt-3>li:before{content:"\0025cf   "}.lst-kix_rf8s22pxlb23-2>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-2,lower-roman) ". "}.lst-kix_h8jpq87mkbro-4>li:before{content:"\0025cb   "}.lst-kix_7j2lgrfbtajv-5>li:before{content:"\0025a0   "}.lst-kix_j08enc9fhspg-0>li{counter-increment:lst-ctn-kix_j08enc9fhspg-0}.lst-kix_wfbxhn4b3sc5-1>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-1}.lst-kix_rvej8okh7pu0-5>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-5,lower-roman) ". "}.lst-kix_pm404dkm1bwn-1>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-1,lower-latin) ". "}ol.lst-kix_vgnmkrp5lbgx-2.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-2 0}.lst-kix_j08enc9fhspg-7>li{counter-increment:lst-ctn-kix_j08enc9fhspg-7}ol.lst-kix_kkxcn2m2pfez-2.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-2 0}.lst-kix_estyhardezh6-6>li:before{content:"\0025cf   "}.lst-kix_ciwimf4uiie7-5>li:before{content:"\0025a0   "}ol.lst-kix_778s5wjh1op-6.start{counter-reset:lst-ctn-kix_778s5wjh1op-6 0}.lst-kix_pm404dkm1bwn-5>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-5,lower-roman) ". "}.lst-kix_estyhardezh6-2>li:before{content:"\0025a0   "}ol.lst-kix_vgnmkrp5lbgx-0{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-1{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-2{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-3{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-4{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-5{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-6{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-7{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-8{list-style-type:none}.lst-kix_hdtt0qm0bdu2-7>li:before{content:"\0025cb   "}ol.lst-kix_v74vdz8bmwxy-4.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-4 0}ul.lst-kix_2w83y07jwups-6{list-style-type:none}ul.lst-kix_2w83y07jwups-7{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-3.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-3 0}ul.lst-kix_2w83y07jwups-8{list-style-type:none}ul.lst-kix_2w83y07jwups-2{list-style-type:none}ul.lst-kix_2w83y07jwups-3{list-style-type:none}.lst-kix_a1uldluhyw5c-5>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-5,lower-roman) ". "}ul.lst-kix_2w83y07jwups-4{list-style-type:none}ol.lst-kix_778s5wjh1op-7.start{counter-reset:lst-ctn-kix_778s5wjh1op-7 0}ul.lst-kix_2w83y07jwups-5{list-style-type:none}.lst-kix_hdtt0qm0bdu2-3>li:before{content:"\0025cf   "}ul.lst-kix_2w83y07jwups-0{list-style-type:none}ul.lst-kix_2w83y07jwups-1{list-style-type:none}.lst-kix_rvej8okh7pu0-1>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-1,lower-latin) ". "}ul.lst-kix_vyyozwbuipsu-1{list-style-type:none}ul.lst-kix_vyyozwbuipsu-2{list-style-type:none}.lst-kix_ow8lx53hrwrw-5>li:before{content:"\0025a0   "}.lst-kix_31p751u2s1-3>li{counter-increment:lst-ctn-kix_31p751u2s1-3}ul.lst-kix_vyyozwbuipsu-0{list-style-type:none}.lst-kix_vyyozwbuipsu-4>li:before{content:"\0025cb   "}.lst-kix_8femmdvqkxfu-8>li:before{content:"\0025a0   "}ul.lst-kix_vyyozwbuipsu-5{list-style-type:none}ul.lst-kix_vyyozwbuipsu-6{list-style-type:none}ul.lst-kix_vyyozwbuipsu-3{list-style-type:none}ul.lst-kix_vyyozwbuipsu-4{list-style-type:none}.lst-kix_vyyozwbuipsu-7>li:before{content:"\0025cb   "}.lst-kix_ow8lx53hrwrw-0>li:before{content:"\0025cf   "}.lst-kix_ow8lx53hrwrw-8>li:before{content:"\0025a0   "}ul.lst-kix_vyyozwbuipsu-7{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-0{list-style-type:none}ul.lst-kix_vyyozwbuipsu-8{list-style-type:none}.lst-kix_i3mfsufx5qs4-8>li:before{content:"\0025a0   "}.lst-kix_ow8lx53hrwrw-2>li:before{content:"\0025a0   "}.lst-kix_uccty6vpnn5d-6>li:before{content:"\0025cf   "}.lst-kix_vyyozwbuipsu-5>li:before{content:"\0025a0   "}.lst-kix_uccty6vpnn5d-1>li:before{content:"\0025cb   "}.lst-kix_8femmdvqkxfu-2>li:before{content:"\0025a0   "}.lst-kix_uccty6vpnn5d-3>li:before{content:"\0025cf   "}.lst-kix_uccty6vpnn5d-4>li:before{content:"\0025cb   "}.lst-kix_8femmdvqkxfu-5>li:before{content:"\0025a0   "}.lst-kix_v74vdz8bmwxy-1>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-1}.lst-kix_ow8lx53hrwrw-7>li:before{content:"\0025cb   "}.lst-kix_8femmdvqkxfu-3>li:before{content:"\0025cf   "}.lst-kix_wfbxhn4b3sc5-1>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-1,lower-latin) ". "}.lst-kix_wfbxhn4b3sc5-2>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-2,lower-roman) ". "}.lst-kix_i3mfsufx5qs4-0>li:before{content:"\0025cf   "}.lst-kix_i3mfsufx5qs4-1>li:before{content:"\0025cb   "}.lst-kix_i3mfsufx5qs4-6>li:before{content:"\0025cf   "}ol.lst-kix_v74vdz8bmwxy-2.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-2 0}.lst-kix_i3mfsufx5qs4-3>li:before{content:"\0025cf   "}.lst-kix_vyyozwbuipsu-2>li:before{content:"\0025a0   "}.lst-kix_ourtwwltf86-2>li:before{content:"\0025a0   "}.lst-kix_a1uldluhyw5c-0>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-0}.lst-kix_ourtwwltf86-5>li:before{content:"\0025a0   "}.lst-kix_ourtwwltf86-7>li:before{content:"\0025cb   "}.lst-kix_vgnmkrp5lbgx-4>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-4}.lst-kix_4koh223rnoq-0>li:before{content:"\0025cf   "}.lst-kix_ourtwwltf86-8>li:before{content:"\0025a0   "}.lst-kix_31p751u2s1-7>li{counter-increment:lst-ctn-kix_31p751u2s1-7}ol.lst-kix_31p751u2s1-6.start{counter-reset:lst-ctn-kix_31p751u2s1-6 0}.lst-kix_wfbxhn4b3sc5-4>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-4,lower-latin) ". "}.lst-kix_pm404dkm1bwn-1>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-1}ol.lst-kix_wfbxhn4b3sc5-1.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-1 0}.lst-kix_wfbxhn4b3sc5-8>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-8}ol.lst-kix_kkxcn2m2pfez-5.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-5 0}ol.lst-kix_vgnmkrp5lbgx-6.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-6 0}.lst-kix_wfbxhn4b3sc5-7>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-7,lower-latin) ". "}.lst-kix_kkxcn2m2pfez-3>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-3}.lst-kix_8femmdvqkxfu-0>li:before{content:"\0025cf   "}ol.lst-kix_dy505sosxomp-2.start{counter-reset:lst-ctn-kix_dy505sosxomp-2 0}ul.lst-kix_2j5yuzkhoae5-6{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-5{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-8{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-7{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-2{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-1{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-4{list-style-type:none}ul.lst-kix_2j5yuzkhoae5-3{list-style-type:none}.lst-kix_j08enc9fhspg-2>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-2,lower-roman) ". "}ol.lst-kix_v74vdz8bmwxy-0.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-0 0}.lst-kix_hxbttwgh00gt-8>li:before{content:"\0025a0   "}.lst-kix_qjf83hxt4fh9-7>li:before{content:"\0025cb   "}ol.lst-kix_31p751u2s1-4.start{counter-reset:lst-ctn-kix_31p751u2s1-4 0}.lst-kix_v74vdz8bmwxy-8>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-8}.lst-kix_v74vdz8bmwxy-4>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-4,lower-latin) ". "}.lst-kix_7j2lgrfbtajv-2>li:before{content:"\0025a0   "}.lst-kix_hdtt0qm0bdu2-0>li:before{content:"\0025cf   "}.lst-kix_v74vdz8bmwxy-6>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-6,decimal) ". "}ol.lst-kix_kkxcn2m2pfez-7.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-7 0}.lst-kix_j08enc9fhspg-4>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-4,lower-latin) ". "}.lst-kix_qjf83hxt4fh9-1>li:before{content:"\0025cb   "}ol.lst-kix_31p751u2s1-1.start{counter-reset:lst-ctn-kix_31p751u2s1-1 0}ol.lst-kix_a1uldluhyw5c-2{list-style-type:none}ul.lst-kix_dhe74h5eopku-4{list-style-type:none}ol.lst-kix_a1uldluhyw5c-1{list-style-type:none}ul.lst-kix_dhe74h5eopku-5{list-style-type:none}ol.lst-kix_a1uldluhyw5c-4{list-style-type:none}ul.lst-kix_dhe74h5eopku-6{list-style-type:none}.lst-kix_4koh223rnoq-8>li:before{content:"\0025a0   "}ol.lst-kix_a1uldluhyw5c-3{list-style-type:none}ul.lst-kix_dhe74h5eopku-7{list-style-type:none}ol.lst-kix_a1uldluhyw5c-6{list-style-type:none}ul.lst-kix_dhe74h5eopku-8{list-style-type:none}ol.lst-kix_a1uldluhyw5c-5{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-8.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-8 0}ol.lst-kix_a1uldluhyw5c-8{list-style-type:none}ol.lst-kix_wfbxhn4b3sc5-8.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-8 0}ol.lst-kix_a1uldluhyw5c-7{list-style-type:none}ul.lst-kix_dhe74h5eopku-0{list-style-type:none}ul.lst-kix_dhe74h5eopku-1{list-style-type:none}.lst-kix_r1iuztannmt0-4>li:before{content:"\0025cb   "}ol.lst-kix_a1uldluhyw5c-0{list-style-type:none}ul.lst-kix_dhe74h5eopku-2{list-style-type:none}ul.lst-kix_dhe74h5eopku-3{list-style-type:none}ul.lst-kix_bfo654b132ys-0{list-style-type:none}ul.lst-kix_bfo654b132ys-1{list-style-type:none}ul.lst-kix_bfo654b132ys-2{list-style-type:none}.lst-kix_7j2lgrfbtajv-4>li:before{content:"\0025cb   "}ul.lst-kix_bfo654b132ys-3{list-style-type:none}ul.lst-kix_bfo654b132ys-4{list-style-type:none}.lst-kix_hxbttwgh00gt-0>li:before{content:"\0025cf   "}ul.lst-kix_bfo654b132ys-5{list-style-type:none}.lst-kix_r1iuztannmt0-6>li:before{content:"\0025cf   "}ul.lst-kix_bfo654b132ys-6{list-style-type:none}ul.lst-kix_bfo654b132ys-7{list-style-type:none}ul.lst-kix_bfo654b132ys-8{list-style-type:none}.lst-kix_4koh223rnoq-6>li:before{content:"\0025cf   "}.lst-kix_hxbttwgh00gt-2>li:before{content:"\0025a0   "}.lst-kix_pm404dkm1bwn-0>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-0,decimal) ". "}.lst-kix_63ny8vky5bcr-3>li:before{content:"\0025cf   "}.lst-kix_ce2poqyf7yx2-2>li:before{content:"\0025a0   "}.lst-kix_31p751u2s1-0>li{counter-increment:lst-ctn-kix_31p751u2s1-0}.lst-kix_ce2poqyf7yx2-8>li:before{content:"\0025a0   "}.lst-kix_63ny8vky5bcr-5>li:before{content:"\0025a0   "}.lst-kix_rvej8okh7pu0-4>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-4}.lst-kix_ce2poqyf7yx2-0>li:before{content:"\0025cf   "}.lst-kix_pm404dkm1bwn-8>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-8,lower-roman) ". "}ul.lst-kix_ctb1u95sakhd-7{list-style-type:none}ul.lst-kix_ctb1u95sakhd-8{list-style-type:none}.lst-kix_pm404dkm1bwn-8>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-8}ul.lst-kix_ctb1u95sakhd-5{list-style-type:none}.lst-kix_dhe74h5eopku-2>li:before{content:"\0025a0   "}.lst-kix_dhe74h5eopku-4>li:before{content:"\0025cb   "}ul.lst-kix_ctb1u95sakhd-6{list-style-type:none}ul.lst-kix_ctb1u95sakhd-3{list-style-type:none}ul.lst-kix_ctb1u95sakhd-4{list-style-type:none}ul.lst-kix_ctb1u95sakhd-1{list-style-type:none}ul.lst-kix_ctb1u95sakhd-2{list-style-type:none}ul.lst-kix_ctb1u95sakhd-0{list-style-type:none}.lst-kix_pm404dkm1bwn-6>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-6,decimal) ". "}.lst-kix_hdtt0qm0bdu2-8>li:before{content:"\0025a0   "}.lst-kix_2wuboluazjjj-1>li:before{content:"\0025cb   "}.lst-kix_6x6q6459hv9k-8>li:before{content:"\0025a0   "}.lst-kix_hdtt0qm0bdu2-2>li:before{content:"\0025a0   "}.lst-kix_6x6q6459hv9k-2>li:before{content:"\0025a0   "}.lst-kix_kkxcn2m2pfez-4>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-4}.lst-kix_rf8s22pxlb23-1>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-1}.lst-kix_rvej8okh7pu0-7>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-7}.lst-kix_6x6q6459hv9k-5>li:before{content:"\0025a0   "}.lst-kix_a1uldluhyw5c-3>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-3}ol.lst-kix_dy505sosxomp-6.start{counter-reset:lst-ctn-kix_dy505sosxomp-6 0}.lst-kix_31p751u2s1-4>li{counter-increment:lst-ctn-kix_31p751u2s1-4}.lst-kix_778s5wjh1op-4>li{counter-increment:lst-ctn-kix_778s5wjh1op-4}.lst-kix_2wuboluazjjj-4>li:before{content:"\0025cb   "}.lst-kix_j08enc9fhspg-6>li{counter-increment:lst-ctn-kix_j08enc9fhspg-6}.lst-kix_g8didx5tqsy8-1>li:before{content:"\0025cb   "}.lst-kix_ctb1u95sakhd-2>li:before{content:"\0025a0   "}.lst-kix_srsp18wge5o6-6>li:before{content:"\0025cf   "}ul.lst-kix_b7fcviwll54m-3{list-style-type:none}ul.lst-kix_b7fcviwll54m-4{list-style-type:none}ol.lst-kix_31p751u2s1-8.start{counter-reset:lst-ctn-kix_31p751u2s1-8 0}.lst-kix_2wuboluazjjj-7>li:before{content:"\0025cb   "}ul.lst-kix_b7fcviwll54m-1{list-style-type:none}ol.lst-kix_dy505sosxomp-0.start{counter-reset:lst-ctn-kix_dy505sosxomp-0 0}ul.lst-kix_b7fcviwll54m-2{list-style-type:none}ul.lst-kix_b7fcviwll54m-0{list-style-type:none}.lst-kix_vgnmkrp5lbgx-7>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-7}ul.lst-kix_1ikdj3ttsm27-2{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-3{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-0{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-1{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-6{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-7{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-4{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-5{list-style-type:none}ol.lst-kix_dy505sosxomp-7{list-style-type:none}ul.lst-kix_1ikdj3ttsm27-8{list-style-type:none}ol.lst-kix_dy505sosxomp-8{list-style-type:none}ol.lst-kix_dy505sosxomp-5{list-style-type:none}ul.lst-kix_uru4kcognknp-7{list-style-type:none}ol.lst-kix_dy505sosxomp-6{list-style-type:none}.lst-kix_4ug2duo9ze3p-2>li:before{content:"\0025a0   "}ul.lst-kix_uru4kcognknp-6{list-style-type:none}ol.lst-kix_dy505sosxomp-3{list-style-type:none}ol.lst-kix_dy505sosxomp-4{list-style-type:none}.lst-kix_778s5wjh1op-8>li{counter-increment:lst-ctn-kix_778s5wjh1op-8}ul.lst-kix_uru4kcognknp-8{list-style-type:none}ol.lst-kix_dy505sosxomp-1{list-style-type:none}.lst-kix_778s5wjh1op-7>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-7,lower-latin) ". "}ol.lst-kix_dy505sosxomp-2{list-style-type:none}ol.lst-kix_dy505sosxomp-1.start{counter-reset:lst-ctn-kix_dy505sosxomp-1 0}ol.lst-kix_dy505sosxomp-0{list-style-type:none}.lst-kix_rwyosv65luta-4>li:before{content:"\0025cb   "}ul.lst-kix_io5u6hn27c2s-8{list-style-type:none}.lst-kix_kkxcn2m2pfez-4>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-4,lower-latin) ". "}ul.lst-kix_io5u6hn27c2s-6{list-style-type:none}ul.lst-kix_io5u6hn27c2s-7{list-style-type:none}.lst-kix_ctb1u95sakhd-7>li:before{content:"\0025cb   "}ul.lst-kix_io5u6hn27c2s-4{list-style-type:none}.lst-kix_4ug2duo9ze3p-7>li:before{content:"\0025cb   "}ul.lst-kix_io5u6hn27c2s-5{list-style-type:none}.lst-kix_aoktuh8ieiw2-3>li:before{content:"\0025cf   "}ul.lst-kix_io5u6hn27c2s-2{list-style-type:none}ul.lst-kix_io5u6hn27c2s-3{list-style-type:none}ul.lst-kix_io5u6hn27c2s-0{list-style-type:none}ul.lst-kix_io5u6hn27c2s-1{list-style-type:none}.lst-kix_rwyosv65luta-7>li:before{content:"\0025cb   "}ul.lst-kix_uru4kcognknp-3{list-style-type:none}ul.lst-kix_uru4kcognknp-2{list-style-type:none}ul.lst-kix_uru4kcognknp-5{list-style-type:none}.lst-kix_srsp18wge5o6-3>li:before{content:"\0025cf   "}ul.lst-kix_uru4kcognknp-4{list-style-type:none}ul.lst-kix_uru4kcognknp-1{list-style-type:none}.lst-kix_aoktuh8ieiw2-0>li:before{content:"\0025cf   "}ul.lst-kix_uru4kcognknp-0{list-style-type:none}.lst-kix_dy505sosxomp-4>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-4,lower-latin) ". "}.lst-kix_bfo654b132ys-1>li:before{content:"\0025cb   "}.lst-kix_rf8s22pxlb23-5>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-5}.lst-kix_bfo654b132ys-4>li:before{content:"\0025cb   "}.lst-kix_b7fcviwll54m-3>li:before{content:"\0025cf   "}ol.lst-kix_31p751u2s1-7.start{counter-reset:lst-ctn-kix_31p751u2s1-7 0}.lst-kix_wfbxhn4b3sc5-2>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-2}.lst-kix_wfbxhn4b3sc5-5>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-5}.lst-kix_uru4kcognknp-6>li:before{content:"\0025cf   "}ol.lst-kix_v74vdz8bmwxy-0{list-style-type:none}.lst-kix_778s5wjh1op-4>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-4,lower-latin) ". "}.lst-kix_kkxcn2m2pfez-7>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-7,lower-latin) ". "}ol.lst-kix_v74vdz8bmwxy-7{list-style-type:none}ol.lst-kix_v74vdz8bmwxy-8{list-style-type:none}ol.lst-kix_v74vdz8bmwxy-5{list-style-type:none}.lst-kix_kkxcn2m2pfez-0>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-0}ol.lst-kix_v74vdz8bmwxy-6{list-style-type:none}.lst-kix_aoktuh8ieiw2-8>li:before{content:"\0025a0   "}ol.lst-kix_v74vdz8bmwxy-3{list-style-type:none}.lst-kix_rf8s22pxlb23-8>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-8}.lst-kix_b7fcviwll54m-6>li:before{content:"\0025cf   "}ol.lst-kix_v74vdz8bmwxy-4{list-style-type:none}ol.lst-kix_v74vdz8bmwxy-1{list-style-type:none}ol.lst-kix_v74vdz8bmwxy-2{list-style-type:none}ul.lst-kix_psog4mq2dzvp-8{list-style-type:none}ul.lst-kix_psog4mq2dzvp-4{list-style-type:none}ul.lst-kix_hxbttwgh00gt-7{list-style-type:none}ul.lst-kix_psog4mq2dzvp-5{list-style-type:none}ul.lst-kix_hxbttwgh00gt-6{list-style-type:none}.lst-kix_rkgjrmjh1cjp-8>li:before{content:"\0025a0   "}.lst-kix_h8jpq87mkbro-2>li:before{content:"\0025a0   "}ul.lst-kix_psog4mq2dzvp-6{list-style-type:none}.lst-kix_qjf83hxt4fh9-4>li:before{content:"\0025cb   "}ul.lst-kix_psog4mq2dzvp-7{list-style-type:none}ul.lst-kix_hxbttwgh00gt-8{list-style-type:none}ul.lst-kix_psog4mq2dzvp-0{list-style-type:none}ul.lst-kix_hxbttwgh00gt-3{list-style-type:none}ul.lst-kix_4ug2duo9ze3p-1{list-style-type:none}ul.lst-kix_psog4mq2dzvp-1{list-style-type:none}ul.lst-kix_hxbttwgh00gt-2{list-style-type:none}ul.lst-kix_4ug2duo9ze3p-0{list-style-type:none}ul.lst-kix_psog4mq2dzvp-2{list-style-type:none}ul.lst-kix_hxbttwgh00gt-5{list-style-type:none}ul.lst-kix_psog4mq2dzvp-3{list-style-type:none}ul.lst-kix_hxbttwgh00gt-4{list-style-type:none}.lst-kix_hxbttwgh00gt-5>li:before{content:"\0025a0   "}.lst-kix_q8jk3tahpxqe-4>li:before{content:"\0025cb   "}.lst-kix_pm404dkm1bwn-5>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-5}.lst-kix_rf8s22pxlb23-0>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-0,lower-latin) ". "}.lst-kix_rf8s22pxlb23-8>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-8,lower-roman) ". "}.lst-kix_9go5hcewotk9-5>li:before{content:"\0025a0   "}ul.lst-kix_hxbttwgh00gt-1{list-style-type:none}ul.lst-kix_hxbttwgh00gt-0{list-style-type:none}.lst-kix_u1fdsix2vcok-7>li:before{content:"\0025cb   "}ul.lst-kix_4ug2duo9ze3p-8{list-style-type:none}ul.lst-kix_4ug2duo9ze3p-7{list-style-type:none}ol.lst-kix_wfbxhn4b3sc5-3.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-3 0}ul.lst-kix_4ug2duo9ze3p-6{list-style-type:none}.lst-kix_j08enc9fhspg-7>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-7,lower-latin) ". "}ul.lst-kix_4ug2duo9ze3p-5{list-style-type:none}ol.lst-kix_dy505sosxomp-7.start{counter-reset:lst-ctn-kix_dy505sosxomp-7 0}ul.lst-kix_4ug2duo9ze3p-4{list-style-type:none}.lst-kix_rkgjrmjh1cjp-0>li:before{content:"\0025cf   "}ul.lst-kix_4ug2duo9ze3p-3{list-style-type:none}ul.lst-kix_4ug2duo9ze3p-2{list-style-type:none}.lst-kix_rvej8okh7pu0-0>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-0}ol.lst-kix_wfbxhn4b3sc5-6.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-6 0}.lst-kix_dy505sosxomp-1>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-1,lower-latin) ". "}.lst-kix_31p751u2s1-1>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-1,lower-latin) ". "}.lst-kix_o0jblmcky2dx-7>li:before{content:"\0025cb   "}.lst-kix_4koh223rnoq-3>li:before{content:"\0025cf   "}.lst-kix_v74vdz8bmwxy-1>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-1,lower-latin) ". "}.lst-kix_7j2lgrfbtajv-7>li:before{content:"\0025cb   "}.lst-kix_aonucg4gowwq-6>li:before{content:"\0025cf   "}.lst-kix_2j5yuzkhoae5-1>li:before{content:"\0025cb   "}.lst-kix_kkxcn2m2pfez-7>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-7}.lst-kix_ce2poqyf7yx2-5>li:before{content:"\0025a0   "}.lst-kix_dhe74h5eopku-7>li:before{content:"\0025cb   "}ol.lst-kix_wfbxhn4b3sc5-5.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-5 0}.lst-kix_wjprrrk3451p-2>li:before{content:"\0025a0   "}.lst-kix_pm404dkm1bwn-3>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-3,decimal) ". "}.lst-kix_ciwimf4uiie7-7>li:before{content:"\0025cb   "}.lst-kix_778s5wjh1op-1>li{counter-increment:lst-ctn-kix_778s5wjh1op-1}.lst-kix_estyhardezh6-0>li:before{content:"\0025cf   "}.lst-kix_r1iuztannmt0-1>li:before{content:"\0025cb   "}.lst-kix_63ny8vky5bcr-8>li:before{content:"\0025a0   "}.lst-kix_v74vdz8bmwxy-4>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-4}ol.lst-kix_dy505sosxomp-4.start{counter-reset:lst-ctn-kix_dy505sosxomp-4 0}.lst-kix_f9h15pl7l8ma-7>li:before{content:"\0025cb   "}ul.lst-kix_u9q7i696xg3u-2{list-style-type:none}ul.lst-kix_u9q7i696xg3u-3{list-style-type:none}ul.lst-kix_u9q7i696xg3u-0{list-style-type:none}ul.lst-kix_u9q7i696xg3u-1{list-style-type:none}ul.lst-kix_u9q7i696xg3u-6{list-style-type:none}ul.lst-kix_u9q7i696xg3u-7{list-style-type:none}ul.lst-kix_u9q7i696xg3u-4{list-style-type:none}ul.lst-kix_u9q7i696xg3u-5{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_u9q7i696xg3u-8{list-style-type:none}.lst-kix_1ikdj3ttsm27-7>li:before{content:"\0025cb   "}ol.lst-kix_wfbxhn4b3sc5-4.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-4 0}.lst-kix_a1uldluhyw5c-7>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-7,lower-latin) ". "}ol.lst-kix_dy505sosxomp-5.start{counter-reset:lst-ctn-kix_dy505sosxomp-5 0}.lst-kix_63ny8vky5bcr-0>li:before{content:"\0025cf   "}.lst-kix_estyhardezh6-8>li:before{content:"\0025a0   "}.lst-kix_hdtt0qm0bdu2-5>li:before{content:"\0025a0   "}.lst-kix_rvej8okh7pu0-3>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-3,decimal) ". "}.lst-kix_vgnmkrp5lbgx-0>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-0}ol.lst-kix_wfbxhn4b3sc5-3{list-style-type:none}ol.lst-kix_dy505sosxomp-3.start{counter-reset:lst-ctn-kix_dy505sosxomp-3 0}ol.lst-kix_wfbxhn4b3sc5-2{list-style-type:none}.lst-kix_62mkmw7g756i-4>li:before{content:"\0025cb   "}.lst-kix_62mkmw7g756i-5>li:before{content:"\0025a0   "}ol.lst-kix_wfbxhn4b3sc5-1{list-style-type:none}ol.lst-kix_wfbxhn4b3sc5-0{list-style-type:none}.lst-kix_62mkmw7g756i-6>li:before{content:"\0025cf   "}.lst-kix_62mkmw7g756i-2>li:before{content:"\0025a0   "}ol.lst-kix_wfbxhn4b3sc5-8{list-style-type:none}ol.lst-kix_wfbxhn4b3sc5-7{list-style-type:none}ol.lst-kix_wfbxhn4b3sc5-6{list-style-type:none}.lst-kix_62mkmw7g756i-3>li:before{content:"\0025cf   "}ol.lst-kix_wfbxhn4b3sc5-5{list-style-type:none}ol.lst-kix_wfbxhn4b3sc5-4{list-style-type:none}ol.lst-kix_31p751u2s1-2{list-style-type:none}.lst-kix_rvej8okh7pu0-6>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-6}ol.lst-kix_31p751u2s1-1{list-style-type:none}ol.lst-kix_31p751u2s1-0{list-style-type:none}ol.lst-kix_pm404dkm1bwn-1.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-1 0}.lst-kix_62mkmw7g756i-1>li:before{content:"\0025cb   "}ul.lst-kix_aoktuh8ieiw2-0{list-style-type:none}ul.lst-kix_aoktuh8ieiw2-1{list-style-type:none}ol.lst-kix_31p751u2s1-8{list-style-type:none}ul.lst-kix_aoktuh8ieiw2-2{list-style-type:none}ol.lst-kix_31p751u2s1-7{list-style-type:none}ul.lst-kix_aoktuh8ieiw2-3{list-style-type:none}ol.lst-kix_31p751u2s1-6{list-style-type:none}ul.lst-kix_aoktuh8ieiw2-4{list-style-type:none}ol.lst-kix_31p751u2s1-5{list-style-type:none}ul.lst-kix_aoktuh8ieiw2-5{list-style-type:none}.lst-kix_62mkmw7g756i-0>li:before{content:"\0025cf   "}ol.lst-kix_31p751u2s1-4{list-style-type:none}ul.lst-kix_aoktuh8ieiw2-6{list-style-type:none}ol.lst-kix_31p751u2s1-3{list-style-type:none}ul.lst-kix_o0jblmcky2dx-1{list-style-type:none}ul.lst-kix_o0jblmcky2dx-0{list-style-type:none}.lst-kix_j08enc9fhspg-3>li{counter-increment:lst-ctn-kix_j08enc9fhspg-3}.lst-kix_gfuvlenv4k2k-7>li:before{content:"\0025cb   "}.lst-kix_gfuvlenv4k2k-8>li:before{content:"\0025a0   "}.lst-kix_dy505sosxomp-2>li{counter-increment:lst-ctn-kix_dy505sosxomp-2}.lst-kix_62mkmw7g756i-8>li:before{content:"\0025a0   "}ul.lst-kix_o0jblmcky2dx-8{list-style-type:none}.lst-kix_31p751u2s1-5>li{counter-increment:lst-ctn-kix_31p751u2s1-5}ul.lst-kix_o0jblmcky2dx-7{list-style-type:none}ul.lst-kix_o0jblmcky2dx-6{list-style-type:none}ul.lst-kix_o0jblmcky2dx-5{list-style-type:none}.lst-kix_62mkmw7g756i-7>li:before{content:"\0025cb   "}ul.lst-kix_o0jblmcky2dx-4{list-style-type:none}ul.lst-kix_o0jblmcky2dx-3{list-style-type:none}ul.lst-kix_o0jblmcky2dx-2{list-style-type:none}.lst-kix_u9q7i696xg3u-2>li:before{content:"\0025a0   "}.lst-kix_l8vz6gu7x6pv-1>li:before{content:"\0025cb   "}.lst-kix_u9q7i696xg3u-1>li:before{content:"\0025cb   "}.lst-kix_u9q7i696xg3u-3>li:before{content:"\0025cf   "}.lst-kix_aonucg4gowwq-8>li:before{content:"\0025a0   "}.lst-kix_l8vz6gu7x6pv-0>li:before{content:"\0025cf   "}.lst-kix_l8vz6gu7x6pv-2>li:before{content:"\0025a0   "}.lst-kix_a1uldluhyw5c-6>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-6}.lst-kix_u9q7i696xg3u-0>li:before{content:"\0025cf   "}.lst-kix_u9q7i696xg3u-4>li:before{content:"\0025cb   "}.lst-kix_aonucg4gowwq-7>li:before{content:"\0025cb   "}.lst-kix_l8vz6gu7x6pv-3>li:before{content:"\0025cf   "}.lst-kix_gfuvlenv4k2k-0>li:before{content:"\0025cf   "}.lst-kix_gfuvlenv4k2k-1>li:before{content:"\0025cb   "}.lst-kix_gfuvlenv4k2k-2>li:before{content:"\0025a0   "}.lst-kix_gfuvlenv4k2k-4>li:before{content:"\0025cb   "}.lst-kix_qrxoa9lchuen-8>li:before{content:"\0025a0   "}.lst-kix_gfuvlenv4k2k-3>li:before{content:"\0025cf   "}.lst-kix_2j5yuzkhoae5-7>li:before{content:"\0025cb   "}.lst-kix_gfuvlenv4k2k-6>li:before{content:"\0025cf   "}ol.lst-kix_v74vdz8bmwxy-1.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-1 0}.lst-kix_2j5yuzkhoae5-8>li:before{content:"\0025a0   "}.lst-kix_gfuvlenv4k2k-5>li:before{content:"\0025a0   "}ul.lst-kix_aoktuh8ieiw2-7{list-style-type:none}.lst-kix_qrxoa9lchuen-2>li:before{content:"\0025a0   "}ul.lst-kix_aoktuh8ieiw2-8{list-style-type:none}.lst-kix_qrxoa9lchuen-1>li:before{content:"\0025cb   "}.lst-kix_qrxoa9lchuen-3>li:before{content:"\0025cf   "}.lst-kix_qrxoa9lchuen-0>li:before{content:"\0025cf   "}.lst-kix_qrxoa9lchuen-4>li:before{content:"\0025cb   "}ol.lst-kix_v74vdz8bmwxy-6.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-6 0}ol.lst-kix_778s5wjh1op-4.start{counter-reset:lst-ctn-kix_778s5wjh1op-4 0}.lst-kix_qrxoa9lchuen-6>li:before{content:"\0025cf   "}.lst-kix_qrxoa9lchuen-5>li:before{content:"\0025a0   "}.lst-kix_qrxoa9lchuen-7>li:before{content:"\0025cb   "}.lst-kix_l8vz6gu7x6pv-8>li:before{content:"\0025a0   "}.lst-kix_u9q7i696xg3u-8>li:before{content:"\0025a0   "}.lst-kix_l8vz6gu7x6pv-7>li:before{content:"\0025cb   "}.lst-kix_u9q7i696xg3u-6>li:before{content:"\0025cf   "}.lst-kix_l8vz6gu7x6pv-5>li:before{content:"\0025a0   "}ol.lst-kix_dy505sosxomp-8.start{counter-reset:lst-ctn-kix_dy505sosxomp-8 0}.lst-kix_u9q7i696xg3u-5>li:before{content:"\0025a0   "}.lst-kix_u9q7i696xg3u-7>li:before{content:"\0025cb   "}.lst-kix_l8vz6gu7x6pv-4>li:before{content:"\0025cb   "}.lst-kix_l8vz6gu7x6pv-6>li:before{content:"\0025cf   "}ol.lst-kix_j08enc9fhspg-8.start{counter-reset:lst-ctn-kix_j08enc9fhspg-8 0}ol.lst-kix_a1uldluhyw5c-4.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-4 0}.lst-kix_9go5hcewotk9-8>li:before{content:"\0025a0   "}.lst-kix_9go5hcewotk9-4>li:before{content:"\0025cb   "}ol.lst-kix_wfbxhn4b3sc5-0.start{counter-reset:lst-ctn-kix_wfbxhn4b3sc5-0 0}.lst-kix_9go5hcewotk9-6>li:before{content:"\0025cf   "}ul.lst-kix_wjprrrk3451p-5{list-style-type:none}.lst-kix_rf8s22pxlb23-6>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-6}ul.lst-kix_wjprrrk3451p-4{list-style-type:none}ul.lst-kix_wjprrrk3451p-7{list-style-type:none}ul.lst-kix_wjprrrk3451p-6{list-style-type:none}ul.lst-kix_wjprrrk3451p-8{list-style-type:none}.lst-kix_rkgjrmjh1cjp-1>li:before{content:"\0025cb   "}.lst-kix_v74vdz8bmwxy-7>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-7}.lst-kix_v74vdz8bmwxy-6>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-6}.lst-kix_31p751u2s1-6>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-6,decimal) ". "}.lst-kix_31p751u2s1-8>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-8,lower-roman) ". "}.lst-kix_2j5yuzkhoae5-2>li:before{content:"\0025a0   "}.lst-kix_2j5yuzkhoae5-4>li:before{content:"\0025cb   "}ol.lst-kix_rf8s22pxlb23-0.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-0 0}.lst-kix_31p751u2s1-2>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-2,lower-roman) ". "}.lst-kix_2j5yuzkhoae5-6>li:before{content:"\0025cf   "}.lst-kix_rkgjrmjh1cjp-3>li:before{content:"\0025cf   "}.lst-kix_9go5hcewotk9-0>li:before{content:"\0025cf   "}ul.lst-kix_8femmdvqkxfu-5{list-style-type:none}.lst-kix_aonucg4gowwq-3>li:before{content:"\0025cf   "}ol.lst-kix_j08enc9fhspg-3.start{counter-reset:lst-ctn-kix_j08enc9fhspg-3 0}ul.lst-kix_8femmdvqkxfu-6{list-style-type:none}.lst-kix_rkgjrmjh1cjp-5>li:before{content:"\0025a0   "}ul.lst-kix_8femmdvqkxfu-7{list-style-type:none}ul.lst-kix_8femmdvqkxfu-8{list-style-type:none}ul.lst-kix_8femmdvqkxfu-1{list-style-type:none}.lst-kix_aonucg4gowwq-5>li:before{content:"\0025a0   "}ul.lst-kix_8femmdvqkxfu-2{list-style-type:none}.lst-kix_rkgjrmjh1cjp-7>li:before{content:"\0025cb   "}ul.lst-kix_8femmdvqkxfu-3{list-style-type:none}.lst-kix_31p751u2s1-4>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-4,lower-latin) ". "}ul.lst-kix_8femmdvqkxfu-4{list-style-type:none}.lst-kix_2j5yuzkhoae5-0>li:before{content:"\0025cf   "}.lst-kix_9go5hcewotk9-2>li:before{content:"\0025a0   "}ul.lst-kix_8femmdvqkxfu-0{list-style-type:none}.lst-kix_wjprrrk3451p-7>li:before{content:"\0025cb   "}ol.lst-kix_rvej8okh7pu0-5.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-5 0}ul.lst-kix_rkgjrmjh1cjp-6{list-style-type:none}ul.lst-kix_rkgjrmjh1cjp-7{list-style-type:none}.lst-kix_1ikdj3ttsm27-0>li:before{content:"\0025cf   "}ul.lst-kix_rkgjrmjh1cjp-4{list-style-type:none}ul.lst-kix_rkgjrmjh1cjp-5{list-style-type:none}ul.lst-kix_rkgjrmjh1cjp-8{list-style-type:none}.lst-kix_wjprrrk3451p-1>li:before{content:"\0025cb   "}.lst-kix_aonucg4gowwq-1>li:before{content:"\0025cb   "}.lst-kix_1ikdj3ttsm27-6>li:before{content:"\0025cf   "}.lst-kix_31p751u2s1-0>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-0,decimal) ". "}.lst-kix_g8didx5tqsy8-4>li:before{content:"\0025cb   "}.lst-kix_a1uldluhyw5c-5>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-5}.lst-kix_wjprrrk3451p-3>li:before{content:"\0025cf   "}.lst-kix_1ikdj3ttsm27-2>li:before{content:"\0025a0   "}.lst-kix_1ikdj3ttsm27-4>li:before{content:"\0025cb   "}.lst-kix_wjprrrk3451p-5>li:before{content:"\0025a0   "}ul.lst-kix_rkgjrmjh1cjp-2{list-style-type:none}ul.lst-kix_rkgjrmjh1cjp-3{list-style-type:none}.lst-kix_g8didx5tqsy8-2>li:before{content:"\0025a0   "}ul.lst-kix_rkgjrmjh1cjp-0{list-style-type:none}ul.lst-kix_rkgjrmjh1cjp-1{list-style-type:none}ul.lst-kix_4koh223rnoq-8{list-style-type:none}.lst-kix_f9h15pl7l8ma-4>li:before{content:"\0025cb   "}.lst-kix_f9h15pl7l8ma-8>li:before{content:"\0025a0   "}ul.lst-kix_wjprrrk3451p-1{list-style-type:none}ul.lst-kix_wjprrrk3451p-0{list-style-type:none}ol.lst-kix_31p751u2s1-3.start{counter-reset:lst-ctn-kix_31p751u2s1-3 0}ul.lst-kix_wjprrrk3451p-3{list-style-type:none}ul.lst-kix_wjprrrk3451p-2{list-style-type:none}.lst-kix_g8didx5tqsy8-6>li:before{content:"\0025cf   "}.lst-kix_f9h15pl7l8ma-6>li:before{content:"\0025cf   "}.lst-kix_1ikdj3ttsm27-8>li:before{content:"\0025a0   "}ul.lst-kix_l8vz6gu7x6pv-0{list-style-type:none}ul.lst-kix_l8vz6gu7x6pv-1{list-style-type:none}.lst-kix_g8didx5tqsy8-8>li:before{content:"\0025a0   "}.lst-kix_f9h15pl7l8ma-0>li:before{content:"\0025cf   "}ul.lst-kix_4koh223rnoq-7{list-style-type:none}ul.lst-kix_l8vz6gu7x6pv-6{list-style-type:none}ul.lst-kix_4koh223rnoq-6{list-style-type:none}ul.lst-kix_l8vz6gu7x6pv-7{list-style-type:none}ul.lst-kix_4koh223rnoq-5{list-style-type:none}ul.lst-kix_l8vz6gu7x6pv-8{list-style-type:none}ul.lst-kix_4koh223rnoq-4{list-style-type:none}ul.lst-kix_4koh223rnoq-3{list-style-type:none}ul.lst-kix_l8vz6gu7x6pv-2{list-style-type:none}ul.lst-kix_4koh223rnoq-2{list-style-type:none}ul.lst-kix_l8vz6gu7x6pv-3{list-style-type:none}ul.lst-kix_4koh223rnoq-1{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-8.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-8 0}ul.lst-kix_l8vz6gu7x6pv-4{list-style-type:none}ul.lst-kix_4koh223rnoq-0{list-style-type:none}.lst-kix_f9h15pl7l8ma-2>li:before{content:"\0025a0   "}ul.lst-kix_l8vz6gu7x6pv-5{list-style-type:none}.lst-kix_dy505sosxomp-1>li{counter-increment:lst-ctn-kix_dy505sosxomp-1}.lst-kix_6x6q6459hv9k-3>li:before{content:"\0025cf   "}.lst-kix_6x6q6459hv9k-0>li:before{content:"\0025cf   "}.lst-kix_6x6q6459hv9k-4>li:before{content:"\0025cb   "}.lst-kix_6x6q6459hv9k-7>li:before{content:"\0025cb   "}.lst-kix_2wuboluazjjj-2>li:before{content:"\0025a0   "}.lst-kix_ctb1u95sakhd-0>li:before{content:"\0025cf   "}.lst-kix_v74vdz8bmwxy-0>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-0}ol.lst-kix_kkxcn2m2pfez-3.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-3 0}.lst-kix_ctb1u95sakhd-1>li:before{content:"\0025cb   "}.lst-kix_g8didx5tqsy8-0>li:before{content:"\0025cf   "}.lst-kix_2wuboluazjjj-5>li:before{content:"\0025a0   "}.lst-kix_2wuboluazjjj-6>li:before{content:"\0025cf   "}ol.lst-kix_31p751u2s1-2.start{counter-reset:lst-ctn-kix_31p751u2s1-2 0}ol.lst-kix_vgnmkrp5lbgx-4.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-4 0}.lst-kix_wjprrrk3451p-0>li:before{content:"\0025cf   "}ol.lst-kix_778s5wjh1op-8.start{counter-reset:lst-ctn-kix_778s5wjh1op-8 0}ol.lst-kix_778s5wjh1op-0{list-style-type:none}ol.lst-kix_778s5wjh1op-1{list-style-type:none}ol.lst-kix_778s5wjh1op-2{list-style-type:none}.lst-kix_srsp18wge5o6-8>li:before{content:"\0025a0   "}ol.lst-kix_778s5wjh1op-3{list-style-type:none}ol.lst-kix_778s5wjh1op-4{list-style-type:none}ol.lst-kix_778s5wjh1op-5{list-style-type:none}ol.lst-kix_778s5wjh1op-6{list-style-type:none}ol.lst-kix_778s5wjh1op-7{list-style-type:none}ol.lst-kix_778s5wjh1op-8{list-style-type:none}ul.lst-kix_ciwimf4uiie7-3{list-style-type:none}.lst-kix_4ug2duo9ze3p-0>li:before{content:"\0025cf   "}.lst-kix_4ug2duo9ze3p-1>li:before{content:"\0025cb   "}ul.lst-kix_ciwimf4uiie7-4{list-style-type:none}ul.lst-kix_ciwimf4uiie7-1{list-style-type:none}ol.lst-kix_rf8s22pxlb23-6.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-6 0}ul.lst-kix_ciwimf4uiie7-2{list-style-type:none}ul.lst-kix_ciwimf4uiie7-0{list-style-type:none}ol.lst-kix_778s5wjh1op-3.start{counter-reset:lst-ctn-kix_778s5wjh1op-3 0}.lst-kix_4ug2duo9ze3p-4>li:before{content:"\0025cb   "}.lst-kix_j08enc9fhspg-2>li{counter-increment:lst-ctn-kix_j08enc9fhspg-2}.lst-kix_4ug2duo9ze3p-5>li:before{content:"\0025a0   "}ul.lst-kix_ciwimf4uiie7-7{list-style-type:none}ul.lst-kix_ciwimf4uiie7-8{list-style-type:none}ul.lst-kix_ciwimf4uiie7-5{list-style-type:none}.lst-kix_778s5wjh1op-2>li{counter-increment:lst-ctn-kix_778s5wjh1op-2}ul.lst-kix_ciwimf4uiie7-6{list-style-type:none}.lst-kix_4ug2duo9ze3p-8>li:before{content:"\0025a0   "}.lst-kix_srsp18wge5o6-5>li:before{content:"\0025a0   "}.lst-kix_ctb1u95sakhd-5>li:before{content:"\0025a0   "}.lst-kix_srsp18wge5o6-4>li:before{content:"\0025cb   "}.lst-kix_ctb1u95sakhd-4>li:before{content:"\0025cb   "}.lst-kix_srsp18wge5o6-1>li:before{content:"\0025cb   "}ol.lst-kix_vgnmkrp5lbgx-5.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-5 0}ul.lst-kix_ce2poqyf7yx2-1{list-style-type:none}ul.lst-kix_ce2poqyf7yx2-0{list-style-type:none}ul.lst-kix_ce2poqyf7yx2-3{list-style-type:none}ul.lst-kix_ce2poqyf7yx2-2{list-style-type:none}ul.lst-kix_ce2poqyf7yx2-5{list-style-type:none}ul.lst-kix_ow8lx53hrwrw-0{list-style-type:none}ul.lst-kix_ce2poqyf7yx2-4{list-style-type:none}.lst-kix_srsp18wge5o6-0>li:before{content:"\0025cf   "}ul.lst-kix_ow8lx53hrwrw-2{list-style-type:none}ul.lst-kix_ow8lx53hrwrw-1{list-style-type:none}ul.lst-kix_ow8lx53hrwrw-4{list-style-type:none}ul.lst-kix_ow8lx53hrwrw-3{list-style-type:none}.lst-kix_778s5wjh1op-1>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-1,lower-latin) ". "}ul.lst-kix_ow8lx53hrwrw-6{list-style-type:none}ul.lst-kix_ow8lx53hrwrw-5{list-style-type:none}ul.lst-kix_ow8lx53hrwrw-8{list-style-type:none}ul.lst-kix_ow8lx53hrwrw-7{list-style-type:none}.lst-kix_ctb1u95sakhd-8>li:before{content:"\0025a0   "}.lst-kix_778s5wjh1op-2>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-2,lower-roman) ". "}ul.lst-kix_ce2poqyf7yx2-7{list-style-type:none}ol.lst-kix_rf8s22pxlb23-1.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-1 0}ul.lst-kix_ce2poqyf7yx2-6{list-style-type:none}.lst-kix_778s5wjh1op-6>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-6,decimal) ". "}ul.lst-kix_ce2poqyf7yx2-8{list-style-type:none}.lst-kix_778s5wjh1op-5>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-5,lower-roman) ". "}ul.lst-kix_2wuboluazjjj-0{list-style-type:none}ul.lst-kix_aonucg4gowwq-4{list-style-type:none}ul.lst-kix_2wuboluazjjj-1{list-style-type:none}ul.lst-kix_aonucg4gowwq-3{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-4.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-4 0}.lst-kix_9go5hcewotk9-7>li:before{content:"\0025cb   "}ul.lst-kix_aonucg4gowwq-6{list-style-type:none}ul.lst-kix_aonucg4gowwq-5{list-style-type:none}ul.lst-kix_2wuboluazjjj-4{list-style-type:none}ul.lst-kix_aonucg4gowwq-8{list-style-type:none}ul.lst-kix_2wuboluazjjj-5{list-style-type:none}ul.lst-kix_aonucg4gowwq-7{list-style-type:none}ul.lst-kix_2wuboluazjjj-2{list-style-type:none}ul.lst-kix_2wuboluazjjj-3{list-style-type:none}.lst-kix_v74vdz8bmwxy-3>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-3,decimal) ". "}.lst-kix_rkgjrmjh1cjp-2>li:before{content:"\0025a0   "}ul.lst-kix_2wuboluazjjj-8{list-style-type:none}ul.lst-kix_2wuboluazjjj-6{list-style-type:none}ul.lst-kix_2wuboluazjjj-7{list-style-type:none}ol.lst-kix_vgnmkrp5lbgx-0.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-0 0}ul.lst-kix_aonucg4gowwq-0{list-style-type:none}.lst-kix_v74vdz8bmwxy-7>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-7,lower-latin) ". "}ol.lst-kix_rf8s22pxlb23-5.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-5 0}ul.lst-kix_aonucg4gowwq-2{list-style-type:none}ul.lst-kix_aonucg4gowwq-1{list-style-type:none}.lst-kix_31p751u2s1-7>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-7,lower-latin) ". "}ul.lst-kix_ourtwwltf86-4{list-style-type:none}ul.lst-kix_ourtwwltf86-3{list-style-type:none}.lst-kix_pm404dkm1bwn-6>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-6}ul.lst-kix_ourtwwltf86-2{list-style-type:none}ul.lst-kix_ourtwwltf86-1{list-style-type:none}ul.lst-kix_ourtwwltf86-0{list-style-type:none}.lst-kix_2j5yuzkhoae5-3>li:before{content:"\0025cf   "}.lst-kix_778s5wjh1op-0>li{counter-increment:lst-ctn-kix_778s5wjh1op-0}.lst-kix_31p751u2s1-3>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-3,decimal) ". "}.lst-kix_r1iuztannmt0-3>li:before{content:"\0025cf   "}ul.lst-kix_u1fdsix2vcok-4{list-style-type:none}ul.lst-kix_u1fdsix2vcok-5{list-style-type:none}ul.lst-kix_u1fdsix2vcok-2{list-style-type:none}ul.lst-kix_u1fdsix2vcok-3{list-style-type:none}ul.lst-kix_9go5hcewotk9-0{list-style-type:none}ul.lst-kix_u1fdsix2vcok-0{list-style-type:none}ul.lst-kix_9go5hcewotk9-1{list-style-type:none}ul.lst-kix_u1fdsix2vcok-1{list-style-type:none}ul.lst-kix_9go5hcewotk9-2{list-style-type:none}ul.lst-kix_9go5hcewotk9-3{list-style-type:none}.lst-kix_4koh223rnoq-5>li:before{content:"\0025a0   "}ul.lst-kix_9go5hcewotk9-4{list-style-type:none}ul.lst-kix_9go5hcewotk9-5{list-style-type:none}.lst-kix_rkgjrmjh1cjp-6>li:before{content:"\0025cf   "}.lst-kix_9go5hcewotk9-3>li:before{content:"\0025cf   "}ul.lst-kix_9go5hcewotk9-6{list-style-type:none}.lst-kix_aonucg4gowwq-4>li:before{content:"\0025cb   "}ul.lst-kix_9go5hcewotk9-7{list-style-type:none}.lst-kix_dy505sosxomp-8>li{counter-increment:lst-ctn-kix_dy505sosxomp-8}ul.lst-kix_9go5hcewotk9-8{list-style-type:none}ul.lst-kix_u1fdsix2vcok-8{list-style-type:none}.lst-kix_r1iuztannmt0-7>li:before{content:"\0025cb   "}ul.lst-kix_u1fdsix2vcok-6{list-style-type:none}ul.lst-kix_u1fdsix2vcok-7{list-style-type:none}.lst-kix_a1uldluhyw5c-7>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-7}ol.lst-kix_v74vdz8bmwxy-5.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-5 0}.lst-kix_ce2poqyf7yx2-3>li:before{content:"\0025cf   "}ul.lst-kix_hdtt0qm0bdu2-2{list-style-type:none}ul.lst-kix_hdtt0qm0bdu2-3{list-style-type:none}.lst-kix_wjprrrk3451p-8>li:before{content:"\0025a0   "}ul.lst-kix_hdtt0qm0bdu2-4{list-style-type:none}ul.lst-kix_hdtt0qm0bdu2-5{list-style-type:none}.lst-kix_1ikdj3ttsm27-1>li:before{content:"\0025cb   "}ul.lst-kix_hdtt0qm0bdu2-6{list-style-type:none}ul.lst-kix_hdtt0qm0bdu2-7{list-style-type:none}ul.lst-kix_hdtt0qm0bdu2-8{list-style-type:none}.lst-kix_ourtwwltf86-0>li:before{content:"\0025cf   "}.lst-kix_ce2poqyf7yx2-7>li:before{content:"\0025cb   "}.lst-kix_rf8s22pxlb23-4>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-4}.lst-kix_63ny8vky5bcr-6>li:before{content:"\0025cf   "}.lst-kix_kkxcn2m2pfez-1>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-1}.lst-kix_aonucg4gowwq-0>li:before{content:"\0025cf   "}.lst-kix_wfbxhn4b3sc5-6>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-6}.lst-kix_778s5wjh1op-7>li{counter-increment:lst-ctn-kix_778s5wjh1op-7}.lst-kix_1ikdj3ttsm27-5>li:before{content:"\0025a0   "}.lst-kix_dhe74h5eopku-1>li:before{content:"\0025cb   "}.lst-kix_dhe74h5eopku-5>li:before{content:"\0025a0   "}ol.lst-kix_rvej8okh7pu0-1.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-1 0}.lst-kix_g8didx5tqsy8-3>li:before{content:"\0025cf   "}.lst-kix_wjprrrk3451p-4>li:before{content:"\0025cb   "}.lst-kix_kkxcn2m2pfez-8>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-8}.lst-kix_dy505sosxomp-3>li{counter-increment:lst-ctn-kix_dy505sosxomp-3}ul.lst-kix_ourtwwltf86-8{list-style-type:none}ul.lst-kix_ourtwwltf86-7{list-style-type:none}ul.lst-kix_ourtwwltf86-6{list-style-type:none}ul.lst-kix_ourtwwltf86-5{list-style-type:none}ul.lst-kix_uccty6vpnn5d-0{list-style-type:none}ul.lst-kix_uccty6vpnn5d-1{list-style-type:none}ul.lst-kix_uccty6vpnn5d-2{list-style-type:none}ul.lst-kix_uccty6vpnn5d-3{list-style-type:none}.lst-kix_f9h15pl7l8ma-5>li:before{content:"\0025a0   "}ul.lst-kix_uccty6vpnn5d-8{list-style-type:none}ul.lst-kix_uccty6vpnn5d-4{list-style-type:none}ul.lst-kix_uccty6vpnn5d-5{list-style-type:none}ul.lst-kix_uccty6vpnn5d-6{list-style-type:none}ul.lst-kix_uccty6vpnn5d-7{list-style-type:none}.lst-kix_g8didx5tqsy8-7>li:before{content:"\0025cb   "}.lst-kix_vgnmkrp5lbgx-6>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-6}ol.lst-kix_rvej8okh7pu0-0.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-0 0}.lst-kix_f9h15pl7l8ma-1>li:before{content:"\0025cb   "}.lst-kix_63ny8vky5bcr-2>li:before{content:"\0025a0   "}ul.lst-kix_hdtt0qm0bdu2-0{list-style-type:none}ul.lst-kix_hdtt0qm0bdu2-1{list-style-type:none}.lst-kix_ow8lx53hrwrw-4>li:before{content:"\0025cb   "}ol.lst-kix_778s5wjh1op-5.start{counter-reset:lst-ctn-kix_778s5wjh1op-5 0}.lst-kix_ow8lx53hrwrw-3>li:before{content:"\0025cf   "}.lst-kix_vyyozwbuipsu-6>li:before{content:"\0025cf   "}.lst-kix_uccty6vpnn5d-7>li:before{content:"\0025cb   "}.lst-kix_uccty6vpnn5d-8>li:before{content:"\0025a0   "}.lst-kix_vyyozwbuipsu-3>li:before{content:"\0025cf   "}.lst-kix_uccty6vpnn5d-5>li:before{content:"\0025a0   "}.lst-kix_8femmdvqkxfu-6>li:before{content:"\0025cf   "}ol.lst-kix_rf8s22pxlb23-4.start{counter-reset:lst-ctn-kix_rf8s22pxlb23-4 0}ol.lst-kix_v74vdz8bmwxy-7.start{counter-reset:lst-ctn-kix_v74vdz8bmwxy-7 0}.lst-kix_ow8lx53hrwrw-1>li:before{content:"\0025cb   "}ol.lst-kix_j08enc9fhspg-2.start{counter-reset:lst-ctn-kix_j08enc9fhspg-2 0}.lst-kix_8femmdvqkxfu-7>li:before{content:"\0025cb   "}ol.lst-kix_j08enc9fhspg-1{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-0{list-style-type:none}ul.lst-kix_6x6q6459hv9k-8{list-style-type:none}ol.lst-kix_j08enc9fhspg-0{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-1{list-style-type:none}ul.lst-kix_6x6q6459hv9k-7{list-style-type:none}ol.lst-kix_j08enc9fhspg-3{list-style-type:none}ul.lst-kix_6x6q6459hv9k-6{list-style-type:none}ol.lst-kix_j08enc9fhspg-2{list-style-type:none}.lst-kix_rf8s22pxlb23-0>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-0}ul.lst-kix_6x6q6459hv9k-5{list-style-type:none}.lst-kix_8femmdvqkxfu-4>li:before{content:"\0025cb   "}ol.lst-kix_j08enc9fhspg-5{list-style-type:none}ul.lst-kix_6x6q6459hv9k-4{list-style-type:none}ol.lst-kix_j08enc9fhspg-4{list-style-type:none}ul.lst-kix_6x6q6459hv9k-3{list-style-type:none}.lst-kix_uccty6vpnn5d-0>li:before{content:"\0025cf   "}ol.lst-kix_j08enc9fhspg-7{list-style-type:none}ul.lst-kix_6x6q6459hv9k-2{list-style-type:none}.lst-kix_8femmdvqkxfu-1>li:before{content:"\0025cb   "}ol.lst-kix_j08enc9fhspg-6{list-style-type:none}ol.lst-kix_j08enc9fhspg-8{list-style-type:none}.lst-kix_dy505sosxomp-0>li{counter-increment:lst-ctn-kix_dy505sosxomp-0}.lst-kix_j08enc9fhspg-5>li{counter-increment:lst-ctn-kix_j08enc9fhspg-5}.lst-kix_vyyozwbuipsu-8>li:before{content:"\0025a0   "}.lst-kix_ow8lx53hrwrw-6>li:before{content:"\0025cf   "}.lst-kix_uccty6vpnn5d-2>li:before{content:"\0025a0   "}.lst-kix_wfbxhn4b3sc5-3>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-3,decimal) ". "}ol.lst-kix_vgnmkrp5lbgx-1.start{counter-reset:lst-ctn-kix_vgnmkrp5lbgx-1 0}ol.lst-kix_a1uldluhyw5c-5.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-5 0}ul.lst-kix_6x6q6459hv9k-1{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-8{list-style-type:none}.lst-kix_i3mfsufx5qs4-2>li:before{content:"\0025a0   "}ul.lst-kix_6x6q6459hv9k-0{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-6{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-7{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-4{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-5{list-style-type:none}.lst-kix_wfbxhn4b3sc5-0>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-0,decimal) ". "}ul.lst-kix_qjf83hxt4fh9-2{list-style-type:none}ul.lst-kix_63ny8vky5bcr-0{list-style-type:none}ul.lst-kix_qjf83hxt4fh9-3{list-style-type:none}ul.lst-kix_63ny8vky5bcr-1{list-style-type:none}ul.lst-kix_63ny8vky5bcr-2{list-style-type:none}ul.lst-kix_63ny8vky5bcr-3{list-style-type:none}ul.lst-kix_63ny8vky5bcr-4{list-style-type:none}ul.lst-kix_63ny8vky5bcr-5{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-0.start{counter-reset:lst-ctn-kix_kkxcn2m2pfez-0 0}ul.lst-kix_63ny8vky5bcr-6{list-style-type:none}ul.lst-kix_63ny8vky5bcr-7{list-style-type:none}.lst-kix_i3mfsufx5qs4-7>li:before{content:"\0025cb   "}ul.lst-kix_63ny8vky5bcr-8{list-style-type:none}.lst-kix_778s5wjh1op-3>li{counter-increment:lst-ctn-kix_778s5wjh1op-3}.lst-kix_a1uldluhyw5c-4>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-4}ol.lst-kix_kkxcn2m2pfez-8{list-style-type:none}.lst-kix_vyyozwbuipsu-0>li:before{content:"\0025cf   "}.lst-kix_i3mfsufx5qs4-4>li:before{content:"\0025cb   "}.lst-kix_rvej8okh7pu0-8>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-8}.lst-kix_i3mfsufx5qs4-5>li:before{content:"\0025a0   "}.lst-kix_vyyozwbuipsu-1>li:before{content:"\0025cb   "}.lst-kix_ourtwwltf86-3>li:before{content:"\0025cf   "}.lst-kix_dy505sosxomp-4>li{counter-increment:lst-ctn-kix_dy505sosxomp-4}.lst-kix_ourtwwltf86-4>li:before{content:"\0025cb   "}ol.lst-kix_j08enc9fhspg-7.start{counter-reset:lst-ctn-kix_j08enc9fhspg-7 0}.lst-kix_ourtwwltf86-6>li:before{content:"\0025cf   "}.lst-kix_4koh223rnoq-2>li:before{content:"\0025a0   "}.lst-kix_4koh223rnoq-1>li:before{content:"\0025cb   "}.lst-kix_vgnmkrp5lbgx-8>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-8}.lst-kix_wfbxhn4b3sc5-5>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-5,lower-roman) ". "}.lst-kix_wfbxhn4b3sc5-6>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-6,decimal) ". "}.lst-kix_wfbxhn4b3sc5-8>li:before{content:"" counter(lst-ctn-kix_wfbxhn4b3sc5-8,lower-roman) ". "}.lst-kix_v74vdz8bmwxy-5>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-5}.lst-kix_qjf83hxt4fh9-3>li:before{content:"\0025cf   "}.lst-kix_j08enc9fhspg-0>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-0,lower-latin) ". "}.lst-kix_7j2lgrfbtajv-8>li:before{content:"\0025a0   "}.lst-kix_hxbttwgh00gt-4>li:before{content:"\0025cb   "}.lst-kix_v74vdz8bmwxy-2>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-2,lower-roman) ". "}.lst-kix_qjf83hxt4fh9-5>li:before{content:"\0025a0   "}.lst-kix_hxbttwgh00gt-6>li:before{content:"\0025cf   "}.lst-kix_rf8s22pxlb23-7>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-7}.lst-kix_dy505sosxomp-7>li{counter-increment:lst-ctn-kix_dy505sosxomp-7}ol.lst-kix_rf8s22pxlb23-5{list-style-type:none}ol.lst-kix_rf8s22pxlb23-6{list-style-type:none}.lst-kix_j08enc9fhspg-8>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-8,lower-roman) ". "}ol.lst-kix_rf8s22pxlb23-7{list-style-type:none}ol.lst-kix_rf8s22pxlb23-8{list-style-type:none}.lst-kix_7j2lgrfbtajv-0>li:before{content:"\0025cf   "}ol.lst-kix_rf8s22pxlb23-1{list-style-type:none}ol.lst-kix_rf8s22pxlb23-2{list-style-type:none}ol.lst-kix_rf8s22pxlb23-3{list-style-type:none}ol.lst-kix_rf8s22pxlb23-4{list-style-type:none}.lst-kix_j08enc9fhspg-6>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-6,decimal) ". "}ol.lst-kix_rf8s22pxlb23-0{list-style-type:none}.lst-kix_v74vdz8bmwxy-8>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-8,lower-roman) ". "}ol.lst-kix_rvej8okh7pu0-4.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-4 0}.lst-kix_dy505sosxomp-2>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-2,lower-roman) ". "}ol.lst-kix_a1uldluhyw5c-3.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-3 0}.lst-kix_dy505sosxomp-0>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-0,upper-latin) ". "}.lst-kix_4koh223rnoq-4>li:before{content:"\0025cb   "}.lst-kix_7j2lgrfbtajv-6>li:before{content:"\0025cf   "}ol.lst-kix_pm404dkm1bwn-2.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-2 0}.lst-kix_wfbxhn4b3sc5-4>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-4}.lst-kix_v74vdz8bmwxy-0>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-0,decimal) ". "}.lst-kix_r1iuztannmt0-8>li:before{content:"\0025a0   "}.lst-kix_31p751u2s1-6>li{counter-increment:lst-ctn-kix_31p751u2s1-6}.lst-kix_ourtwwltf86-1>li:before{content:"\0025cb   "}.lst-kix_pm404dkm1bwn-4>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-4,lower-latin) ". "}.lst-kix_ce2poqyf7yx2-6>li:before{content:"\0025cf   "}.lst-kix_63ny8vky5bcr-7>li:before{content:"\0025cb   "}.lst-kix_dhe74h5eopku-6>li:before{content:"\0025cf   "}.lst-kix_dhe74h5eopku-8>li:before{content:"\0025a0   "}.lst-kix_pm404dkm1bwn-2>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-2,lower-roman) ". "}ol.lst-kix_j08enc9fhspg-4.start{counter-reset:lst-ctn-kix_j08enc9fhspg-4 0}ul.lst-kix_l3gnvclgwx98-1{list-style-type:none}ul.lst-kix_l3gnvclgwx98-2{list-style-type:none}.lst-kix_r1iuztannmt0-2>li:before{content:"\0025a0   "}ul.lst-kix_l3gnvclgwx98-0{list-style-type:none}ul.lst-kix_l3gnvclgwx98-5{list-style-type:none}ul.lst-kix_l3gnvclgwx98-6{list-style-type:none}ul.lst-kix_l3gnvclgwx98-3{list-style-type:none}ul.lst-kix_l3gnvclgwx98-4{list-style-type:none}ol.lst-kix_a1uldluhyw5c-0.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-0 0}ul.lst-kix_l3gnvclgwx98-7{list-style-type:none}.lst-kix_r1iuztannmt0-0>li:before{content:"\0025cf   "}ul.lst-kix_l3gnvclgwx98-8{list-style-type:none}.lst-kix_hdtt0qm0bdu2-6>li:before{content:"\0025cf   "}.lst-kix_dhe74h5eopku-0>li:before{content:"\0025cf   "}ol.lst-kix_rvej8okh7pu0-6.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-6 0}ol.lst-kix_pm404dkm1bwn-0.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-0 0}ol.lst-kix_j08enc9fhspg-5.start{counter-reset:lst-ctn-kix_j08enc9fhspg-5 0}ol.lst-kix_778s5wjh1op-0.start{counter-reset:lst-ctn-kix_778s5wjh1op-0 0}.lst-kix_hdtt0qm0bdu2-4>li:before{content:"\0025cb   "}.lst-kix_63ny8vky5bcr-1>li:before{content:"\0025cb   "}.lst-kix_ce2poqyf7yx2-4>li:before{content:"\0025cb   "}ol.lst-kix_pm404dkm1bwn-4.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-4 0}.lst-kix_6x6q6459hv9k-6>li:before{content:"\0025cf   "}ol.lst-kix_a1uldluhyw5c-2.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-2 0}.lst-kix_j08enc9fhspg-4>li{counter-increment:lst-ctn-kix_j08enc9fhspg-4}.lst-kix_2wuboluazjjj-3>li:before{content:"\0025cf   "}.lst-kix_a1uldluhyw5c-0>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-0,lower-latin) ". "}.lst-kix_6x6q6459hv9k-1>li:before{content:"\0025cb   "}.lst-kix_vgnmkrp5lbgx-5>li{counter-increment:lst-ctn-kix_vgnmkrp5lbgx-5}.lst-kix_pm404dkm1bwn-0>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-0}.lst-kix_2wuboluazjjj-8>li:before{content:"\0025a0   "}ol.lst-kix_rvej8okh7pu0-8.start{counter-reset:lst-ctn-kix_rvej8okh7pu0-8 0}.lst-kix_srsp18wge5o6-7>li:before{content:"\0025cb   "}.lst-kix_a1uldluhyw5c-1>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-1}.lst-kix_wfbxhn4b3sc5-7>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-7}.lst-kix_rvej8okh7pu0-5>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-5}.lst-kix_778s5wjh1op-6>li{counter-increment:lst-ctn-kix_778s5wjh1op-6}.lst-kix_31p751u2s1-2>li{counter-increment:lst-ctn-kix_31p751u2s1-2}.lst-kix_rwyosv65luta-3>li:before{content:"\0025cf   "}ol.lst-kix_j08enc9fhspg-0.start{counter-reset:lst-ctn-kix_j08enc9fhspg-0 0}.lst-kix_pm404dkm1bwn-7>li{counter-increment:lst-ctn-kix_pm404dkm1bwn-7}ul.lst-kix_q8jk3tahpxqe-0{list-style-type:none}.lst-kix_778s5wjh1op-8>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-8,lower-roman) ". "}.lst-kix_4ug2duo9ze3p-3>li:before{content:"\0025cf   "}.lst-kix_kkxcn2m2pfez-3>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-3,decimal) ". "}ul.lst-kix_q8jk3tahpxqe-2{list-style-type:none}ul.lst-kix_q8jk3tahpxqe-1{list-style-type:none}ul.lst-kix_q8jk3tahpxqe-4{list-style-type:none}ul.lst-kix_q8jk3tahpxqe-3{list-style-type:none}ul.lst-kix_q8jk3tahpxqe-6{list-style-type:none}ul.lst-kix_q8jk3tahpxqe-5{list-style-type:none}ul.lst-kix_q8jk3tahpxqe-8{list-style-type:none}ul.lst-kix_q8jk3tahpxqe-7{list-style-type:none}.lst-kix_4ug2duo9ze3p-6>li:before{content:"\0025cf   "}.lst-kix_ctb1u95sakhd-6>li:before{content:"\0025cf   "}.lst-kix_ctb1u95sakhd-3>li:before{content:"\0025cf   "}.lst-kix_j08enc9fhspg-8>li{counter-increment:lst-ctn-kix_j08enc9fhspg-8}.lst-kix_o0jblmcky2dx-3>li:before{content:"\0025cf   "}.lst-kix_rwyosv65luta-8>li:before{content:"\0025a0   "}.lst-kix_srsp18wge5o6-2>li:before{content:"\0025a0   "}.lst-kix_kkxcn2m2pfez-0>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-0,lower-latin) ". "}.lst-kix_uru4kcognknp-2>li:before{content:"\0025a0   "}ol.lst-kix_pm404dkm1bwn-1{list-style-type:none}.lst-kix_bfo654b132ys-0>li:before{content:"\0025cf   "}ol.lst-kix_pm404dkm1bwn-0{list-style-type:none}.lst-kix_b7fcviwll54m-7>li:before{content:"\0025cb   "}.lst-kix_dy505sosxomp-5>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-5,lower-roman) ". "}ol.lst-kix_pm404dkm1bwn-3{list-style-type:none}ol.lst-kix_pm404dkm1bwn-3.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-3 0}ol.lst-kix_pm404dkm1bwn-2{list-style-type:none}.lst-kix_dy505sosxomp-8>li:before{content:"" counter(lst-ctn-kix_dy505sosxomp-8,lower-roman) ". "}.lst-kix_o0jblmcky2dx-0>li:before{content:"\0025cf   "}.lst-kix_778s5wjh1op-0>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-0,decimal) ". "}.lst-kix_uru4kcognknp-5>li:before{content:"\0025a0   "}.lst-kix_b7fcviwll54m-2>li:before{content:"\0025a0   "}.lst-kix_aoktuh8ieiw2-4>li:before{content:"\0025cb   "}.lst-kix_778s5wjh1op-3>li:before{content:"" counter(lst-ctn-kix_778s5wjh1op-3,decimal) ". "}.lst-kix_bfo654b132ys-8>li:before{content:"\0025a0   "}.lst-kix_bfo654b132ys-5>li:before{content:"\0025a0   "}.lst-kix_qjf83hxt4fh9-8>li:before{content:"\0025a0   "}ol.lst-kix_pm404dkm1bwn-8{list-style-type:none}.lst-kix_aoktuh8ieiw2-7>li:before{content:"\0025cb   "}.lst-kix_v74vdz8bmwxy-2>li{counter-increment:lst-ctn-kix_v74vdz8bmwxy-2}ol.lst-kix_pm404dkm1bwn-5{list-style-type:none}ol.lst-kix_pm404dkm1bwn-4{list-style-type:none}.lst-kix_rwyosv65luta-0>li:before{content:"\0025cf   "}ol.lst-kix_pm404dkm1bwn-7{list-style-type:none}.lst-kix_kkxcn2m2pfez-8>li:before{content:"" counter(lst-ctn-kix_kkxcn2m2pfez-8,lower-roman) ". "}ol.lst-kix_pm404dkm1bwn-6{list-style-type:none}ul.lst-kix_qrxoa9lchuen-2{list-style-type:none}ul.lst-kix_qrxoa9lchuen-3{list-style-type:none}ul.lst-kix_qrxoa9lchuen-0{list-style-type:none}ul.lst-kix_qrxoa9lchuen-1{list-style-type:none}.lst-kix_j08enc9fhspg-3>li:before{content:"" counter(lst-ctn-kix_j08enc9fhspg-3,decimal) ". "}.lst-kix_qjf83hxt4fh9-0>li:before{content:"\0025cf   "}.lst-kix_u1fdsix2vcok-3>li:before{content:"\0025cf   "}ol.lst-kix_a1uldluhyw5c-6.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-6 0}.lst-kix_2w83y07jwups-3>li:before{content:"\0025cf   "}.lst-kix_v74vdz8bmwxy-5>li:before{content:"" counter(lst-ctn-kix_v74vdz8bmwxy-5,lower-roman) ". "}.lst-kix_q8jk3tahpxqe-8>li:before{content:"\0025a0   "}.lst-kix_rvej8okh7pu0-2>li{counter-increment:lst-ctn-kix_rvej8okh7pu0-2}ol.lst-kix_pm404dkm1bwn-8.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-8 0}ol.lst-kix_pm404dkm1bwn-5.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-5 0}.lst-kix_2j5yuzkhoae5-5>li:before{content:"\0025a0   "}.lst-kix_r1iuztannmt0-5>li:before{content:"\0025a0   "}.lst-kix_7j2lgrfbtajv-3>li:before{content:"\0025cf   "}.lst-kix_aonucg4gowwq-2>li:before{content:"\0025a0   "}.lst-kix_rkgjrmjh1cjp-4>li:before{content:"\0025cb   "}.lst-kix_h8jpq87mkbro-6>li:before{content:"\0025cf   "}ul.lst-kix_qrxoa9lchuen-8{list-style-type:none}.lst-kix_hxbttwgh00gt-1>li:before{content:"\0025cb   "}ul.lst-kix_qrxoa9lchuen-6{list-style-type:none}.lst-kix_31p751u2s1-5>li:before{content:"" counter(lst-ctn-kix_31p751u2s1-5,lower-roman) ". "}.lst-kix_rf8s22pxlb23-4>li:before{content:"" counter(lst-ctn-kix_rf8s22pxlb23-4,lower-latin) ". "}ul.lst-kix_qrxoa9lchuen-7{list-style-type:none}.lst-kix_9go5hcewotk9-1>li:before{content:"\0025cb   "}ul.lst-kix_qrxoa9lchuen-4{list-style-type:none}.lst-kix_4koh223rnoq-7>li:before{content:"\0025cb   "}ul.lst-kix_qrxoa9lchuen-5{list-style-type:none}.lst-kix_estyhardezh6-4>li:before{content:"\0025cb   "}ol.lst-kix_kkxcn2m2pfez-3{list-style-type:none}.lst-kix_63ny8vky5bcr-4>li:before{content:"\0025cb   "}ol.lst-kix_kkxcn2m2pfez-2{list-style-type:none}.lst-kix_wjprrrk3451p-6>li:before{content:"\0025cf   "}.lst-kix_rvej8okh7pu0-7>li:before{content:"" counter(lst-ctn-kix_rvej8okh7pu0-7,lower-latin) ". "}ol.lst-kix_kkxcn2m2pfez-1{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-0{list-style-type:none}.lst-kix_ce2poqyf7yx2-1>li:before{content:"\0025cb   "}ol.lst-kix_kkxcn2m2pfez-7{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-6{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-5{list-style-type:none}ol.lst-kix_kkxcn2m2pfez-4{list-style-type:none}.lst-kix_wfbxhn4b3sc5-0>li{counter-increment:lst-ctn-kix_wfbxhn4b3sc5-0}.lst-kix_dhe74h5eopku-3>li:before{content:"\0025cf   "}ol.lst-kix_pm404dkm1bwn-6.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-6 0}.lst-kix_pm404dkm1bwn-7>li:before{content:"" counter(lst-ctn-kix_pm404dkm1bwn-7,lower-latin) ". "}.lst-kix_ciwimf4uiie7-3>li:before{content:"\0025cf   "}.lst-kix_kkxcn2m2pfez-2>li{counter-increment:lst-ctn-kix_kkxcn2m2pfez-2}.lst-kix_g8didx5tqsy8-5>li:before{content:"\0025a0   "}.lst-kix_j08enc9fhspg-1>li{counter-increment:lst-ctn-kix_j08enc9fhspg-1}.lst-kix_1ikdj3ttsm27-3>li:before{content:"\0025cf   "}ol.lst-kix_a1uldluhyw5c-8.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-8 0}.lst-kix_rf8s22pxlb23-3>li{counter-increment:lst-ctn-kix_rf8s22pxlb23-3}.lst-kix_2wuboluazjjj-0>li:before{content:"\0025cf   "}.lst-kix_a1uldluhyw5c-3>li:before{content:"" counter(lst-ctn-kix_a1uldluhyw5c-3,decimal) ". "}.lst-kix_a1uldluhyw5c-8>li{counter-increment:lst-ctn-kix_a1uldluhyw5c-8}.lst-kix_hdtt0qm0bdu2-1>li:before{content:"\0025cb   "}ol.lst-kix_a1uldluhyw5c-7.start{counter-reset:lst-ctn-kix_a1uldluhyw5c-7 0}.lst-kix_f9h15pl7l8ma-3>li:before{content:"\0025cf   "}ol.lst-kix_pm404dkm1bwn-7.start{counter-reset:lst-ctn-kix_pm404dkm1bwn-7 0}ol{margin:0;padding:0}table td,table th{padding:0}.c55{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:89.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c69{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:111.8pt;border-top-color:#cccccc;border-bottom-style:solid}.c30{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c133{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:94.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c79{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:93pt;border-top-color:#cccccc;border-bottom-style:solid}.c60{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:43.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c116{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:42.8pt;border-top-color:#cccccc;border-bottom-style:solid}.c1{color:#343434;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Courier New";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c12{color:#343434;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c87{margin-left:0pt;padding-top:17.8pt;list-style-position:inside;text-indent:45pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c8{color:#343434;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Courier New";font-style:normal}.c75{margin-left:0pt;padding-top:26.2pt;list-style-position:inside;text-indent:45pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c11{color:#000099;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c5{color:#343434;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c35{color:#4d4d4d;font-weight:400;text-decoration:none;font-size:11pt;font-family:"Arial";font-style:normal}.c0{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;padding-left:0pt;text-align:left}.c25{color:#000000;font-weight:400;text-decoration:none;font-size:15pt;font-family:"Arial";font-style:italic}.c34{color:#343434;font-weight:400;text-decoration:none;font-size:10pt;font-family:"Arial";font-style:normal}.c144{padding-top:0pt;padding-bottom:2pt;line-height:1.0;page-break-after:avoid;text-align:left}.c20{padding-top:50.6pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c81{padding-top:24pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c16{text-decoration-skip-ink:none;font-size:14pt;-webkit-text-decoration-skip:none;font-weight:700;text-decoration:underline}.c124{padding-top:18pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c39{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c148{padding-top:17.8pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c4{text-decoration-skip-ink:none;font-size:15pt;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c164{padding-top:26.6pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c94{padding-top:17.8pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c127{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:center}.c41{padding-top:50.9pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c24{color:#000000;font-weight:400;text-decoration:none;font-family:"Arial";font-style:normal}.c103{border-spacing:0;border-collapse:collapse;margin-right:auto}.c154{padding-top:16.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c7{padding-top:17.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c130{padding-top:18.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c170{padding-top:50.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c90{padding-top:18pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c100{padding-top:46.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c2{padding-top:15.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c106{padding-top:26.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c95{padding-top:37pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c26{color:#343434;text-decoration:none;font-size:13pt;font-style:normal}.c166{padding-top:187.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c27{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c93{padding-top:83.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c9{padding-top:16.1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c143{padding-top:15.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c139{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c19{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c165{padding-top:433pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c109{padding-top:23.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c115{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c82{padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c44{font-weight:400;text-decoration:none;font-family:"Arial";font-style:normal}.c129{padding-top:51.1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c155{padding-top:17pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c91{padding-top:43.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c146{padding-top:45.1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c141{padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c113{padding-top:149.5pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c126{margin-left:auto;border-spacing:0;border-collapse:collapse;margin-right:auto}.c150{padding-top:72.5pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c45{padding-top:10pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c107{padding-top:16.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c6{padding-top:5pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c73{padding-top:15.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c151{padding-top:22.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c159{padding-top:26.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c169{padding-top:38.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c92{padding-top:16.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c153{padding-top:14.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c102{padding-top:100.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c66{padding-top:89pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c128{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c152{padding-top:50.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c135{padding-top:17.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c122{padding-top:15.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c161{padding-top:50.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c160{padding-top:25.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c72{padding-top:25pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c142{padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c74{padding-top:46.1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c10{padding-top:15.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c42{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c136{padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c56{padding-top:7.7pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c28{padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c88{padding-top:22.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c162{padding-top:19.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c145{padding-top:18pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c31{padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c131{padding-top:45.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c158{padding-top:23.5pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c43{padding-top:17.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c105{padding-top:50.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c33{padding-top:3.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c99{padding-top:5.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c86{padding-top:17.5pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c137{padding-top:15.4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c29{padding-top:18pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c22{padding-top:13.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c85{padding-top:19.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c89{padding-top:44.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c83{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c47{text-decoration:none;font-size:12pt;font-style:normal}.c50{text-decoration:none;font-size:11pt;font-style:normal}.c77{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c38{color:#000000;text-decoration:none;font-style:normal}.c125{margin-left:0pt;list-style-position:inside;text-indent:45pt}.c59{text-decoration:none;font-style:normal}.c123{vertical-align:sub;font-size:24pt}.c3{color:inherit;text-decoration:inherit}.c96{vertical-align:sub;font-size:25pt}.c62{orphans:2;widows:2}.c76{color:#d90b00;font-size:24pt}.c21{color:#ff0000;vertical-align:baseline}.c134{vertical-align:sub;font-size:16.7pt}.c14{color:#343434;font-size:15pt}.c17{font-family:"Courier New";font-weight:400}.c65{font-family:"Arial";font-style:normal}.c13{margin-left:36pt;padding-left:0pt}.c32{padding:0;margin:0}.c118{vertical-align:sub;font-size:21.7pt}.c57{font-weight:700;font-family:"Arial"}.c51{height:20.2pt}.c53{font-size:12pt}.c111{text-indent:36pt}.c119{font-size:20.9pt}.c46{font-style:italic}.c147{vertical-align:sub}.c40{font-size:15pt}.c78{color:#d90b00}.c23{vertical-align:baseline}.c63{font-size:13.4pt}.c52{height:11pt}.c117{font-size:18pt}.c58{font-size:17pt}.c70{color:#ff0000}.c108{background-color:#efefef}.c64{font-weight:400}.c157{font-size:15.7pt}.c121{font-size:17.9pt}.c104{height:14pt}.c120{height:53.2pt}.c84{font-size:36pt}.c54{font-size:11pt}.c168{font-size:76pt}.c37{font-size:20pt}.c61{color:#000099}.c48{font-size:10pt}.c132{font-size:10.6pt}.c98{font-family:"Arial"}.c67{font-size:13pt}.c156{height:12pt}.c18{font-size:14pt}.c138{vertical-align:super}.c140{height:19.5pt}.c36{color:#343434}.c110{margin-left:36pt}.c71{color:#4d4d4d}.c80{font-family:"Courier New"}.c149{color:#0000ff}.c97{text-decoration:none}.c171{color:#ffffff}.c49{font-size:9pt}.c68{font-weight:700}.c114{font-size:12.7pt}.c101{color:#000000}.c112{margin-left:108pt}.c167{font-size:12.9pt}.c163{font-size:18.3pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c77 doc-content"><div><p class="c19 c62 c52"><span class="c24 c23 c54"></span></p></div><h3 class="c127" id="h.dh5fsmbvsnte"><span class="c78 c23 c121">VERSION </span><span class="c78 c121">4</span><span class="c78 c23 c121">: AN INTRODUCTION TO <br></span><span class="c23 c168">Data Science</span></h3><h3 class="c127" id="h.ovhbg0iz0kzj"><span class="c23 c119">by Jan</span><span class="c119">&nbsp;Pearce, Berea College</span><span class="c24 c23 c63"><br></span><span class="c63"><br></span><span class="c59 c23 c18 c71 c68">&copy; V4 2020 by Jan Pearce</span></h3><p class="c27"><span class="c71">Forked from &copy; V3 </span><span class="c44 c23 c71">2012, 2013 </span><span class="c71">b</span><span class="c44 c23 c71">y Jeffrey Stanton, Portions &copy; 2013 </span><span class="c71">b</span><span class="c35 c23">y Robert De Graaf </span></p><p class="c27 c52"><span class="c23 c35"></span></p><p class="c19 c62"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 648.00px; height: 141.33px;"><img alt="A world cloud of the words in this textbook created by Jan Pearce on May 7, 2020 at https://wordart.com/" src="images/image24.png" style="width: 648.00px; height: 141.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="DATA World Cloud"></span></p><h4 class="c144 c156" id="h.23bh94pkgx7f"><span class="c47 c57 c23 c101"></span></h4><h4 class="c144" id="h.p7crugpu6k1j"><span class="c47 c57 c23 c101">Open Source License</span></h4><p class="c19 c62 c52"><span class="c24 c23 c54"></span></p><p class="c42"><span class="c23 c64 c36">This book is distributed under the Creative Commons</span><span class="c36">&nbsp;</span><span class="c23 c64 c36">AttributionNonCommercial-ShareAlike 3.0 license. You are free to copy, distribute, and transmit this work. You are free to add or adapt the work. You must attribute the work to the author(s) listed above. You may not use this work or derivative works for commercial purposes. If you alter, transform, or build upon this work you may di</span><span class="c36">s</span><span class="c23 c64 c36">tribute the resulting work only under the same or similar license. For additional details, please see: </span><span class="c83 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://creativecommons.org/licenses/by-nc-sa/3.0/&amp;sa=D&amp;source=editors&amp;ust=1751550344878684&amp;usg=AOvVaw1Vhst6lIkI8aDowu-_9j8U">http://creativecommons.org/licenses/by-nc-sa/3.0/</a></span></p><p class="c19 c52 c62"><span class="c24 c23 c54"></span></p><h4 class="c144" id="h.ud6ueh70lcen"><span class="c47 c57 c23 c101">A Brief History of This Book</span></h4><p class="c42 c52"><span class="c44 c23 c54 c36"></span></p><p class="c42"><span class="c36">The original version of this</span><span class="c23 c36">&nbsp;book was developed for the Certificate of Data Science program at Syracuse University&rsquo;s School of Information Studies by </span><span class="c36">the original author, Jeffrey Stanton</span><span class="c23 c36">.</span><span class="c36">&nbsp;</span><span class="c23 c36">A PDF version of version 3 of this book and code examples used in </span><span class="c36">version 3 of the </span><span class="c23 c36">book </span><span class="c36">were once </span><span class="c23 c36">available at: </span><span class="c83 c23"><a class="c3" href="https://www.google.com/url?q=http://jsresearch.net/wiki/projects/teachdatascience&amp;sa=D&amp;source=editors&amp;ust=1751550344880178&amp;usg=AOvVaw26yd12t-YNMZ59Y5vEvBnG">http://jsresearch.net/wiki/projects/teachdatascience</a></span><span>&nbsp;and were &nbsp;forked to create this version.</span><span>&nbsp;</span><span class="c23">The material provided in this book is provided &quot;as is&quot; with no warranty or guarantees with respect to its accuracy or suitability for any purpose. The original authors thank</span><span>ed</span><span class="c23">&nbsp;Ashish Verma for help with revisions to </span><span class="c24 c23 c54">the chapter on Twitter.</span></p><p class="c164 title" id="h.rxeklcrsotyc"><span class="c70">Preface</span><span><br></span><span class="c23">Dat</span><span>a </span><span class="c38 c57 c23 c84">Science: Many Skills</span></p><p class="c159"><span class="c40 c23 c68">&nbsp;</span><span class="c14 c46 c23 c68">Data Science refers to an emerging area of work</span><span class="c14 c46 c68">&nbsp;</span><span class="c14 c46 c23 c68">concerned with the collection, preparation, analysis, visualization, management, and preservation of large collections of information. Although the name Data Science seems to connect most strongly with areas such as databases and computer science, many different kinds of skills including</span><span class="c14 c46 c68">&nbsp;</span><span class="c14 c46 c23 c68">non-mathematical skills are needed. </span></p><h2 class="c124" id="h.l4dujgkb8ccs"><span>O</span><span class="c38 c57 c23 c117">verview </span></h2><ol class="c32 lst-kix_v74vdz8bmwxy-0 start" start="1"><li class="c0 li-bullet-0"><span class="c24 c23 c18">Data science includes data analysis as an important component of the skill set required for many jobs in this area, but is not the only necessary skill. </span></li><li class="c0 li-bullet-0"><span class="c24 c23 c18">A brief case study of a supermarket point of sale system illustrates the many challenges involved in data science work. </span></li><li class="c0 li-bullet-0"><span class="c24 c23 c18">Data scientists play active roles in the design and implementation work of four related areas: data architecture, data acquisition, data analysis, and data archiving. </span></li><li class="c0 li-bullet-0"><span class="c44 c23 c18">Key skills highlighted by the brief case study include communication skills, data analysis skills, and ethical </span><span class="c18">reasoning skills.</span></li></ol><p class="c19"><span class="c25 c23">Word frequencies from the definitions in a Shakespeare glossary </span><span class="c46 c40">in a bar graph format aka a Pareto graph format where the height of the bar indicates the word frequency.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 596.00px; height: 416.00px;"><img alt="" src="images/image58.png" style="width: 596.00px; height: 416.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c19 c52"><span class="c25 c23"></span></p><p class="c19 c62"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 648.00px; height: 141.33px;"><img alt="A world cloud of the words in this textbook created by Jan Pearce on May 7, 2020 at https://wordart.com/" src="images/image24.png" style="width: 648.00px; height: 141.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="DATA World Cloud"></span></p><p class="c19"><span class="c25 c23">Word frequencies of this text book in a word cloud format where the relative size of the word indicates its frequency. While professional data scientists do need skills with mathematics and statistics, much of the data in the world is unstructured and non-numeric.</span></p><p class="c19 c52"><span class="c23 c25"></span></p><p class="c19"><span class="c5">For some, the term &quot;Data Science&quot; evokes images of statisticians in white lab coats staring fixedly at blinking computer screens filled with scrolling numbers. Nothing could be further from the truth. First of all, statisticians do not wear lab coats: this fashion statement is reserved for biologists, doctors, and others who have to keep their clothes clean in environments filled with unusual fluids. Second, much of the data in the world is non-numeric and unstructured. In this context, unstructured means that the data are not arranged in neat rows and columns. Think of a web page full of photographs and short messages among friends: very few numbers</span><span class="c14">&nbsp;</span><span class="c5">to work</span><span class="c14">&nbsp;w</span><span class="c5">ith there. While it is certainly true that companies, schools, and governments use plenty of numeric information, sales of products, grade point averages, and tax assessments are a few examples there is lots of other information in the world that mathematicians and statisticians look at and cringe. So, while it is always useful to have great math skills, there is much to be accomplished in the world of data science for those of us who are presently more comfortable working with words, lists, photographs, sounds, and other kinds of information. </span></p><p class="c7"><span class="c5">In addition, data science is much more than simply analyzing data. There are many people who enjoy analyzing data and who could happily spend all day looking at histograms and averages, but for those who prefer other activities, data science offers a range of roles and requires a range of skills. Let&rsquo;s consider this idea by thinking about some of the data involved in buying a box of cereal. </span></p><p class="c7"><span class="c5">Whatever your cereal preferences are: fruity, chocolaty, fibrous, or nutty you prepare for the purchase by writing &quot;cereal&quot; on your grocery list. Already your planned purchase is a piece of data, albeit a pencil scribble on the back on an envelope that only you can read. When you get to the grocery store, you use your data as a reminder to grab that jumbo box of FruityChocoBoms off the shelf and put it in your cart. At the checkout line the cashier scans the barcode on your box and the cash register logs the price. Back in the warehouse, a computer tells the stock manager that it is time to request another order from the distributor, as your purchase was one of the last boxes in the store. You also have a coupon for your big box and the cashier scans that, giving you a predetermined discount. At the end of the week, a report of all the scanned manufacturer coupons gets uploaded to the cereal company so that they can issue a reimbursement to the grocery store for all of the coupon discounts they have handed out to customers. Finally, at the end of the month, a store manager looks at a colorful collection of pie charts showing all of the different kinds of cereal that were sold, and on the basis of strong sales of fruity cereals, decides to offer more varieties of these on the store&rsquo;s limited shelf space next month. </span></p><p class="c7"><span class="c5">So the small piece of information that began as a scribble on your grocery list ended up in many different places, but most notably on the desk of a manager as an aid to decision making. On the trip from your pencil </span><span class="c14">to the manager&#39;s</span><span class="c5">&nbsp;desk, the data went through many transformations. In addition to the computers where the data may have stopped by or stayed on for the long term, lots of other pieces of hardware such as the barcode scanner were involved in collecting, manipulating, transmitting, and storing the data. In addition, many different pieces of software were used to organize, aggregate, visualize, and present the data. Finally, many different &quot;human systems&quot; were involved in working with the data. People decided which systems to buy and install, who should get access to what kinds of data, and what would happen to the data after its immediate purpose was fulfilled. The personnel of the grocery chain and its partners made a thousand other detailed decisions and negotiations before the scenario described above could become reality. </span></p><p class="c29"><span class="c5">Obviously data scientists are not involved in all of these steps. Data scientists don&rsquo;t design and build computers or barcode readers, for instance. So where would the data scientists play the most valuable role? Generally speaking, data scientists play the most active roles in the four A&rsquo;s of data: data architecture, data acquisition, data analysis, and data archiving. Using our cereal example, let&rsquo;s look at them one by one. First, with respect to architecture, it was important in the design of the &quot;point of sale&quot; system (what retailers call their cash registers and related gear) to think through in advance how different people would make use of the data coming through the system. The system architect, for example, had a keen appreciation that both the stock manager and the store manager would need to use the data scanned at the registers, albeit for somewhat different purposes. A data scientist would help the system architect by providing input on how the data would need to be routed and organized to support the analysis, visualization, and presentation of the data to the appropriate people. </span></p><p class="c29"><span class="c5">Next, acquisition focuses on how the data are collected, and, importantly, how the data are represented prior to analysis and presentation. For example, each barcode represents a number that, by itself, is not very descriptive of the product it represents. At what point after the barcode scanner does its job should the number be associated with a text description of the product or its price or its net weight or its packaging type? Different barcodes are used for the same product (for example, for different sized boxes of cereal). When should we make note that purchase X and purchase Y are the same product, just in different packages? Representing, transforming, grouping, and linking the data are all tasks that need to occur before the data can be profitably analyzed, and these are all tasks in which the data scientist is actively involved. </span></p><p class="c7"><span class="c5">The analysis phase is where data scientists are most heavily involved. In this context we are using analysis to include summarization of the data, using portions of data (samples) to make inferences about the larger context, and visualization of the data by presenting it in tables, graphs, and even animations. Although there are many technical, mathematical, and statistical aspects to these activities, keep in mind that the ultimate audience for data analysis is always a person or people. These people are the &quot;data users,&quot; and fulfilling their needs is the primary job of a data scientist. This point highlights the need for excellent communication skills in data science. The most sophisticated statistical analysis ever developed will be useless unless the results can be effectively communicated to the data user. </span></p><p class="c7"><span class="c5">Finally, the data scientist must become involved in the archiving of the data. Preservation of collected data in a form that makes it highly reusable what you might think of as &quot;data curation&quot; is a difficult challenge because it is so hard to anticipate all of the future uses of the data. For example, when the developers of Twitter were working on how to store tweets, they probably never anticipated that tweets would be used to pinpoint earthquakes and tsunamis, but they had enough foresight to realize that &quot;geocodes&quot; data that shows the geographical location from which a tweet was sent could be a useful element to store with the data. </span></p><p class="c7"><span class="c5">All in all, our cereal box and grocery store example helps to highlight where data scientists get involved and the skills they need. Here are some of the skills that the example suggested: </span></p><ul class="c32 lst-kix_62mkmw7g756i-0 start"><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Learning the application domain</span><span class="c5">&nbsp;<br>The data scientist must quickly learn how the data will be used in a particular context. </span></li><li class="c29 c13 li-bullet-0"><span class="c59 c14 c23 c68">Communicating with data users <br></span><span class="c5">A data scientist must possess strong skills for learning the needs and preferences of users. Translating back and forth between the technical terms of computing and statistics and the vocabulary of the application domain is a critical skill. </span></li><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Seeing the big picture of a complex system</span><span class="c5">&nbsp;<br>After developing an understanding of the application domain, the data scientist must imagine how data will move around among all of the relevant systems and people. </span></li><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Knowing how data can be represented</span><span class="c5">&nbsp;<br>Data scientists must have a clear understanding about how data can be stored and linked, as well as about &quot;metadata&quot; (data that describes how other data are arranged). </span></li><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Data transformation and analysis</span><span class="c5">&nbsp;<br>When data </span><span class="c14">becomes</span><span class="c5">&nbsp;available for the use of decision makers, data scientists must know how to transform, summarize, and make inferences from the data. As noted above, being able to communicate the results of analyses to users is also a critical skill here. </span></li><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Visualization and presentation</span><span class="c5">&nbsp;<br>Although numbers often have the edge in precision and detail, a good data display (e.g., a bar chart) can often be a more effective means of communicating results to data users. </span></li><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Attention to quality <br></span><span class="c5">No matter how good a set of data may be, there is no such thing as perfect data. Data scientists must know the limitations of the data they work with, know how to quantify its accuracy, and be able to make suggestions for improving the quality of the data in the future. </span></li><li class="c29 c13 li-bullet-0"><span class="c59 c14 c23 c68">Ethical reasoning <br></span><span class="c5">If data are important enough to collect, they are often important enough to affect people&rsquo;s lives. Data scientists must understand important ethical issues such as privacy, and must be able to communicate the limitations of data to try to prevent misuse of data or analytical results. </span></li></ul><p class="c7"><span class="c5">The skills and capabilities noted above are just the tip of the iceberg, of course, but notice what a wide range is represented here. While a keen understanding of numbers and mathematics is important, particularly for data analysis, the data scientist also needs to have excellent</span><span class="c14">&nbsp;</span><span class="c5">communication skills, be a great systems thinker, have a good eye for visual displays, and be highly capable of thinking critically about how data will be used to make decisions and affect people&rsquo;s lives. Of course there are very few people who are good at all of these things, so some of the people interested in data will specialize in one area, while others will become experts in another area. This highlights the importance of teamwork, as well. </span></p><p class="c7"><span class="c5">In this Introduction to Data Science eBook, a series of data problems of increasing complexity is used to illustrate the skills and capabilities needed by data scientists. The open source data analysis program known as &quot;R&quot; and its graphical user interface companion &quot;R-Studio&quot; are used to work with real data examples to illustrate both the challenges of data science and some of the techniques used to address those challenges. To the greatest extent possible, real datasets reflecting important contemporary issues are used as the basis of the discussions. </span></p><p class="c7"><span class="c5">No one book can cover the wide range of activities and capabilities involved in a field as diverse and broad as data science. Throughout t</span><span class="c14">his book</span><span class="c5">&nbsp;references</span><span class="c14">&nbsp;we reference </span><span class="c5">guides and additional resources </span><span class="c14">that &nbsp;provide</span><span class="c5">&nbsp;the interested reader access to additional information. In the open source spirit of &quot;R&quot; and &quot;R Studio&quot; these are, wherever possible, web-based and free. In fact, one of the guides that appears most frequently in these pages is &quot;Wikipedia,&quot; the free, online, user sourced encyclopedia. Although some teachers and librarians have legitimate complaints and concerns about Wikipedia, and it is admittedly not perfect, it is a very useful learning resource. Because it is free, because it covers about 50 times more topics than a printed encyclopedia, and because it keeps up with fast moving topics (like data science) better than printed encyclopedias, Wikipedia is very useful for getting a quick introduction to a topic. You can&rsquo;t become an expert on a topic by only consulting Wikipedia, but you can certainly become smarter by starting there. </span></p><p class="c7"><span class="c5">Another very useful resource is Khan Academy. </span><span class="c14">Thousands of people</span><span class="c5">&nbsp;around the world use Khan Academy as a refresher course for a range of topics or as a quick introduction to a topic that they never studied before. All of the lessons at Khan Academy are free, and if you log in</span><span class="c14">, </span><span class="c5">you can do exercises and keep track of your progress. </span></p><p class="c7"><span class="c5">At the end of each chapter of this book,</span><span class="c14">&nbsp;</span><span class="c5">sources and other resources show the key topics relevant to the chapter. These sources provide a great place to start if you want to learn more about any of the topics that chapter does not explain in detail. </span></p><p class="c7"><span class="c5">It is </span><span class="c14">essential to</span><span class="c5">&nbsp;have access to the Internet while you are reading, so that you can follow some of the many links this book provides. Also, as you move into the sections in the book where open source software such as the R data analysis system is used, you will need to have access </span><span class="c14">to the Internet in order to</span><span class="c5">&nbsp;</span><span class="c14">create and run programs</span><span class="c5">. </span></p><p class="c7"><span class="c5">One last thing: The book presents topics in an order that should work well for people with little or no experience in computer science or statistics. </span><span class="c14">Hopefully, t</span><span class="c5">here&rsquo;s something here for everyone and, after all, you can&rsquo;t beat the price of $0! </span></p><h3 class="c39" id="h.u62j4ypbyy9n"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_u9q7i696xg3u-0 start"><li class="c87"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/E-Science&amp;sa=D&amp;source=editors&amp;ust=1751550344900496&amp;usg=AOvVaw1iQABTLpIkGdm1Cezx7Mk8">Wikipedia: E-Science </a></span></li><li class="c87"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/E-Science_librarianship&amp;sa=D&amp;source=editors&amp;ust=1751550344900690&amp;usg=AOvVaw3J_GBkq0NLmxncQJDCtzD4">Wikipedia: E-Science Llibrarianship </a></span></li><li class="c87"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Wikipedia:Size_comparisons&amp;sa=D&amp;source=editors&amp;ust=1751550344900872&amp;usg=AOvVaw2bFUfy7pPVZpWYj1IDrOwl">Wikipedia: Wikipedia: Size comparisons</a></span><span class="c11">&nbsp;</span></li><li class="c87"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Statistician&amp;sa=D&amp;source=editors&amp;ust=1751550344901126&amp;usg=AOvVaw1GUPRzP1eonTArI60AgyCQ">http://en.wikipedia.org/wiki/Statistician</a></span><span class="c11">&nbsp;</span></li><li class="c87"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Visualization_(computer_graphics)&amp;sa=D&amp;source=editors&amp;ust=1751550344901348&amp;usg=AOvVaw3WaYBbQs7uN4KxIJom_zrt">http://en.wikipedia.org/wiki/Visualization_(computer_graphics)</a></span><span class="c5">&nbsp;</span></li><li class="c87"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.khanacademy.org/&amp;sa=D&amp;source=editors&amp;ust=1751550344901488&amp;usg=AOvVaw15M-4d20WnYJqkvXtxYrBN">Khan Academy</a></span></li><li class="c87"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/E-Science_librarianship&amp;sa=D&amp;source=editors&amp;ust=1751550344901676&amp;usg=AOvVaw0bm6FI0rEmTUtQe61c_qe-">http://en.wikipedia.org/wiki/E-Science_librarianship</a></span></li><li class="c87"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.readwriteweb.com/hack/2011/09/unlocking-big-dat&amp;sa=D&amp;source=editors&amp;ust=1751550344901883&amp;usg=AOvVaw1Y-iFE-1W2BtSckT5kYB9D">Unlocking Big Data with R by David Smith </a></span></li><li class="c87"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://rstudio.org/&amp;sa=D&amp;source=editors&amp;ust=1751550344902009&amp;usg=AOvVaw0iyTrneyxVqR1awzDy48A9">R-Studio</a></span></li></ul><p class="c81 c62 title" id="h.of0cqou4p9ze"><span class="c21">CHAPTER 1</span><span class="c23">&nbsp;<br>About</span><span>&nbsp;</span><span class="c23">Dat</span><span>a<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 672.00px; height: 348.00px;"><img alt="" src="images/image13.png" style="width: 672.00px; height: 348.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="ASCII Table showing ASCII and binary"></span></p><p class="c19 c62"><span class="c25 c23">Data comes from the Latin word, &quot;datum,&quot; meaning a &quot;thing given.&quot; Although the term &quot;data&quot; has been used since as early as the 1500s, modern usage started in the 1940s and 1950s as practical electronic computers began to input, process, and output data. This chapter discusses the nature of data and introduces key concepts for newcomers without computer science experience. </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c5">The inventor of the World Wide Web, Tim Berners-Lee, is often quoted as having said, &quot;Data is not information, information is not knowledge, knowledge is not understanding, understanding is not wisdom.&quot; This quote suggests a kind of pyramid, where data are the raw materials that make up the foundation at the bottom of the pile, and information, knowledge, understanding and wisdom represent higher and higher levels of the pyramid. In one sense, the major goal of a data scientist is to help people to turn data into information and onwards up the pyramid. Before getting started on this goal, though, it is important to have a solid sense of what data actually are. (Notice that this book treats the word &quot;data&quot; as a plural noun in common usage you may often hear it referred to as singular instead.) </span><span class="c14">R</span><span class="c5">ead on for an introduction to the most basic ingredient to the data scientist&rsquo;s efforts: data. </span></p><p class="c7"><span class="c5">A substantial amount of what we know and say about data in the present day comes from work by a U.S. mathematician named Claude Shannon. Shannon worked before, during, and after World War II on a variety of mathematical and engineering problems related to data and information. Not to go crazy with quotes, or anything, but Shannon is quoted as having said, </span><span class="c5">&quot;The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.&quot; </span><span class="c5">Th</span><span class="c14">is means that if you came into a conversation in the middle, you would have a hard time reconstructing the beginning even approximately! </span><span class="c5">This quote helpfully captures key ideas about data that are important in this book by focusing on the idea of data as a message that moves from a source to a recipient. Think about the simplest possible message that you could send to another person over the phone, via a text message, vid</span><span class="c14">eo conference </span><span class="c5">or even in person. Let&rsquo;s say that a </span><span class="c14">family member </span><span class="c5">had asked you a question, for example whether you wanted to</span><span class="c14">&nbsp;eat fish for</span><span class="c5">&nbsp;dinner th</span><span class="c14">at</span><span class="c5">&nbsp;</span><span class="c14">night</span><span class="c5">. You can answer yes or no. You can call </span><span class="c14">your family member</span><span class="c5">&nbsp;on the phone, and say yes or no. You might have a bad connection, though, and your </span><span class="c14">family member </span><span class="c5">might not be able to hear you. Likewise, you could send them a text message with your answer, yes or no, and hope that they have their phone turned on so that they can receive the message. Or you could tell </span><span class="c14">that family member</span><span class="c5">&nbsp;face to face, hoping that </span><span class="c14">they </span><span class="c5">did not have </span><span class="c14">their </span><span class="c5">earbuds turned up so loud that </span><span class="c14">they </span><span class="c5">couldn&rsquo;t hear you. In all three cases you have a one &quot;bit&quot; message that you want to send to your </span><span class="c14">family member</span><span class="c5">, yes or no, with the goal of &quot;reducing </span><span class="c14">their </span><span class="c5">uncertainty&quot; about whether you </span><span class="c14">want fish for dinner</span><span class="c5">. Assuming that message gets through without being garbled or lost, you will have successfully transmitted one bit of information from you to </span><span class="c14">them</span><span class="c5">. Claude Shannon developed some mathematics, now often referred to as &quot;Information Theory,&quot; that carefully quantified how bits of data transmitted accurately from a source to a recipient can reduce uncertainty by providing information. A great deal of the computer networking equipment and software in the world today and especially the huge linked worldwide network we call the Internet is primarily concerned with this one basic task of getting bits of information from a source to a destination. </span></p><p class="c7"><span class="c5">Once we are comfortable with the idea of a &quot;bit&quot; as the most basic unit of information, either &quot;yes&quot; or &quot;no,&quot; we can combine bits together to make more complicated structures. First, let&rsquo;s switch labels just slightly. Instead of &quot;no&quot; we will start using zero, and instead of &quot;yes&quot; we will start using one. So we now have a single digit, albeit one that has only two possible states: zero or one (we&rsquo;re temporarily making a rule against allowing any of the bigger digits like three or seven). This is in fact the origin of the word &quot;bit,&quot; which is a squashed down version of the phrase &quot;Binary digIT.&quot; A single binary digit can be 0 or 1, and that would give us just two options like </span><span class="c14">&ldquo;yes&rdquo; or &ldquo;no&rdquo; </span><span class="c5">but there is nothing stopping us from using more than one binary digit in our messages. Have a look at the example in the table below: </span><span class="c5"><br></span></p><table class="c103"><tr class="c51"><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">MEANING</span></p></td><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">2ND DIGIT</span></p></td><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">1ST DIGIT</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">No</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">0</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">0</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Maybe</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">0</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">1</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Probably</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">1</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">0</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Definitely</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">1</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">1</span></p></td></tr></table><p class="c7"><span class="c5">Here we have started to use two binary digits two bits to create a &quot;code book&quot; for four different messages that we might want to transmit to our friend about her dinner party. If we were certain that we would not attend, we would send her the message 0 0. If we definitely planned to attend we would send her 1 1. But we have two additional possibilities, &quot;Maybe&quot; which is represented by 0 1, and &quot;Probably&quot; which is represented by 1 0. It is interesting to compare our original yes/no message of one bit with this new four-option message with two bits. In fact, every time you add a new bit you double the number of possible messages you can send. So three bits would give eight options and four bits would give 16 options. How many options would there be for five </span><span class="c5">bits</span><span class="c5">? </span></p><p class="c7"><span class="c5">When we get up to eight bits which provides 256 different combinations we finally have something of a reasonably useful size to work with. Eight bits is commonly referred to as a &quot;byte&quot; this term probably started out as a play on words with the word bit. (Try looking up the word </span><span class="c5">&quot;n</span><span class="c14">i</span><span class="c5">bble&quot;</span><span class="c5">&nbsp;online!) A byte offers enough different combinations to encode all of the letters of the alphabet, including capital and small letters. There is an old rulebook called &quot;ASCII&quot; the American Standard Code for Information Interchange which matches up patterns of eight bits with the letters of the alphabet, punctuation, and a few other odds and ends. For example the bit pattern 0100 0001 represents the capital letter A and the next higher pattern, 0100 0010, represents capital B. Try looking up an ASCII table online (for example, </span><span class="c4 c23 c64 c65"><a class="c3" href="https://www.google.com/url?q=http://www.asciitable.com/&amp;sa=D&amp;source=editors&amp;ust=1751550344912148&amp;usg=AOvVaw3Y6Ig9hkKrr7E-EU9VKw6z">http://www.asciitable.com/</a></span><span class="c5">) and you can find all of the combinations. Note that the codes may not actually be shown in binary because it is so difficult for people to read long strings of ones and zeroes. Instead you may see the equivalent codes shown in hexadecimal (base 16), octal (base 8), or the most familiar form that we all use everyday, base 10. Although you might remember base conversions from high school math class,</span><span class="c14">&nbsp;some people find it fun to</span><span class="c5">&nbsp;practice conver</span><span class="c14">ting</span><span class="c5">&nbsp;between binary, hexadecimal, and decimal (base 10). You might also enjoy Vi Hart&rsquo;s </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DOCYZTg3jahU&amp;sa=D&amp;source=editors&amp;ust=1751550344912950&amp;usg=AOvVaw38MyZXDEwxKPm9AyjjXyQ3">Binary Hand Dance</a></span><span class="c14">&nbsp;video in which she and her family count in binary on their fingers.</span></p><p class="c7"><span class="c5">Most of the work we do in this book will be in decimal, but more complex work with data often requires understanding hexadecimal and being able to know how a hexadecimal number, like 0xA3, translates into a bit pattern. We don</span><span class="c14">&rsquo;t need that right now, though.</span><span class="c5">&nbsp;</span><span class="c14">If you are interested, you might t</span><span class="c5">ry searching online for &quot;binary conversion tutorial&quot; and you will find lots of useful sites. </span></p><h2 class="c124" id="h.asxtltee43ns"><span class="c38 c57 c23 c117">Combining Bytes into Larger Structures </span></h2><p class="c7"><span class="c5">Now that we have the idea of a byte as a small collection of bits (usually eight) that can be used to store and transmit things like letters and punctuation marks, we can start to build up to bigger and better things. First, it is very easy to see that we can put bytes together into lists in order to make a &quot;string&quot; of letters, what is often referred to as a &quot;character string.&quot; If we have a piece of text, like &quot;this is a piece of text&quot; we can use a collection of bytes to represent it like this: </span></p><p class="c43"><span class="c59 c14 c57 c23">011101000110100001101001011100110010000001101001011100110010 000001100001001000000111000001101001011001010110001101100101 001000000110111101100110001000000111010001100101011110000111 0100 </span></p><p class="c7"><span class="c5">Now nobody wants to look at that, let alone encode or decode it by hand, but fortunately, the computers and software we use these days takes care of the conversion and storage automatically. For example, when we tell the open source data language &quot;R&quot; to store &quot;this is a piece of text&quot; for us like this: </span></p><p class="c2"><span class="c1">myText &lt;- &quot;this is a piece of text&quot; </span></p><p class="c29"><span class="c5">...we can be certain that inside the computer there is a long list of zeroes and ones that represent the text that we just stored. By the way, in order to be able to get our piece of text back later on, we have made a kind of storage label for it (the word &quot;myText&quot; above). Anytime that we want to remember our piece of text or use it for something else, we can use the label &quot;myText&quot; to open up the chunk of computer memory where we have put that long list of binary digits that represent our text. The left-pointing arrow made up out of the less-than character (&quot;</span><span class="c14 c17">&lt;</span><span class="c5">&quot;) and the dash character (&quot;</span><span class="c14 c17">-</span><span class="c5">&quot;) gives R the command to take what is on the right hand side (the quoted text) and put it into what is on the left hand side (the storage area we have labeled &quot;</span><span class="c1">myText</span><span class="c5">&quot;). Some people call this the assignment arrow and it is used in some computer languages to make it clear to the human who writes or reads it which direction the information is flowing. </span></p><p class="c7"><span class="c5">From the computer&rsquo;s standpoint, it is even simpler to store, remember, and manipulate numbers instead of text. Remember that an eight bit byte can hold 256 combinations, so just using that very small amount we could store the numbers from 0 to 255. (Of course, we could have also done 1 to 256, but much of the counting and numbering that goes on in computers starts with zero instead of one.) Really, though, 255 is not much to work with. We couldn&rsquo;t count the number of houses in most towns or the number of cars in a large parking garage unless we can count higher than 255. If we put together two bytes to make 16 bits we can count from zero up to 65,535, but that is still not enough for some of the really big numbers in the world today (for example, there are more than 200 million cars in the U.S. alone). Most of the time, if we want to be flexible in representing an integer (a number with no decimals), we use four bytes stuck together. Four bytes stuck together is a total of 32 bits, and that allows us to store an integer as high as 4,294,967,295. </span></p><p class="c7"><span class="c5">Things get slightly more complicated when we want to store a negative number or a number that has digits after the decimal point. If you are curious, try looking up &quot;two&#39;s complement&quot; for more information about how signed numbers are stored and &quot;floating point&quot; for information about how numbers with digits after the decimal point are stored. For our purposes in this book, the most important thing to remember is that text is stored differently than numbers, and among numbers integers are stored differently than floating point. Later we will find that it is sometimes necessary to convert between these different representations, so it is always important to know how it is represented. </span></p><p class="c7"><span class="c5">So far we have mainly looked at how to store one thing at a time, like one number or one letter, but when we are solving problems with data we often need to store a group of related things together. The simplest place to start is with a list of things that are all stored in the same way. For example, we could have a list of integers, where each thing in the list is the age of a person in your family. The list might look like this: 43, 42, 12, 8, 5. The first two numbers are the ages of the parents and the last three numbers are the ages of the kids. Naturally, inside the computer each number is stored in binary, but fortunately we don&rsquo;t have to type them in that way or look at them that way. Because there are no decimal points, these are just plain integers and a 32 bit integer (4 bytes) is more than enough to store each one. This list contains items that are all the same &quot;type&quot; or &quot;mode.&quot; &nbsp;</span><span class="c14">In R, </span><span class="c14">&nbsp;&quot;</span><span class="c14 c68">vector</span><span class="c14">&quot; i</span><span class="c5">s an ordered list of data elements of the same data type</span><span class="c14">&nbsp;(e.</span><span class="c5">g. all integers or all str</span><span class="c14">ings, etc).</span><span class="c5">&nbsp;We can create a vector with R very easily by listing the </span><span class="c14">data elements</span><span class="c5">, separated by commas and inside parentheses: </span></p><p class="c2"><span class="c1">c(43, 42, 12, 8, 5) </span></p><p class="c29"><span class="c5">The letter &quot;</span><span class="c1">c</span><span class="c5">&quot; in front of the opening parenthesis stands for concatenate, which means to join things together. We can also put in some of what we learned</span><span class="c14">&nbsp;</span><span class="c5">above to store our vector in a named location</span><span class="c14">:</span></p><p class="c10"><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;&lt;- c(43, 42, 12, 8, 5)</span></p><p class="c43"><span class="c5">W</span><span class="c14">hen we learn to run our code, we will have just created our </span><span class="c5">first &quot;data set.&quot; It is very small, for sure, only five items, but also very useful for illustrating several major concepts about data. Here&rsquo;s a recap: </span></p><ul class="c32 lst-kix_4ug2duo9ze3p-0 start"><li class="c0 li-bullet-0"><span class="c5">In the heart of the computer, all data are represented in binary. One binary digit, or bit, is the smallest chunk of data that we can send from one place to another. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Although all data are at heart binary, computers and software help to represent data in more convenient forms for people to see. Three important representations are: </span><span class="c5">&quot;character&quot; for representing text, &quot;integer&quot; for representing numbers with no digits after the decimal point, and &quot;floating point&quot; for numbers that may have digits after the decimal point. </span><span class="c5">The list of numbers in our tiny data set just above are integers. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Numbers and text can be collected into lists, which the open source program &quot;R&quot; calls vectors. A vector has a length, which is the number of items in it, and a &quot;mode&quot; which is the type of data stored in the vector. The vector we were just working on has a length of 5 and a mode of integer. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">In order to be able to remember where we stored a piece of data, most computer programs, including R, give us a way of labeling a chunk of computer memory. We chose to give the 5-item vector up above the name &quot;</span><span class="c14">myFamAge</span><span class="c5">.&quot; Some people might refer to this named list as a &quot;variable,&quot; because the value of it varies, depending upon which member of the list you are examining. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">If we gather together one or more variables into a sensible group, we can refer to them together as a &quot;data set.&quot; Usually, it doesn&rsquo;t make sense to refer to something with just one variable as a data set, so usually we need at least two variables. Technically, though, even our very simple &quot;</span><span class="c14">myFamAge</span><span class="c5">&quot; counts as a data set, albeit a very tiny one. </span></li><li class="c0 li-bullet-0"><span class="c5">Later in the book we will install and run the open source &quot;R&quot; data program and learn more about how to create data sets, summarize the information in those data sets, and perform some simple calculations and transformations on those data sets. </span></li></ul><h3 class="c94" id="h.2mdle779dojy"><span class="c38 c57 c23 c18">Chapter Challenge </span></h3><p class="c7"><span class="c5">Discover the meaning of &quot;Boolean Logic&quot; and the rules for &quot;and&quot;, &quot;or&quot;, &quot;not&quot;, and &quot;exclusive or&quot;. Once you have studied this for a while, write down on a piece of paper, without looking, all of the binary operations that demonstrate these rules. </span></p><h3 class="c94" id="h.ubph8tm9db1m"><span class="c23">Sources </span></h3><ul class="c32 lst-kix_qrxoa9lchuen-0 start"><li class="c87"><span class="c83 c18"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Claude_Shannon&amp;sa=D&amp;source=editors&amp;ust=1751550344922380&amp;usg=AOvVaw01PxtD9oSneK9VBb7q98Xo">Claude Shannon</a></span></li><li class="c87"><span class="c83 c18"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Information_theory&amp;sa=D&amp;source=editors&amp;ust=1751550344922547&amp;usg=AOvVaw3s6CufSDUMw7LTqyfIU7JM">Information theory</a></span></li><li class="c125 c145"><span class="c18 c83"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/doc/manuals/R-intro.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550344922678&amp;usg=AOvVaw2f5TPErr7SX5x4vw8bLhGq">An Introduction to R</a></span></li><li class="c87"><span class="c83 c18"><a class="c3" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DOCYZTg3jahU&amp;sa=D&amp;source=editors&amp;ust=1751550344922818&amp;usg=AOvVaw2FNnBWl0zdl_CtLj-PHBWD">Binary Hand Dance by Vi Hart</a></span></li><li class="c87"><span class="c83 c18"><a class="c3" href="https://www.google.com/url?q=http://www.asciitable.com/&amp;sa=D&amp;source=editors&amp;ust=1751550344923019&amp;usg=AOvVaw3vy1UUWnDciqVnkPxbl_v7">ASCII Table and Description</a></span></li><li class="c87"><span class="c83 c18"><a class="c3" href="https://www.google.com/url?q=https://mybinder.org/v2/gh/pearcej/ids-jupyter/master&amp;sa=D&amp;source=editors&amp;ust=1751550344923202&amp;usg=AOvVaw1N0o-oTB9F0n5OgQRGVAZ3">Jupyter Repository for Introduction to Data Science</a></span><span class="c18 c36">&nbsp;</span></li></ul><h3 class="c39" id="h.horedg4b9gc6"><span class="c23">Test Yourself</span></h3><p class="c19"><span class="c38 c57 c23 c58">Question</span><span class="c58 c68">:</span><span class="c38 c57 c23 c58">&nbsp;</span><span class="c5">The smallest unit of information commonly in use in today&rsquo;s computers is called: </span></p><ol class="c32 lst-kix_j08enc9fhspg-0 start" start="1"><li class="c19 c125"><span class="c24 c53 c23">A Bit </span></li><li class="c19 c125"><span class="c24 c53 c23">A Byte </span></li><li class="c125 c160"><span class="c24 c53 c23">A N</span><span class="c53">i</span><span class="c24 c53 c23">bble </span></li><li class="c75"><span class="c24 c53 c23">An Intege</span><span class="c53">r</span></li></ol><p class="c81 c62 title" id="h.v5xlyy7xhl06"><span class="c78">CHAPTER 2 </span><span>&nbsp;<br></span><span class="c23">Identifying Data Problems</span></p><p class="c19 c62"><span class="c46 c40 c23">Data Science is different from other areas such as mathematics or statistics. Data Science is an applied activity and data scientists serve the needs and solve the problems of data users. Before you can solve a problem, you need to identify it and this process is not always as obvious as it might seem. In this chapter, we discuss the identification of data problems.</span><span class="c23">&nbsp;</span></p><p class="c19 c62 c52"><span class="c24 c23 c54"></span></p><p class="c19"><span class="c5">Apple farmers live in constant fear, first for their blossoms and later for their fruit. A late spring frost can kill the blossoms. Hail or extreme wind in the summer can damage the fruit. More generally, farming is an activity that is first and foremost in the physical world, with complex natural processes and forces, like weather, that are beyond the control of humankind. </span></p><p class="c7"><span class="c5">In this highly physical world of unpredictable natural forces, is there any role for data science? On the surface there does not seem to be. But how can we know for sure? Having a nose for identifying data problems requires openness, curiosity, creativity, and a willingness to ask a lot of questions. In fact, if you took away from the first chapter the impression that a data scientist sits in front </span><span class="c14">of a</span><span class="c5">&nbsp;computer all day and works a crazy program like R, that is a mistake. Every data scientist must (eventually) become immersed in the problem domain where she is working. The data scientist may never actually become a farmer, but if you are going to identify a data problem that a farmer has, you have to learn to think like a farmer, to some degree. </span></p><p class="c7"><span class="c5">To get this domain knowledge you can read or watch videos, but the best way is to ask &quot;subject matter experts&quot; (in this case farmers) about what they do. The whole process of asking questions deserves its own treatment, but for now there are three things to think about when asking questions. First, you want the subject matter experts, or SMEs, as they are sometimes called, to tell stories of what they do. Then you want to ask them about anomalies: the unusual things that happen for better or for worse. Finally, you want to ask about risks and uncertainty: what are the situations where it is hard to tell what will happen next and what happens next could have a profound effect on whether the situation ends badly or well. Each of these three areas of questioning reflects an approach to identifying data problems that may turn up something good that could be accomplished with data, information, and the right decision at the right time. </span></p><p class="c7"><span class="c5">The purpose of asking about stories is that people mainly think in stories. From farmers to teachers to managers to CEOs, people know and tell stories about success and failure in their particular domain. Stories are powerful ways of communicating wisdom between different members of the same profession and they are ways of collecting a sense of identity that sets one profession apart from another profession. The only problem is that stories can be wrong. </span></p><p class="c7"><span class="c5">If you can get a professional to tell the main stories that guide how she conducts her work, you can then consider how to verify those stories. Without questioning the veracity of the person that tells the story, you can imagine ways of measuring the different aspects of how things happen in the story with an eye towards eventually verifying (or sometimes debunking) the stories that guide professional work. </span></p><p class="c7"><span class="c5">For example, the farmer might say that in the deep spring frost that occurred five years ago, the trees in the hollow were spared frost damage while the trees around the ridge of the hill had more damage. For this reason, on a cold night the farmer places most of the smudgepots (containers that hold a fuel that creates a smoky fire) around the ridge. The farmer strongly believes that this strategy works, but does it? It would be possible to collect time-series temperature data from multiple locations within the orchard on cold and warm nights, and on nights with and without smudgepots. The data could be used to create a model of temperature changes in the different areas of the orchard and this model could support, improve, or debunk the story. </span></p><p class="c7"><span class="c5">A second strategy for problem identification is to look for the exception cases, both good and bad. A little later in the book we will learn about how the core of classic methods of statistical inference is to characterize &quot;the center&quot; the most typical cases that occur and then examine the extreme cases that are far from the center for information that could help us understand an intervention or an unusual combination of circumstances. Identifying unusual cases is a powerful way of understanding how things work, but it is necessary first to define the central or most typical occurrences in order to have an accurate idea of what constitutes an unusual case. </span></p><p class="c29"><span class="c5">Coming back to our farmer friend, in advance of a thunderstorm late last summer, a powerful wind came through the orchard, tearing the fruit off the trees. Most of the trees lost a small amount of fruit: the dropped apples could be seen near the base of the tree. One small grouping of trees seemed to lose a much larger amount of fruit, however, and the drops were apparently scattered much further from the trees. Is it possible that some strange wind conditions made the situation worse in this one spot? Or is it just a matter of chance that a few trees in the same area all lost a bit more fruit than would be typical. </span></p><p class="c7"><span class="c5">A systematic count of lost fruit underneath a random sample of trees would help to answer this question. The bulk of the trees would probably have each lost about the same amount, but more importantly, that &quot;typical&quot; group would give us a yardstick against which we could determine what would really count as unusual. When we found an unusual set of cases that was truly beyond the </span></p><p class="c19"><span class="c5">limits of typical, we could rightly focus our attention on these to try to understand the anomaly. </span></p><p class="c7"><span class="c5">A third strategy for identifying data problems is to find out about risk and uncertainty. If you read the previous chapter you may remember that a basic function of information is to reduce uncertainty. It is often valuable to reduce uncertainty because of how risk affects the things we all do. At work, at school, at home, life is full of risks: making a decision or failing to do so sets off a chain of events that may lead to something good or something not so good. It is difficult to say, but in general we would like to narrow things down in a way that maximizes the chances of a good outcome and minimizes the chance of a bad one. To do this, we need to make better decisions and to make better decisions we need to reduce uncertainty. By asking questions about risks and uncertainty (and decisions) a data scientist can zero in on the problems that matter. You can even look at the previous two strategies asking about the stories that comprise professional wisdom and asking about anomalies/unusual cases in terms of the potential for reducing uncertainty and risk. </span></p><p class="c7"><span class="c5">In the case of the farmer, much of the risk comes from the weather, and the uncertainty revolves around which countermeasures will be cost effective under prevailing conditions. Consuming lots of expensive oil in smudgepots on a night that turns out to be quite warm is a waste of resources that could make the difference between a profitable or an unprofitable year. So more precise and timely information about local weather conditions might be a key focus area for problem solving with data. What if a live stream of national weather service doppler radar could appear on the farmer&rsquo;s </span><span class="c14">smartphone</span><span class="c5">? Let&rsquo;s build an app for that... </span></p><p class="c7 c52"><span class="c5"></span></p><p class="c81 c62 c52 title" id="h.kcb50zcasxgy"><span class="c59 c21 c57 c84"></span></p><p class="c81 c62 title" id="h.jrwgliq7s2cr"><span class="c70">CHAPTER 3</span><span>&nbsp;<br></span><span class="c23">R</span><span class="c38 c57 c23 c84">, RStudio, and <br>R-Markdown</span></p><p class="c19 c62"><span class="c46 c40"><br>&quot;R&quot; is an </span><span class="c46 c40">open source software</span><span class="c14 c46">&nbsp;programming language designed for statistical computing and graphics. </span><span class="c46 c40">It was </span><span class="c46 c40 c23">developed by volunteers as a service to the community of scientists, researchers, and data </span><span class="c46 c40">scientists and data </span><span class="c46 c40 c23">analysts who now use it. R is free to download and use. Lots of advice and guidance is available online to help users learn R, which is good because it is a powerful and complex </span><span class="c46 c40">language</span><span class="c25 c23">, in reality it is a full featured programming language that is dedicated to understanding data.</span></p><p class="c19 c62 c52"><span class="c25 c23"></span></p><p class="c19 c62"><span class="c14 c46">As an open source program with an active user community, R enjoys constant innovation thanks to the dedicated developers who work on it. One useful innovation was the development of R-Studio, an integrated development environment or IDE for R where one can easily edit code and text as well as run R code and combine the results with text. The cloud-based version of R-Studio is located at </span><span class="c4 c46"><a class="c3" href="https://www.google.com/url?q=https://rstudio.cloud/&amp;sa=D&amp;source=editors&amp;ust=1751550344933659&amp;usg=AOvVaw2ha-ohyOW_kRcJEM1d5syn">https://rstudio.cloud/</a></span><span class="c14 c46">.</span><span class="c14 c46 c23 c64 c98 c97">&nbsp;</span></p><p class="c19 c62 c52"><span class="c14 c46 c23 c64 c98 c97"></span></p><p class="c19 c62"><span class="c5">If you are new to computers, programming, and/or data </span><span class="c14">science, welcome</span><span class="c5">&nbsp;to an exciting chapter that will open the door to the most powerful free data analytics tool ever created anywhere in the universe, no joke. On the other hand, if you are experienced with spreadsheets, statistical analysis, or accounting software you are probably thinking that this book has now gone off the deep end, never to return to sanity and all that is good and right in user interface design. Both perspectives are reasonable. The &quot;R&quot; open source data analysis programming </span><span class="c14">language</span><span class="c5">&nbsp;is immensely powerful, flexible, and especially &quot;extensible&quot; (meaning that people can create new capabilities for it quite easily). At the same time, R is &quot;command line&quot; oriented, meaning that most of the work that one needs to perform is done through carefully crafted text instructions, many of which have tricky </span><span class="c59 c14 c23 c68">syntax </span><span class="c5">(the punctuation, grammar, and related rules for making a command that works). In addition, R is not especially good at giving feedback or error messages that help the user to repair mistakes or figure out what is wrong when results look funny. </span></p><p class="c7"><span class="c5">But there is a method to the madness here. One of the virtues of R as a teaching tool is that it hides very little. The successful user must fully understand what the &quot;data situation&quot; is or else the R commands will not work. With a spreadsheet, it is easy to type in a lot of numbers and a formula like </span><span class="c1">=FORECAST()</span><span class="c5">&nbsp;and a result pops into a cell like magic, whether it makes any sense or not. With R you have to know your data, know what you can do with it, know how it has to be transformed, and know how to check for problems. Because R is a programming language, it forces users to think about problems in terms of data objects, methods that can be applied to those objects, and procedures for applying those methods. These are important metaphors used in modern programming languages, and no data scientist can succeed without having at least a rudimentary understanding of how software is programmed, tested, and integrated into working systems. The extensibility of R means that new modules are being added all the time by volunteers: R was among the first analysis programs to integrate capabilities for drawing data directly from the Twitter</span><span class="c14">&reg;</span><span class="c5">&nbsp;social media platform. So you can be sure that whatever the next big development is in the world of data, that someone in the R community will start to develop a new &quot;package&quot; for R that will make use of it. Finally, the lessons one learns in working with R are almost universally applicable to other programs and environments. If one has mastered R, it is a relatively small step to get the hang of </span><span class="c14">other statistical languages like </span><span class="c5">SAS</span><span class="c14">&reg;</span><span class="c5">&nbsp;statistical programming language or SPSS</span><span class="c14">&reg;</span><span class="c5">. (SAS and SPSS are two of the most widely used commercial statistical analysis programs). So with no need for any licensing fees paid by school, student, or teacher it is possible to learn the most powerful data analysis system in the universe and take those lessons with you no matter where you go. It will take a bit of patience though, so please hang in there! </span></p><p class="c19 c62 c52"><span class="c24 c23 c54"></span></p><p class="c19"><span class="c5">Joseph J. Allaire is an entrepreneur, software engineer, and the originator of some remarkable software products including &quot;ColdFusion,&quot; which was later sold to the web media tools giant Macromedia and Windows Live Writer, a Microsoft blogging tool. Starting in 2009, Allaire began working with a small team to develop an open source program that enhances the usability and power of R. It is called R-Studio. </span></p><h3 class="c94" id="h.le26z03pb3wz"><span>R-studio </span></h3><p class="c7"><span class="c5">R-Studio is an Integrated Development Environment, abbreviated as IDE. Every software engineer knows that if you want to get serious about building something out of code, you must use an IDE because it will make your programming so much easier.</span></p><p class="c7"><span class="c5">Before we start that, let&rsquo;s consider why we need an IDE to work with R. One can run R commands using what is known as the &quot;R console,&quot; but &nbsp;the console is an old technology term that dates back to the days when computers were so big that they each occupied their own air conditioned room. Within that room there was often one &quot;master control station&quot; where a computer operator could do just about anything to control the giant computer by typing in commands. That station was known as the console. The term console is now used in many cases to refer to any interface where you can directly type in commands.</span></p><p class="c7"><span class="c5">It is way too easy to make a mistake to create what computer scientists refer to as a bug if you are doing every little task via the console. Using an IDE, we can easily build reusable pieces of code. The IDE gives us the capability to open up the process of creation, to peer into the component parts when we need to, and to close the hood and hide them when we don&rsquo;t. Because we are working with data, we also need a way of closely inspecting the data, both its contents and its structure. It can get pretty tedious doing this at the R console, where almost every piece of output is a chunk of text and longer chunks scroll off the screen before you can see them. As an IDE for R, R-Studio allows us to control and monitor both our code and our text in a way that supports the creation of reusable elements. </span></p><h3 class="c94" id="h.4e19wsl8305v"><span class="c38 c57 c23 c18">R Markdown</span></h3><p class="c7"><span class="c5">R Markdown is a language that many consider to be a key authoring framework for data science. You can use a single R Markdown file to save and execute code as well as to create high quality reports. It can be used to actually run the code and embed the results in your report right alongside the text that you write.</span></p><p class="c7"><span class="c14">R-Markdown files have an .Rmd extension. It is this file that contains a combination of R Markdown and R code chunks. The .Rmd file is run by pushing a button in R-Studio labelled &quot;knitr,&quot; which stands for &quot;knit R.&quot; When this </span><span class="c14">button</span><span class="c5">&nbsp;is pushed, R-Studio executes all of the R code chunks and combines the results with the text in the document. You can use R-Markdown to create a web page, pdf, MS Word document, slide show, and a whole host of other file types. This may sound a bit complicated, but R-Studio makes it pretty easy.</span></p><h3 class="c94" id="h.l89mi8b9xnmd"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 386.50px; height: 242.32px;"><img alt="" src="images/image36.png" style="width: 386.50px; height: 242.32px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Video: What is R-Markdown"></span><span><br></span><span class="c83"><a class="c3" href="https://www.google.com/url?q=https://vimeo.com/178485416&amp;sa=D&amp;source=editors&amp;ust=1751550344941094&amp;usg=AOvVaw29ypYnlfM9JKcwkTR07IzG">What is R-Markdown</a></span></h3><p class="c19 c62 c52"><span class="c24 c23 c54"></span></p><h3 class="c94" id="h.vhd42b8iaoal"><span class="c38 c57 c23 c18">Using R in R-Studio Cloud</span></h3><p class="c7"><span class="c5">Let&rsquo;s get started. You</span><span class="c14">r instructor has created an R-Studio classroom </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://rstudio.cloud/spaces/64575/join?access_code%3DUcuTmD93NDctYYHzsy92hCw3EvsBllPiI3d9Wc8V&amp;sa=D&amp;source=editors&amp;ust=1751550344941531&amp;usg=AOvVaw0B-EsH5U02_byIs-BnUFU5">R-Studio Cloud CSC100S Intro to Data Science</a></span><span class="c5">. Please follow this link and sign up using your Berea email and your real name.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 266.50px; height: 439.43px;"><img alt="" src="images/image67.png" style="width: 266.50px; height: 439.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Make and account at R-studio Cloud"></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 314.00px; height: 257.00px;"><img alt="" src="images/image8.png" style="width: 314.00px; height: 257.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c52"><span class="c5"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 337.32px;"><img alt="Main screen with IntroToDataScience tab open. The window open read Welcome to IntroToDataScience." src="images/image32.png" style="width: 624.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Main Page of R Studio Cloud"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 338.32px;"><img alt="The Console is open with the &quot;greater than&quot; sign highlighted in red at the bottom of the generic R Version text." src="images/image26.png" style="width: 624.00px; height: 351.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="TryingTytorials Tab Open"></span><span class="c5">&nbsp;</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 142.67px;"><img alt="" src="images/image31.png" style="width: 624.00px; height: 142.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5">&nbsp; </span></p><p class="c7"><span class="c5">Notice near the bottom of the screenshot a greater than (&quot;</span><span class="c1">&gt;</span><span class="c5">&quot;) symbol. This is the command prompt: When R is running and it is the active application on your desktop, if you type a command it appears after the &quot;</span><span class="c1">&gt;</span><span class="c5">&quot; symbol. If you press the &quot;enter&quot; or &quot;return&quot; key, the command is sent to R for processing. When the processing is done, a result may appear just under the &quot;</span><span class="c1">&gt;</span><span class="c5">.&quot; When R is done processing, another command prompt (&quot;&gt;&quot;) appears and R is ready for your next command. </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 558.36px; height: 392.82px;"><img alt="The example of a console screen use with the comman line reading 1 + 1 expression and outputting [1] 2 in the next line." src="images/image43.png" style="width: 558.36px; height: 392.82px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="The Console Screen, 1 + 1 example"></span></p><p class="c7"><span class="c5">In the </span><span class="c14">screenshot</span><span class="c5">, </span><span class="c5">the user has typed &quot;1+1&quot; and pressed the enter key. The formula 1+1 is used by elementary school students everywhere to insult each other&rsquo;s math skills, but R dutifully reports the result as 2. If you are a careful observer, you will notice that just before the 2 there is a &quot;1&quot; in brackets, like this: [1]. That [1] is a line number that helps to keep track of the results that R displays. Pretty pointless when only showing one line of results, but R likes to be consistent, so we will see quite a lot of those numbers in brackets as we dig deeper. </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 273.33px;"><img alt="Example includes a console with a data set assigned to a variable myFamilyAges on which different methods are performed, including mean, sum, and range." src="images/image5.png" style="width: 624.00px; height: 273.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Example of different R functions: sum, mean, range"></span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">Remember the list of ages of family members from the About Data chapter? No? Well, here it is again: 43, 42, 12, 8, 5, for dad, mom, sis, bro, and the dog, respectively. We mentioned that this was a list of items, all of the same mode, namely &quot;integer.&quot; Remember that you can tell that they are OK to be integers because there are no decimal points and therefore nothing after the decimal point. </span><span class="c14">Recalling that &ldquo;c&rdquo; stands for concatenate, w</span><span class="c5">e can create a vector of integers in r using the &quot;</span><span class="c1">c()</span><span class="c5">&quot; command. Take a look at the </span><span class="c14">screenshot</span><span class="c5">&nbsp;just above. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c14">This </span><span class="c5">is just about the last time that the whole screenshot from the R console will appear in the book. From here on out we will just look at commands and output</span><span class="c14">.</span><span class="c5">The first command line in the screenshot is exactly what appeared in an earlier chapter: </span></p><p class="c2"><span class="c1">c(43, 42, 12, 8, 5) </span></p><p class="c19"><span class="c14"><br></span><span class="c5">You may notice that on the following line, R dutifully reports the vector that you just typed. After the line number &quot;</span><span class="c1">[1]</span><span class="c5">&quot;, we see the list 43, 42, 12, 8, and 5. R &quot;echoes&quot; this list back to us, because we didn&rsquo;t ask it to store the vector anywhere. In contrast, the next command line (also the same as in the previous chapter), says: </span></p><p class="c2"><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;&lt;- c(43, 42, 12, 8, 5) </span></p><p class="c29"><span class="c5">We have typed in the same list of numbers, but this time we have assigned it, using the left pointing arrow, into a storage area that we have named &quot;</span><span class="c14 c17">myFamAge</span><span class="c5">.&quot; This time, R responds just with an empty command prompt. That&rsquo;s why the third command line requests a report of what </span><span class="c14">myFamAge</span><span class="c5">&nbsp;contains (Look after the yellow &quot;&gt;&quot;. The text in blue is what you should type.) This is a simple but very important tool. Any time you want to know what is in a data object in R, just type the name of the object and R will report it back to you. In the next command we begin to see the power of</span><span class="c14">&nbsp;</span><span class="c5">R:</span></p><p class="c29"><span class="c59 c17 c36 c123">sum(</span><span class="c123 c17 c36">myFamAge</span><span class="c59 c123 c17 c36">) </span><span class="c59 c123 c17 c36"><br></span></p><p class="c19"><span class="c5">This command asks R to add together all of the numbers in</span></p><p class="c19"><span class="c5">myFamAge, which turns out to be 110 (you can check it yourself</span></p><p class="c19"><span class="c14">if you want). This is perhaps a bit of a weird thing to do with the ages of family members, but it shows how </span><span class="c5">with a very short and simple command you can unleash quite a bit of processing on your data. In the next line we ask for the &quot;mean&quot; (what non-data people call the average) of all of the ages and this turns out to be 22 years. The command right afterwards, called &quot;range,&quot; shows the lowest and highest ages in the list. Finally, just for fun, we tried to issue the command &quot;</span><span class="c1">fish(</span><span class="c14 c17">myFamAge</span><span class="c1">)</span><span class="c5">&quot;. Pretty much as you might expect, R does not contain a &quot;</span><span class="c1">fish()</span><span class="c5">&quot; function and so we received an error message to that effect. This shows another important principle for working with R: You can freely try things out at </span><span class="c14">any time</span><span class="c5">&nbsp;without fear of breaking anything. If R can&rsquo;t understand what you want to accomplish, or you haven&rsquo;t quite figured out how to do something, R will calmly respond with an error message and will not make any other changes until you give it a new command. The error messages from R are not always super helpful, but with some strategies that the book will discuss in future chapters you can break down the problem and figure out how to get R to do what you want. </span></p><p class="c7"><span class="c5">Let&rsquo;s take stock for a moment. First, you should definitely try all of the commands noted above on your own computer. You can read about the commands in this book all you want, but you will learn a lot more if you actually try things out. Second, if you try a command that is shown in these pages and it does not work for some reason, you should try to figure out why. Begin by checking your spelling and punctuation, because R is very persnickety about how commands are typed. Remember that capitalization matters in R: </span><span class="c14">myFamAge</span><span class="c5">&nbsp;is not the same as </span><span class="c14 c17">myFamAge</span><span class="c5">. If you verify that you have typed a command just as you see in the book and it still does not work, try to go online and look for some help. There&rsquo;s lots of help at </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stackoverflow.com&amp;sa=D&amp;source=editors&amp;ust=1751550344949230&amp;usg=AOvVaw1OeFJKdJZ_wiPeaWjAPbBz">Stack Overflow</a></span><span class="c5">, at </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://stat.ethz.ch&amp;sa=D&amp;source=editors&amp;ust=1751550344949409&amp;usg=AOvVaw2xvefdYljFiuR3phDQB6uE">Z&uuml;rich Seminar for Statistics</a></span><span class="c5">, and also at </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.statmethods.net/&amp;sa=D&amp;source=editors&amp;ust=1751550344949504&amp;usg=AOvVaw3x8WhEUaFvhLOj9de3JF_z">Quick-R</a></span><span class="c5">. If you can figure out what </span></p><p class="c19"><span class="c5">went wrong on your own </span><span class="c14">and </span><span class="c5">you will probably learn something very valuable about working with R. Third, you should take a moment to experiment a bit with each new set of commands that you learn. For example, just using the commands discussed earlier in the chapter you could do this totally new thing: </span></p><p class="c2"><span class="c1">myRange &lt;- range(</span><span class="c14 c17">myFamAge</span><span class="c1">) </span></p><p class="c29"><span class="c5">What would happen if you did that command, and then typed</span><span class="c14">&nbsp;</span><span class="c5">&quot;</span><span class="c1">myRange</span><span class="c5">&quot; (without the double quotes) on the next command line to report back what is stored there ? What would you see? Then think about how that worked and try to imagine some other experiments that you could try. The more you experiment on your own, the more you will learn. Some of the best stuff ever invented for computers was the result of just experimenting to see what was possible. At this point, with just the few commands that you have already tried, you already know the following things about R (and about data): </span></p><ul class="c32 lst-kix_f9h15pl7l8ma-0 start"><li class="c7 c13 li-bullet-0"><span class="c5">How to </span><span class="c14">use R in R blocks within R-Studio Cloud.</span></li><li class="c107 c13 li-bullet-0"><span class="c14">How to use</span><span class="c5">&nbsp;the &quot;</span><span class="c1">c()</span><span class="c5">&quot; function</span><span class="c14">. R</span><span class="c5">emember that &quot;</span><span class="c1">c</span><span class="c5">&quot; stands for concatenate, which just means to join things together. You can put a list of items inside the parentheses, separated by commas. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">That a vector is pretty much the most basic form of data storage in R, and that it consists of a list of items of the same</span><span class="c14">&nbsp;type or </span><span class="c5">mode. </span></li><li class="c43 c13 li-bullet-0"><span class="c5">That a vector can be stored in a named location using the assignment arrow (a left pointing arrow made of a dash and a less than symbol, like this: &quot;</span><span class="c1">&lt;-</span><span class="c5">&quot;).</span></li><li class="c0 li-bullet-0"><span class="c5">That you can get a report of the data object that is in any named location just by typing that name</span><span class="c14">&nbsp;in a command block</span><span class="c5">. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">That you can &quot;run&quot; a function, such as</span><span class="c1">&nbsp;mean()</span><span class="c5">, on a vector of numbers to transform them into something else. (The </span><span class="c1">mean()</span><span class="c5">&nbsp;function calculates the average, which is one of the most basic numeric summaries there is.) </span></li><li class="c7 c13 li-bullet-0"><span class="c5">That </span><span class="c1">sum()</span><span class="c5">, </span><span class="c1">mean()</span><span class="c5">, and </span><span class="c1">range(</span><span class="c5">) are all legal functions in R whereas fish() is not. </span></li></ul><p class="c7"><span class="c5">In the next chapter we will move forward a step or two by starting to work with text and by combining our list of family ages with the names of the family members and some other information about them. </span></p><h3 class="c124" id="h.otfmfj5lgebs"><span class="c38 c57 c23 c18">Chapter Challenge </span></h3><p class="c7"><span class="c5">Using logic and online resources to get help if you need it, learn how to use the </span><span class="c1">c()</span><span class="c5">&nbsp;function to add another family member&rsquo;s age on the end of the </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span><span class="c5">vector. </span></p><p class="c7"><span class="c14">Write and test a new function called</span><span class="c14 c17">&nbsp;MySamplingDistribution() </span><span class="c5">that creates a sampling distribution of means from a numeric input vector. You will need to integrate your knowledge of creating new functions from this chapter with your knowledge of creating sampling distributions from the previous chapter in order to create a working function. Make sure to give careful thought about the parameters you will need to pass to your function and what kind of data object your function will return. </span></p><h3 class="c94" id="h.98qgh0fkuz5l"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_i3mfsufx5qs4-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/R_(programming_language)&amp;sa=D&amp;source=editors&amp;ust=1751550344953883&amp;usg=AOvVaw1_IBzeYnY2c1rlw_7uH9eK">R (programming language)</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Joseph_J._Allaire&amp;sa=D&amp;source=editors&amp;ust=1751550344954029&amp;usg=AOvVaw2iLRk-FesbiJpHpeAZIMjY">Wikipedia: Joseph J. Allaire</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.statmethods.net/interface/packages.html&amp;sa=D&amp;source=editors&amp;ust=1751550344954181&amp;usg=AOvVaw0C-LRs_qL5LplvQHh0y_xJ">R-packages</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.youtube.com/watch?v%3D7sAmqkZ3Be8&amp;sa=D&amp;source=editors&amp;ust=1751550344954372&amp;usg=AOvVaw1t5J4bbJV6JM4nCwzUGlRk">Using RStudio as a &#39;Front End&#39; to the R Console</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://a-little-book-of-r-for-biomedical-statistics.readthedocs.org/&amp;sa=D&amp;source=editors&amp;ust=1751550344954622&amp;usg=AOvVaw144n8NIjN0pNSHK6sg2iPd">Welcome to a Little Book of R for Biomedical Statistics!</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/&amp;sa=D&amp;source=editors&amp;ust=1751550344954765&amp;usg=AOvVaw2fSN9H5srEwuZ7ux5glFbO">The Comprehensive R Archive Network</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikibooks.org/wiki/R_Programming&amp;sa=D&amp;source=editors&amp;ust=1751550344954874&amp;usg=AOvVaw11zCcQWYGf91wjB7_HOFyy">R Programming</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stackoverflow.com&amp;sa=D&amp;source=editors&amp;ust=1751550344954972&amp;usg=AOvVaw3i_ud9_W1yMstq1qDu8XlG">Stack Overflow</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://stat.ethz.ch&amp;sa=D&amp;source=editors&amp;ust=1751550344955080&amp;usg=AOvVaw2-qZUHQJsT1QqJ4cYJdFai">Z&uuml;rich Seminar for Statistics</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.statmethods.net/&amp;sa=D&amp;source=editors&amp;ust=1751550344955171&amp;usg=AOvVaw3ZVjNA-Ax661-GGIQX1Li6">Quick-R</a></span><span class="c18">&nbsp;</span></li></ul><p class="c7 c52"><span class="c24 c23 c18"></span></p><h3 class="c39 c104" id="h.vovwjvozmcu9"><span class="c38 c57 c23 c18"></span></h3><p class="c72"><span class="c58 c68">Question: </span><span class="c14">One common definition for the statistical mode is: </span><span class="c53 c68">&nbsp;</span></p><ol class="c32 lst-kix_vgnmkrp5lbgx-0 start" start="1"><li class="c13 c141 li-bullet-0"><span class="c24 c53 c23">The sum of all values divided by the number of values. </span></li><li class="c13 c106 li-bullet-0"><span class="c24 c53 c23">The most frequently occurring value in the data. </span></li><li class="c106 c13 li-bullet-0"><span class="c24 c53 c23">The halfway point through the data. </span></li><li class="c142 c13 li-bullet-0"><span class="c53">The distance between the smallest value and the largest value. </span></li></ol><p class="c19 c52"><span class="c59 c14 c57 c23"></span></p><p class="c19"><span class="c59 c14 c57 c23">R Functions Used in This Chapter </span></p><ul class="c32 lst-kix_63ny8vky5bcr-0 start"><li class="c0 li-bullet-0"><span class="c1">c() </span><span class="c5">Concatenates data elements together into a vector</span></li><li class="c0 li-bullet-0"><span class="c1">&lt;-</span><span class="c5">&nbsp;Assignment arrow <br>e.g. </span><span class="c14 c17 c23 c97">myDa</span><span class="c14 c17">ta &lt;- 5</span><span class="c14">&nbsp;assigns the value of </span><span class="c14 c17">5</span><span class="c14">&nbsp;to </span><span class="c14 c17">myData</span></li><li class="c0 li-bullet-0"><span class="c1">sum() </span><span class="c5">Adds data elements </span></li><li class="c0 li-bullet-0"><span class="c1">range() </span><span class="c5">Min value and max value </span></li><li class="c0 li-bullet-0"><span class="c1">mean() </span><span class="c5">The average </span></li></ul><p class="c7 c52 c110"><span class="c5"></span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19 c52"><span class="c24 c53 c23"></span></p><p class="c19 c52"><span class="c47 c57 c78 c23"></span></p><p class="c19 c52"><span class="c44 c76 c23"></span></p><p class="c81 c62 title" id="h.bz32v4alrc0m"><span class="c21">CHAPTER 4</span><span class="c23">&nbsp;</span><span><br></span><span class="c23">Follow &nbsp;the Data</span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 510.67px;"><img alt="" src="images/image19.png" style="width: 624.00px; height: 510.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Joke image &quot;Department of Redundancy Department&quot;"></span><span class="c25 c23">An old adage in detective work is to, &quot;follow the money.&quot; In data science, one key to success is to &quot;follow the data.&quot; In most cases, a data scientist will not help to design an information system from scratch. Instead, there will be several or many legacy systems where data resides; a big part of the challenge to the data scientist lies in integrating those systems. </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c5">Hate to nag, but have you had a checkup lately? If you have been to the doctor for any reason you may recall that the doctor&rsquo;s office is awash with data. First off, the doctor has loads of digital sensors, everything from blood pressure monitors to ultrasound machines, and all of these produce mountains of data. </span><span class="c5">Perhaps of greater concern in this era </span><span class="c14">is the </span><span class="c5">debate about health insurance</span><span class="c5">, </span><span class="c14">making </span><span class="c5">the </span><span class="c14">doctor&#39;s</span><span class="c5">&nbsp;office one of the big jumping off points for financial and insurance data. One of the notable &quot;features&quot; of the U.S. healthcare system is our most common method of healthcare delivery: paying by the procedure. When you experience a &quot;procedure&quot; at the doctor&rsquo;s office, whether it is a consultation, an examination, a test, or something else, this initiates a chain of data events with far reaching consequences. </span></p><p class="c7"><span class="c5">If your doctor is typical, the starting point of these events is a paper form. Have you ever looked at one of these in detail? Most of the form will be covered by a large matrix of procedures and codes. Although some of the better equipped places may use this form digitally on a tablet or other computer, paper forms are still ubiquitous. Somewhere either in the doctor&rsquo;s office or at an outsourced service company, the data on the paper form are entered into a system that begins the insurance reimbursement and/or billing process. </span></p><p class="c7"><span class="c5">Where do these procedure data go? What other kinds of data (such as patient account information) may get attached to them in a subsequent step? What kinds of networks do these linked data travel over, and what kind of security do they have? How many steps are there in processing the data before they get to the insurance company? How does the insurance company process and analyze the data before issuing the reimbursement? How is the money &quot;trans</span></p><p class="c19"><span class="c5">mitted&quot; once the insurance company&rsquo;s systems have given approval to the reimbursement? These questions barely scratch the surface: there are dozens or hundreds of processing steps that we haven&rsquo;t yet imagined. </span></p><p class="c7"><span class="c5">It is easy to see from this example, that the likelihood of being able to throw it all out and start designing a better or at least more standardized system from scratch is nil. But what if you had the job of improving the efficiency of the system, or auditing the insurance reimbursements to make sure they were compliant with insurance records, or using the data to detect and predict outbreaks and epidemics, or providing feedback to consumers about how much they can expect to pay out of pocket for various procedures? </span></p><p class="c29"><span class="c5">The critical starting point for your project would be to follow the data. You would need to be like a detective, finding out in a substantial degree of detail the content, format, senders, receivers, transmission methods, repositories, and users of data at each step in the process and at each organization where the data are processed or housed. </span></p><p class="c7"><span class="c5">Fortunately there is an extensive area of study and practice called &quot;data modeling&quot; that provides theories, strategies, and tools to help with the data scientist&rsquo;s goal of following the data. These ideas started in earnest in the 1970s with the introduction by computer scientist Ed Yourdon of a methodology called Data Flow Diagrams. A more contemporary approach, that is strongly linked with the practice of creating relational databases, is called the entity relationship model. Professionals using this model develop Entity Relationship Diagrams (ERDs) that describe the structure and movement of data in a system. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">Entity-relationship modeling occurs at different levels ranging from an abstract conceptual level to a physical storage level. At the conceptual level an entity is an object or thing, usually something in the real world. In the doctor&rsquo;s office example, one important &quot;object&quot; is the patient. Another entity is the doctor. The patient and the doctor are linked by a relationship: in modern health care lingo this is the &quot;provider&quot; relationship. If the patient is Mr. X and the doctor is Dr. Y, the provider relationship provides a bidirectional link: </span></p><ul class="c32 lst-kix_h8jpq87mkbro-0 start"><li class="c7 c13 li-bullet-0"><span class="c5">Dr. Y is the provider for Mr. X </span></li><li class="c107 c13 li-bullet-0"><span class="c5">Mr. X&rsquo;s provider is Dr. Y </span></li></ul><p class="c154"><span class="c5">Naturally there is a range of data that can represent Mr. X: name address, age, etc. Likewise, there </span><span class="c14">is</span><span class="c5">&nbsp;data that </span><span class="c14">represents</span><span class="c5">&nbsp;Dr. Y</span><span class="c14">&rsquo;s</span><span class="c5">&nbsp;years of experience as a doctor, specialty areas, certifications, licenses. Importantly, there is also a chunk of data that represents the linkage between X and Y, and this is the relationship. </span></p><p class="c7"><span class="c5">Creating an ERD requires investigating and enumerating all of the entities, such as patients and doctors, as well as all of the relationships that may exist among them. As the beginning of the chapter suggested, this may have to occur across multiple organizations (e.g., the doctor&rsquo;s office and the insurance company) depending upon the purpose of the information system that is being designed. Eventually, the ERDs must become detailed enough that they can serve as a specification for the physical storage in a database. </span></p><p class="c29"><span class="c5">In an application area like health care, there are so many choices for different ways of designing the data that it requires some experience and possibly some &quot;art&quot; to create a workable system. Part of </span></p><p class="c19"><span class="c5">the</span><span class="c5">&nbsp;art lies in understanding the users&rsquo; current information needs and anticipating how those needs may change in the future. If an organization is redesigning a system, adding to a system, or creating brand new systems, they are doing so in the expectation of a future benefit. This benefit may arise from greater efficiency, reduction of errors/inaccuracies, or the hope of providing a new product or service with the enhanced information capabilities. </span></p><p class="c7"><span class="c5">Whatever the goal, the data scientist has an important and difficult challenge of taking the methods of today including paper forms and manual data entry </span><span class="c14">to</span><span class="c5">&nbsp;imag</span><span class="c14">ine</span><span class="c5">&nbsp;the methods of tomorrow. Follow the data! </span></p><p class="c7"><span class="c5">In the next chapter we look at one of the most common and most useful ways of organizing data, namely in a rectangular structure that has rows and columns. This rectangular arrangement of data appears in spreadsheets and databases that are used for a variety of applications. Understanding how these rows and columns are organized is critical to most tasks in data science. </span></p><h3 class="c20" id="h.x1522jrjq0it"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_hxbttwgh00gt-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Data_modeling&amp;sa=D&amp;source=editors&amp;ust=1751550344965379&amp;usg=AOvVaw1tuKwONoNN3YAgCkGoXD3T">Data modeling </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Entity-relationship_diagram&amp;sa=D&amp;source=editors&amp;ust=1751550344965560&amp;usg=AOvVaw3wSvnCPskP02IgsXatYaO5">Entity Relationship Diagram</a></span><span class="c11">&nbsp;</span><span class="c24 c23 c18">&nbsp;</span></li></ul><p class="c81 c62 c52 title" id="h.u2oyl29tb3by"><span class="c59 c21 c57 c84"></span></p><p class="c81 c62 title" id="h.doya5w6bnvld"><span class="c21">CHAPTER 5 </span><span class="c23"><br>Rows and Columns</span></p><p class="c19 c62"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 190.67px;"><img alt="" src="images/image42.png" style="width: 624.00px; height: 190.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Picture of Greek Columns and a Rose"></span><span class="c46 c40 c23">One of the most basic and widely used methods of representing data is to use rows and columns, where each row is a case or instance and each column is a variable and attribute. Most spreadsheets arrange their data in rows and columns, although spreadsheets don&rsquo;t usually refer to these as cases or variables. R represents rows and columns in an object called a </span><span class="c46 c40">D</span><span class="c46 c40 c23">ata </span><span class="c46 c40">F</span><span class="c25 c23">rame. </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c5">Although we live in a three dimensional world, where a box of cereal has height, width, and depth, it is a sad fact of modern life that pieces of paper, chalkboards, whiteboards, and computer screens are still only two dimensional. As a result, most of the statisticians, accountants, computer scientists, and engineers who work with lots of numbers tend to organize them in rows and columns. There&rsquo;s really no good reason for this other than it makes it easy to fill a rectangular piece of paper with numbers. Rows and columns can be organized any way that you want, but the most common way is to have the rows be &quot;cases&quot; or &quot;instances&quot; and the columns be &quot;attributes&quot; or &quot;variables.&quot; Take a look at this nice, two dimensional representation of rows and columns: </span><span class="c14"><br></span></p><table class="c103"><tr class="c51"><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">NAME</span></p></td><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">AGE</span></p></td><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">GENDER</span></p></td><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">WEIGHT</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Dad</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">43</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Male</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">188</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Mom</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">42</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Female</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">136</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Sis</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">12</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Female</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">83</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Bro</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">8</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Male</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">61</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Dog</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">5</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Female</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">44</span></p></td></tr></table><p class="c151"><span class="c5">Pretty obvious what&rsquo;s going on, right? The top line, in bold, is not really part of the data. Instead, the top line contains the attribute or variable names. Note that computer scientists tend to call them attributes while statisticians call them variables. Either term is OK. For example, age is an attribute that every living thing has, and you could count it in minutes, hours, days, months, years, or other units of time. Here we have the Age attribute calibrated in years. Technically speaking, the variable names in the top line are &quot;meta</span><span class="c14">&nbsp;</span><span class="c5">data&quot; or what you could think of as data about data. Imagine how much more difficult it would be to understand what was going on in that table without the metadata. There&rsquo;s a lot</span><span class="c14">&nbsp;</span><span class="c5">of different kinds of metadata: variable names are just one simple type of metadata. </span></p><p class="c7"><span class="c5">So if you ignore the top row, which contains the variable names, each of the remaining rows is an instance or a case. Again, computer scientists may call them instances, and statisticians may call them cases, but either term is fine. The important thing is that each row refers to an actual thing. In this case all of our things are living creatures in a family. You could think of the Name column as &quot;case labels&quot; in that each one of these labels refers to one and only one row in our data. Most of the time when you are working with a large dataset, there is a number used for the case label, and that number is unique for each case (in other words, the same number would never appear in more than one row). Computer scientists sometimes refer to this column of unique numbers as a &quot;</span><span class="c59 c14 c23 c68">key</span><span class="c5">.&quot; A key is very useful particularly for matching things up from different data sources, and we will run into this idea again a bit later. For now, though, just take note that the &quot;Dad&quot; row can be distinguished from the &quot;Bro&quot; row, even though they are both Male. Even if we added an &quot;Uncle&quot; row that had the same Age, Gender, and Weight as &quot;Dad&quot; we would still be able to tell the two rows apart because one would have the name &quot;Dad&quot; and the other would have the name &quot;Uncle.&quot; </span></p><p class="c29"><span class="c5">One other important note: Look how each column contains the same kind of data all the way down. For example, the Age column is all numbers. There&rsquo;s nothing in the Age column like &quot;Old&quot; or &quot;Young.&quot; This is a really valuable way of keeping things organized. After all, we could not run the </span><span class="c1">mean()</span><span class="c5">&nbsp;function on the Age column if it contained a little piece of text, like &quot;Old&quot; or &quot;Young.&quot; On a related note, every cell (that is an intersection of a row and a column, for example, Sis&rsquo;s Age) contains just one piece of information. Although a spreadsheet or a word processing program might allow us to put more than one thing in a cell, a real data handling program will not. Finally, see that every column has the same number of entries, so that the whole forms a nice rectangle. When statisticians and other people who work with databases work with a dataset, they expect this rectangular arrangement. </span></p><p class="c7"><span class="c5">Now let&rsquo;s figure out how to get these rows and columns into R. One thing you will quickly learn about R is that there is almost always more than one way to accomplish a goal. Sometimes the quickest or most efficient way is not the easiest to understand. In this case we will build each column one by one and then join them together into a single data frame. This is a bit labor intensive, and not the usual way that we would work with a data set, but it is easy to understand. First, run this command to make the column of names: </span></p><p class="c2"><span class="c14 c17">myFamName</span><span class="c1">&nbsp;&lt;- c(&quot;Dad&quot;,&quot;Mom&quot;,&quot;Sis&quot;,&quot;Bro&quot;,&quot;Dog&quot;) </span></p><p class="c29"><span class="c5">One thing you might notice is that every name is placed within double quotes. This is how you signal to R that you want it to treat something as </span><span class="c5">a string</span><span class="c5">&nbsp;of characters rather than the name of a storage location. If we had asked R to use Dad instead of &quot;Dad&quot; it would have looked for a storage location (a data object) named Dad. Another thing to notice is that the commas separating the different values are outside of the double quotes. If you were writing a regular sentence this is not how things would look, but for computer programming the comma can only do its job of separating the different values if it is not included inside the quotes. Once you have typed the line above, remember that you can check the contents of </span><span class="c14 c17">myFamName</span><span class="c1">&nbsp;</span><span class="c5">by typing it on the next command line: </span></p><p class="c2"><span class="c14 c17">myFamName</span><span class="c1">&nbsp;</span></p><p class="c29"><span class="c5">The output should look like this: </span></p><p class="c2"><span class="c1">[1] &quot;Dad&quot; &quot;Mom&quot; &quot;Sis&quot; &quot;Bro&quot; &quot;Dog&quot; </span></p><p class="c29"><span class="c5">Next, you can create a vector of the ages of the family members, like this: </span></p><p class="c2"><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;&lt;- c(43, 42, 12, 8, 5) </span></p><p class="c29"><span class="c5">Note that this is exactly the same command we used in the last chapter, so if you have kept R running between then and now you would not even have to retype this command because </span><span class="c14">myFamAge</span><span class="c5">&nbsp;would still be there. Actually, if you closed R since working the examples from the last chapter you will have been prompted to &quot;save the workspace&quot; and if you did so, then R restored all of the data objects you were using in the last session. You can always check by typing </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span><span class="c5">on a blank command line. The output should look like this: </span></p><p class="c10"><span class="c1">[1] 43 42 12 8 5 </span></p><p class="c7"><span class="c5">Hey, now you have used the </span><span class="c1">c() </span><span class="c5">function and the assignment arrow to make </span><span class="c14 c17">myFamName</span><span class="c1">&nbsp;</span><span class="c5">and </span><span class="c14 c17">myFamAge</span><span class="c5">. If you look at the data table earlier in the chapter you should be able to figure out the commands for creating </span><span class="c14 c17">myFamGend</span><span class="c1">&nbsp;</span><span class="c5">and </span><span class="c14 c17">myFamWeight</span><span class="c5">. In case you run into trouble, these commands also appear on the next page, but you should try to figure them out for yourself before you turn the page. In each case after you type the command to create the new data object, you should also type the name of the data</span><span class="c14">&nbsp;</span><span class="c5">object at the command line to make sure that it looks the way it should. Four variables, each one with five values in it. Two of the variables are character data and two of the variables are integer data. Here are those two extra commands in case you need them: </span></p><p class="c10"><span class="c17 c53 c36">myFamGend</span><span class="c47 c17 c23 c36">&nbsp;&lt;- c(&quot;Male&quot;,&quot;Female&quot;,&quot;Female&quot;,&quot;Male&quot;,&quot;Female&quot;) </span></p><p class="c82"><span class="c17 c53 c36">myFamWeight</span><span class="c47 c17 c23 c36">&nbsp;&lt;- c(188,136,83,61,44) </span></p><p class="c92"><span class="c5">Now we are ready to tackle the </span><span class="c14 c17">d</span><span class="c1">ata.frame</span><span class="c5">. In R, a </span><span class="c14 c17">d</span><span class="c1">ata.frame</span><span class="c5">&nbsp;is a list of columns, where each element in the list is a vector. Each vector is the same length, which is how we get our nice rectangular row and column setup, and generally each vector also has its own name. The command to make a data frame is very simple: </span></p><p class="c137"><span class="c14 c17">myFam</span><span class="c1">&nbsp;&lt;- data.frame(</span><span class="c14 c17">myFamName</span><span class="c1">, + </span></p><p class="c137"><span class="c14 c17">myFamAge</span><span class="c1">, </span><span class="c14 c17">myFamGend</span><span class="c1">, </span><span class="c14 c17">myFamWeight</span><span class="c1">) </span></p><p class="c29"><span class="c5">Look out! We&rsquo;re starting to get commands that are long enough that they break onto more than one line. The + at the end of the first line tells R to wait for more input on the next line before trying to process the command. If you want to, you can type the whole thing as one line in R, but if you do, just leave out the plus sign. Anyway, the </span><span class="c1">data.frame()</span><span class="c5">&nbsp;function makes a dataframe from the four vectors that we previously typed in. Notice that we have also used the assignment arrow to make a new stored location where R puts the data frame. This new data object, called </span><span class="c14 c17">myFam</span><span class="c5">, is our dataframe. Once you have gotten that command to work, type </span><span class="c14 c17">myFam</span><span class="c1">&nbsp;</span><span class="c5">at the command line to get a report back of what the data frame contains. Here&rsquo;s the output you should see: </span></p><p class="c29"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myFamName&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myFamAge&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myFamGend&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myFamWeight</span></p><p class="c29"><span class="c1">1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dad&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;43&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Male&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;188</span></p><p class="c29"><span class="c1">2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mom&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;42&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Female&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;136</span></p><p class="c29"><span class="c1">3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Female&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;83</span></p><p class="c29"><span class="c1">4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bro&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Male&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;61</span></p><p class="c29"><span class="c1">5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dog&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Female&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;44</span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 121.33px;"><img alt="This is an example of a created Data Frame in the console that is printed after running the name of the data frame in teh command line." src="images/image61.png" style="width: 624.00px; height: 121.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Data Frame showcase in the Console example"></span></p><p class="c29"><span class="c5">This looks great. Notice that R has put row numbers in front of each row of our data. These are different from the output line numbers we saw in brackets before, because these are actual &quot;indices&quot; into the data frame. In other words, they are the row numbers that R uses to keep track of which row a particular piece of data is in. </span></p><p class="c7"><span class="c5">With a small data set like this one, only five rows, it is pretty easy just to take a look at all of the data. But when we get to a bigger data set this won&rsquo;t be practical. We need to have other ways of summarizing what we have. </span><span class="c14">R&rsquo;s </span><span class="c14 c17">str() </span><span class="c5">method stands for </span><span class="c59 c14 c23 c68">str</span><span class="c5">ucture and compactly reveals the type of &quot;structure&quot; that R has used to store a data object. </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 114.67px;"><img alt="This is an example of what is output in the console after str function was performed on the existing dataframe." src="images/image29.png" style="width: 624.00px; height: 114.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Example of the Str function in R using a Data Frame"></span></p><p class="c2"><span class="c1">&gt; str(</span><span class="c14 c17">myFam</span><span class="c1">) </span></p><p class="c10"><span class="c1">&#39;data.frame&#39;: 5 obs. of 4 variables: </span></p><p class="c10"><span class="c1">$ </span><span class="c14 c17">myFamName</span><span class="c1">&nbsp;: Factor w/ 5 levels &quot;Bro&quot;,&quot;Dad&quot;,&quot;Dog&quot;,..: 2 4 5 1 3 </span></p><p class="c2"><span class="c1">$ </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;: num 43 42 12 8 5 </span></p><p class="c10"><span class="c1">$ </span><span class="c14 c17">myFamGend</span><span class="c1">: Factor w/ 2 levels </span></p><p class="c10"><span class="c1">&quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 1 </span></p><p class="c10"><span class="c1">$ </span><span class="c14 c17">myFamWeight</span><span class="c1">: num 188 136 83 61 44 </span></p><p class="c10 c52"><span class="c1"></span></p><p class="c29"><span class="c5">Take note that for the first time, the example shows the command prompt &quot;</span><span class="c1">&gt;</span><span class="c5">&quot; in order to differentiate the command from the output that follows. You don&rsquo;t need to type this: R provides it whenever it is ready to receive new input. From now on in the book, there will be examples of R commands and output that are mixed together, so always be on the lookout for &quot;</span><span class="c1">&gt;</span><span class="c5">&quot; because the command after that is what you have to type. </span></p><p class="c7"><span class="c5">OK, so the function &quot;</span><span class="c1">str()&quot;</span><span class="c5">&nbsp;reveals the structure of the data object that you name between the parentheses. In this case we pretty well knew that </span><span class="c14">myFam</span><span class="c5">&nbsp;was a data frame because we just set that up in a previous command. In the future, however, we will run into many situations where we are not sure how R has created a data object, so it is important to know </span><span class="c1">str()</span><span class="c5">&nbsp;so that you can ask R to report what an object is at any time. </span></p><p class="c7"><span class="c5">In the first line of output we have the confirmation that </span><span class="c14 c17">myFam</span><span class="c1">&nbsp;</span><span class="c5">is a data frame as well as an indication that there are five observations (&quot;obs.&quot; which is another word that statisticians use instead of cases or instances) and four variables. After that first line of output, we have four sections that each begin with &quot;</span><span class="c1">$</span><span class="c5">&quot;. For each of the four variables, these sections describe the component columns of the </span><span class="c14">myFam</span><span class="c5">&nbsp;dataframe object. </span></p><p class="c7"><span class="c5">Each of the four variables has a &quot;mode&quot; or type that is reported by R right after the colon on the line that names the variable: </span></p><p class="c2"><span class="c1">$ </span><span class="c14 c17">myFamGend</span><span class="c59 c14 c23 c80 c68">: </span><span class="c1">Factor w/ 2 levels </span></p><p class="c29"><span class="c5">For example, </span><span class="c14 c17">myFamGend</span><span class="c1">&nbsp;</span><span class="c5">is shown as a &quot;</span><span class="c1">Factor</span><span class="c5">.&quot; In the terminology that R uses, Factor refers to a special type of label that can be used to identify and organize groups of cases. R has organized these labels alphabetically and then listed out the first few cases (because our dataframe is so small it actually is showing us all of the cases). For </span><span class="c14 c17">myFamGend</span><span class="c1">&nbsp;</span><span class="c5">we see that there are two &quot;levels,&quot; meaning that there are two different options: female and male. </span></p><p class="c19"><span class="c5">R assigns a number, starting with one, to each of these levels, so every case that is &quot;</span><span class="c1">Female</span><span class="c5">&quot; gets assigned a 1 and every case that is &quot;</span><span class="c1">Male</span><span class="c5">&quot; gets assigned a 2 (because Female comes before Male in the alphabet, so Female is the first Factor label, so it gets a 1). If you have your thinking cap on, you may be wondering why we started out by typing in small strings of text, like &quot;</span><span class="c1">Male</span><span class="c5">,&quot; but then R has gone ahead and converted these small pieces of text into numbers that it calls &quot;</span><span class="c1">Factors</span><span class="c5">.&quot; The reason for this lies in the statistical origins of R. For years, researchers have done things like calling an experimental group &quot;</span><span class="c1">Exp</span><span class="c5">&quot; and a control</span><span class="c14">&nbsp;</span><span class="c5">group &quot;</span><span class="c1">Ctl</span><span class="c5">&quot; without intending to use these small strings of text for anything other than labels. So R assumes, unless you tell it otherwise, that when you type in a short string like &quot;</span><span class="c1">Male</span><span class="c5">&quot; that you are referring to the label of a group, and that R should prepare for the use of Male as a &quot;</span><span class="c1">Level</span><span class="c5">&quot; of a &quot;</span><span class="c1">Factor</span><span class="c5">.&quot; When you don&rsquo;t want this to happen you can instruct R to stop doing this with an option on the </span><span class="c1">data.frame()</span><span class="c5">&nbsp;function: </span><span class="c1">stringsAsFactors=FALSE</span><span class="c5">. We will look with more detail at options and defaults a little later on. </span></p><p class="c7"><span class="c5">Phew, that was complicated! By contrast, our two numeric variables, </span><span class="c14">myFamAge</span><span class="c5">&nbsp;and </span><span class="c14">myFamWeight</span><span class="c5">, are very simple. You can see that after the colon the mode is shown as &quot;num&quot; (which stands for numeric) and that the first few values are reported: </span></p><p class="c10"><span class="c1">$ </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;: num 43 42 12 8 5 </span></p><p class="c7"><span class="c5">Putting it all together, we have pretty complete information about the </span><span class="c14">myFam</span><span class="c5">&nbsp;dataframe and we are just about ready to do some more work with it. We have seen firsthand that R has some pretty cryptic labels for things as well as some obscure strategies for converting this to that. R was designed for experts, rather than novices, so we will just have to take our lumps so that one day we can be experts too. </span></p><p class="c7"><span class="c5">Next, we will examine another very useful function called </span><span class="c1">summary()</span><span class="c14">which </span><span class="c5">provides some overlapping information to </span><span class="c1">str() </span><span class="c5">but also goes a little bit further, particularly with numeric variables. </span></p><p class="c7"><span class="c5">Here&rsquo;s what we get: </span></p><p class="c2"><span class="c1">&gt; summary(</span><span class="c14 c17">myFam</span><span class="c1">) </span></p><p class="c10"><span class="c14 c17">myFamName</span><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span></p><p class="c10"><span class="c1">Bro: 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Min. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 5 </span></p><p class="c10"><span class="c1">Dad: 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1st Qu.</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">: 8 </span></p><p class="c10"><span class="c1">Dog: 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Median</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">: 12 </span></p><p class="c10"><span class="c1">Mom: 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mean &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 22 </span></p><p class="c10"><span class="c1">Sis: 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3rd Qu.</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">: 42 </span></p><p class="c100"><span class="c14 c17">myFamGend</span><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c14 c17">myFamWeight</span><span class="c1">&nbsp;</span></p><p class="c2"><span class="c1">Female</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 Min.</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">: 44 </span></p><p class="c10"><span class="c1">Male &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 1st Qu.</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">: 61.0 </span></p><p class="c10 c111 c112"><span class="c1">Median &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 83.0 </span></p><p class="c10 c111 c112"><span class="c1">Mean &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 102.4 </span></p><p class="c10 c111 c112"><span class="c1">3rd Qu. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 136.0 </span></p><p class="c10 c111 c112"><span class="c1">Max &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 188.0 </span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 152.00px;"><img alt="This is an example of what the console outputs when Summmary function is performed on an existing data frame." src="images/image11.png" style="width: 624.00px; height: 152.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Summary function example on a data frame"></span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">In order to fit on the page properly, these columns have been reorganized a bit. The name of a column/variable, sits up above the information that pertains to it, and each block of information is independent of the others (so it is meaningless, for instance, that &quot;</span><span class="c1">Bro: 1</span><span class="c5">&quot; and &quot;</span><span class="c1">Min</span><span class="c5">.&quot; happen to be on the same line of output). Notice, as with </span><span class="c1">str()</span><span class="c5">, that the output is quite different depending upon whether we are talking about a </span><span class="c14">F</span><span class="c5">actor, like </span><span class="c14 c17">myFamName</span><span class="c1">&nbsp;</span><span class="c5">or </span><span class="c14 c17">myFamGend</span><span class="c5">, versus a numeric variable like </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span><span class="c5">and </span><span class="c14 c17">myFamWeight</span><span class="c5">. The columns for the Factors list out a few of the Factor names along with the number of occurrences of cases that are coded with that factor. So for instance, under </span><span class="c14">myFamGend</span><span class="c5">&nbsp;it shows three females and two males. In contrast, for the numeric variables we get five different calculated quantities that help to summarize the variable. There&rsquo;s no time like the present to start to learn about what these are, so here goes: </span></p><ul class="c32 lst-kix_psog4mq2dzvp-0 start"><li class="c7 c13 li-bullet-0"><span class="c5">&quot;</span><span class="c1">Min</span><span class="c5">.&quot; refers to the minimum or lowest value among all the cases. For this dataframe, 5 is the age of the dog and it is the lowest age of all of the family members. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">&quot;</span><span class="c1">1st Qu</span><span class="c5">.&quot; refers to the dividing line at the top of the first quartile. If we took all the cases and lined them up side by side in order of age (or weight) we could then divide up the whole into four groups, where each group had the same number of observations. <br></span></li></ul><table class="c103"><tr class="c51"><td class="c69 c108" colspan="1" rowspan="1"><p class="c19"><span class="c16">1ST QUARTILE</span></p></td><td class="c69 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">2ND QUARTILE</span></p></td><td class="c69 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">3RD QUARTILE</span></p></td><td class="c69 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">4TH QUARTILE</span></p></td></tr><tr class="c120"><td class="c69" colspan="1" rowspan="1"><p class="c27"><span class="c18">25% of cases with the smallest values here</span></p></td><td class="c69" colspan="1" rowspan="1"><p class="c27"><span class="c18">25% of cases just below the median here</span></p></td><td class="c69" colspan="1" rowspan="1"><p class="c27"><span class="c18">25% of cases just above the mean here</span></p></td><td class="c69" colspan="1" rowspan="1"><p class="c27"><span class="c18">25% of cases with the largest values here</span></p></td></tr></table><p class="c19"><span class="c5">Just like a number line, the smallest cases would be on the left with the largest on the right. If we&rsquo;re looking at </span><span class="c14">myFamAge</span><span class="c5">, the leftmost group, which contains one quarter of all the cases, would start with five on the low end (the dog) and would have eight on the high end (Bro). So the &quot;first quartile&quot; is the value of age (or another variable) that divides the first quarter of the cases from the other three quarters. Note that if we don&rsquo;t have a number of cases that divides evenly by four, the value is an approximation. </span></p><ul class="c32 lst-kix_6x6q6459hv9k-0 start"><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c80 c68">Median </span><span class="c5">refers to the value of the case that splits the whole group in half, with half of the cases having higher values and half having lower values. If you think about it a little bit, the median is also the dividing line that separates the second quartile from the third quartile. </span></li><li class="c43 c13 li-bullet-0"><span class="c59 c14 c23 c80 c68">Mean</span><span class="c5">, as we have learned before, is the numeric average of all of the values. For instance, the average age in the family is reported as 22. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">&bull; &quot;</span><span class="c59 c14 c23 c80 c68">3rd Qu.</span><span class="c5">&quot; is the third quartile. If you remember back to the first quartile and the median, this is the third and final dividing line that splits up all of the cases into four equal sized parts. You may be wondering about these quartiles and what they are useful for. Statisticians like them because they give a quick sense of the shape of the distribution. Everyone has the experience of sorting and dividing things up pieces of pizza, playing cards into hands, a bunch of players into teams and it is easy for most people to visualize four equal sized groups and useful to know how high you need to go in age or weight (or another variable) to get to the next dividing line between the groups. </span></li><li class="c0 li-bullet-0"><span class="c5">Finally, &quot;</span><span class="c59 c14 c23 c80 c68">Max</span><span class="c5">&quot; is the maximum value and as you might expect displays the highest value among all of the available cases. For example, in this dataframe Dad has the highest weight: 188. Seems like a pretty trim guy. </span></li></ul><p class="c7"><span class="c5">Just one more topic to pack in before ending this chapter: How to access the stored variables in our new dataframe. R stores the </span><span class="c1">dataframe</span><span class="c5">&nbsp;as a list of vectors and we can use the name of the </span><span class="c1">dataframe </span><span class="c5">together with the name of a vector to refer to each one using the &quot;$&quot; to connect the two labels like this: </span></p><p class="c2"><span class="c1">&gt; </span><span class="c14 c17">myFam</span><span class="c1">$</span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span></p><p class="c10"><span class="c1">[1] 43 42 12 8 5 </span></p><p class="c29"><span class="c5">If you&rsquo;re alert you might wonder why we went to the trouble of typing out that big long thing with the $ in the middle, when we could have just referred to &quot;</span><span class="c14 c17">myFamAge</span><span class="c5">&quot; as we did earlier when we were setting up the data. Well, this is a very important point. When we created the </span><span class="c14">myFam</span><span class="c5">&nbsp;dataframe, we </span><span class="c59 c14 c57 c23">copied </span><span class="c5">all of the information from the individual vectors that we had before into a brand new storage space. So now that we have created the </span><span class="c14">myFam</span><span class="c5">&nbsp;dataframe, </span><span class="c14">myFam</span><span class="c5">$</span><span class="c14">myFamAge</span><span class="c5">&nbsp;actually refers to a completely separate (but so far identical) vector of values. You can prove this to yourself very easily, and you should, by adding some data to the original vector, </span><span class="c14">myFamAge</span><span class="c5">: </span></p><p class="c2"><span class="c1">&gt; </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;&lt;- c(</span><span class="c14 c17">myFamAge</span><span class="c1">, 11) </span></p><p class="c10"><span class="c1">&gt; </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span></p><p class="c10"><span class="c1">[1] 43 42 12 8 5 11 </span></p><p class="c10"><span class="c1">&gt; </span><span class="c14 c17">myFam</span><span class="c1">$</span><span class="c14 c17">myFamAge<br></span></p><p class="c19"><span class="c1">[1] 43 42 12 8 5 </span></p><p class="c19 c52"><span class="c1"></span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 92.00px;"><img alt="This is an example that showcases a new value being added to an independent variable named myFamAge. After, the example prints both myFamAge and myFamAge that was stored in the data frame. The variable have differing values." src="images/image12.png" style="width: 624.00px; height: 92.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Changes in variables example"></span></p><p class="c7"><span class="c5">Look very closely at the five lines above. In the first line, we use the </span><span class="c1">c()</span><span class="c5">&nbsp;command to add the value 11 to the original list of ages that we had stored in </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span><span class="c5">(perhaps we have adopted an older cat into the family). In the second line we ask R to report what the vector </span><span class="c14">myFamAge</span><span class="c5">&nbsp;now contains. Dutifully, on the third line above, R reports that </span><span class="c14 c17">myFamAge</span><span class="c1">&nbsp;</span><span class="c5">now contains the original five values and the new value of 11 on the end of the list. When we ask R to report </span><span class="c14 c17">myFam</span><span class="c1">$</span><span class="c14 c17">myFamAge</span><span class="c5">, however, we still have the original list of five values only. This shows that the dataframe and its component columns/vectors is now a completely independent piece of data. We must be very careful, if we established a dataframe that we want to use for subsequent analysis, that we don&rsquo;t make a mistake and keep using some of the original data from which we assembled the dataframe. </span></p><p class="c7"><span class="c5">Here&rsquo;s a puzzle that follows on from this question. We have a nice dataframe with five observations and four variables. This is a rectangular shaped data set, as we discussed at the beginning of the chapter. What if we tried to add on a new piece of data on the end of one of the variables? In other words, what if we tried something like this command: </span></p><p class="c73"><span class="c17 c18 c36">myFam</span><span class="c8">$</span><span class="c17 c18 c36">myFamAge</span><span class="c8">&nbsp;&lt;- c(</span><span class="c17 c18 c36">myFam</span><span class="c8">$</span><span class="c17 c18 c36">myFamAge</span><span class="c8">, 11) </span></p><p class="c86"><span class="c5">If this worked, we would have a pretty weird situation: The variable in the dataframe that contained the family members&rsquo; ages would all of a sudden have one more observation than the other variables: no more perfect rectangle! Try it out and see what happens. The result helps to illuminate how R approaches situations like this. </span></p><p class="c19"><span class="c5">So what new skills and knowledge do we have at this point? Here are a few of the key points from this chapter: </span></p><ul class="c32 lst-kix_uru4kcognknp-0 start"><li class="c7 c13 li-bullet-0"><span class="c5">In R, as in other programs, a vector is a list of elements/things that are all of the same kind, or what R refers to as a mode. For example, a vector of mode &quot;numeric&quot; would contain only numbers. </span></li><li class="c13 c43 li-bullet-0"><span class="c5">Statisticians, database experts and others like to work with rectangular datasets where the rows are cases or instances and the columns are variables or attributes. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">In R, one of the typical ways of storing these rectangular structures is in an object known as a dataframe. Technically speaking a dataframe is a list of vectors where each vector has the exact same number of elements as the others (making a nice rectangle). </span></li><li class="c7 c13 li-bullet-0"><span class="c5">In R, the </span><span class="c1">data.frame()</span><span class="c5">&nbsp;function organizes a set of vectors into a dataframe. A dataframe is a conventional, rectangular shaped data object where each column is a vector of uniform mode and having the same number of elements as the other columns in the dataframe. Data </span><span class="c14">is</span><span class="c5">&nbsp;copied from the original source vectors into new storage space. The variables/columns of the dataframe can be accessed using &quot;</span><span class="c1">$</span><span class="c5">&quot; to connect the name of the dataframe to the name of the variable/column. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">&bull; The </span><span class="c1">str()</span><span class="c5">&nbsp;and </span><span class="c1">summary()</span><span class="c5">&nbsp;functions can be used to reveal the structure and contents of a dataframe (as well as of other data objects stored by R). The </span><span class="c1">str()</span><span class="c5">&nbsp;function shows the structure of a data object, while </span><span class="c1">summary()</span><span class="c5">&nbsp;provides numerical summaries of numeric variables and overviews of non-numeric variables.</span></li><li class="c0 li-bullet-0"><span class="c5">A </span><span class="c59 c14 c23 c68">factor </span><span class="c5">is a labeling system often used to organize groups of cases or observations. In R, as well as in many other software programs, a factor is represented internally with a numeric ID number, but factors also typically have labels like &quot;Male&quot; and &quot;Female&quot; or &quot;Experiment&quot; and &quot;Control.&quot; Factors always have </span><span class="c14">&quot;levels,&quot; and these are the different groups that the factor signifies. For example, if a factor variable called Gender codes all cases as either &quot;Male&quot; or &quot;Female&quot; or &quot;Other&quot; then that factor has exactly three levels. </span></li></ul><ul class="c32 lst-kix_bfo654b132ys-0 start"><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Quartiles </span><span class="c5">are a division of a sorted vector into four evenly sized groups. The first quartile contains the lowest-valued elements, for example the lightest weights, whereas the fourth quartile contains the highest-valued items. Because there are four groups, there are three dividing lines that separate them. The middle dividing line that splits the vector exactly in half is the median. The term &quot;first quartile&quot; often refers to the dividing line to the left of the median that splits up the lower two quarters and the value of the first quartile is the value of the element of the vector that sits right at that dividing line. Third quartile is the same idea, but to the right of the median and splitting up the two higher quarters. </span></li><li class="c7 c13 li-bullet-0"><span class="c59 c14 c23 c68">Min </span><span class="c5">and </span><span class="c59 c14 c23 c68">max </span><span class="c5">are often used as abbreviations for minimum and maximum and these are the terms used for the highest and lowest values in a vector. Bonus: The &quot;range&quot; of a set of numbers is the maximum minus the minimum. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">The </span><span class="c59 c14 c23 c68">mean </span><span class="c5">is the same thing that most people think of as the average. Bonus: The mean and the median are both measures of what statisticians call &quot;central tendency.&quot; </span></li></ul><h3 class="c94" id="h.6o152nlhgd9m"><span class="c38 c57 c23 c18">Chapter Challenge </span></h3><p class="c29"><span class="c5">Create another variable containing information about family members (for example, each family member&rsquo;s estimated IQ; you can make up the data). Take that new variable and put it in the existin</span><span class="c14">g </span><span class="c14 c17">myFam</span><span class="c1">&nbsp;</span><span class="c5">dataframe. Rerun the </span><span class="c1">summary() </span><span class="c5">function on </span><span class="c14 c17">myFam</span><span class="c1">&nbsp;</span><span class="c5">to get descriptive information on your new variable. </span></p><p class="c7"><span class="c59 c14 c23 c80 c68">Sources </span></p><ul class="c32 lst-kix_ow8lx53hrwrw-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Central_tendency&amp;sa=D&amp;source=editors&amp;ust=1751550345013597&amp;usg=AOvVaw3EWLMjGauGIXK5mBDRxZDN">Central Tendency </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Median&amp;sa=D&amp;source=editors&amp;ust=1751550345013824&amp;usg=AOvVaw2cZdAvkzn-dQZNkOknYYor">Median </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Relational_model&amp;sa=D&amp;source=editors&amp;ust=1751550345014035&amp;usg=AOvVaw1ci2LWrGTzRf_raFVgTAgi">Relational Model</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.burns-stat.com/pages/Tutor/hints_R_begin.html&amp;sa=D&amp;source=editors&amp;ust=1751550345014253&amp;usg=AOvVaw1N6x4L767PmCDjCsbk30CY">Impatient R Translation</a></span></li></ul><h3 class="c41" id="h.77twwx5q5j6m"><span class="c38 c57 c23 c18">R Functions Used in This Chapter </span></h3><ul class="c32 lst-kix_b7fcviwll54m-0 start"><li class="c7 c13 li-bullet-0"><span class="c1">c() </span><span class="c5">Concatenates data elements together to create a vector</span></li><li class="c7 c13 li-bullet-0"><span class="c1">&lt;- </span><span class="c5">Assignment arrow </span></li><li class="c7 c13 li-bullet-0"><span class="c1">data.frame() </span><span class="c5">Makes a dataframe from separate vectors </span></li><li class="c7 c13 li-bullet-0"><span class="c1">str() </span><span class="c5">Reports the structure of a data object </span></li><li class="c7 c13 li-bullet-0"><span class="c1">summary() </span><span class="c5">Reports data modes/types and a data overvie</span><span class="c5">w</span></li></ul><p class="c72"><span class="c58 c68">Question: </span><span class="c5">What is the name of the data object that R uses to store a rectangular dataset of cases and variables? </span></p><ol class="c32 lst-kix_kkxcn2m2pfez-0 start" start="1"><li class="c13 c72 li-bullet-0"><span class="c24 c53 c23">A list </span></li><li class="c106 c13 li-bullet-0"><span class="c24 c53 c23">A mode </span></li><li class="c160 c13 li-bullet-0"><span class="c24 c53 c23">A vector </span></li><li class="c106 c13 li-bullet-0"><span class="c24 c53 c23">A dataframe </span></li></ol><hr style="page-break-before:always;display:none;"><p class="c81 c52 title" id="h.gcxdla7394l9"><span class="c59 c21 c57 c84"></span></p><p class="c81 title" id="h.bh88zao5ir32"><span class="c21">CHAPTER 6 </span><span class="c70"><br></span><span class="c38 c57 c23 c84">Beer, Farms, and Peas </span></p><p class="c19 c62"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 500.67px; height: 375.50px;"><img alt="" src="images/image50.png" style="width: 500.67px; height: 375.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="silly image of peas on x-y axes"></span></p><p class="c19 c62"><span class="c25 c23">Many of the simplest and most practical methods for summarizing collections of numbers come to us from four guys who were born in the 1800s at the start of the industrial revolution. A considerable amount of the work they did was focused on solving real world problems in manufacturing and agriculture by using data to describe and draw inferences from what they observed. </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c5">The end of the 1800s and the early 1900s were a time of astonishing progress in mathematics and science. Given enough time, paper, and pencils, scientists and mathematicians of that age imagined that just about any problem facing humankind including the limitations of people themselves could be measured, broken down, analyzed, and rebuilt to become more efficient. Four Englishmen who epitomized both this scientific progress and these idealistic beliefs were Francis Galton, Karl Pearson, William Sealy Gosset, and Ronald Fisher. </span></p><p class="c7"><span class="c5">First on the scene was Francis Galton, a half-cousin to the more widely known Charles Darwin, but quite the intellectual force himself. Galton was an English gentleman of independent means who studied Latin, Greek, medicine, and mathematics, and who made a name for himself as an African explorer. He is most widely known as a proponent of &quot;eugenics,&quot; and is credited with coining the term. Eugenics is the idea that the human race could be improved through selective breeding. Galton studied heredity in peas, rabbits, and people and concluded that certain people should be paid to get married and have children because their offspring would improve the human race. These ideas were later horribly misused in the 20th century, most notably by the Nazis as a justification for killing people because they belonged to supposedly inferior races. Setting eugenics aside, however, Galton made several notable and valuable contributions to mathematics and statistics, in particular illuminating two basic techniques that are widely used today: correlation and regression. </span></p><p class="c7"><span class="c5">For all his studying and theorizing, Galton was not an outstanding mathematician, but he had a junior partner, Karl Pearson, who is often credited with founding the field of mathematical statistics. </span></p><p class="c19"><span class="c5">Pearson refined the math behind correlation and regression and did a lot else besides to contribute to our modern abilities to manage numbers. Like Galton, Pearson was a proponent of eugenics, but he also is credited with inspiring some of Einstein&rsquo;s thoughts about relativity and was an early advocate of women&rsquo;s rights. </span></p><p class="c7"><span class="c5">Next to the statistical party was William Sealy Gosset, a wizard at both math and chemistry. It was probably the latter expertise that led the Guinness Brewery in Dublin Ireland to hire Gosset after college. As a forward looking business, the Guinness brewery was on the lookout for ways of making batches of beer more consistent in quality. Gosset stepped in and developed what we now refer to as small sample statistical </span><span class="c14">techniques, a way</span><span class="c5">&nbsp;of generalizing from the results of a relatively few observations. Of course, brewing a batch of beer is a time consuming and expensive process, so in order to draw conclusions from experimental methods applied to just a few batches, Gosset had to figure out the role of chance in determining how a batch of beer had turned out. Guinness frowned upon academic publications, so Gosset had to publish his results under the modest pseudonym, &quot;Student.&quot; If you ever hear someone discussing the &quot;Student&rsquo;s t-Test,&quot; that is where the name came from. </span></p><p class="c7"><span class="c5">Last but not least among the born-in-the-1800s bunch was Ronald Fisher, another mathematician who also studied the natural sciences, in his case biology and genetics. Unlike Galton, Fisher was not a gentleman of independent means, in fact, during his early married life he and his wife struggled as subsistence farmers. One of Fisher&rsquo;s professional postings was to an agricultural research farm called Rothhamsted Experimental Station. Here, he had access to data about variations in crop yield that led to his development of an essential statistical technique known as the analysis of variance. Fisher also pioneered the area of experimental design, which includes matters of factors, levels, experimental groups, and control groups that we noted in the previous chapter. </span></p><p class="c7"><span class="c5">Of course, these four are certainly not the only 19th and 20th century mathematicians to have made substantial contributions to practical statistics, but they are notable with respect to the applications of mathematics and statistics to the other sciences (and &quot;Beer, Farms, and Peas&quot; makes a good chapter title as well). </span></p><p class="c7"><span class="c5">One of the critical distinctions woven throughout the work of these four is between the &quot;sample&quot; of data that you have available to analyze and the larger &quot;population&quot; of possible cases that may or do exist. When Gosset ran batches of beer at the brewery, he knew that it was impractical to run every possible batch of beer with every possible variation in recipe and preparation. Gosset knew that he had to run a few batches, describe what he had found and then generalize or infer what might happen in future batches. This is a fundamental aspect of working with all types and amounts of data: Whatever data you have, there&rsquo;s always more out there. There&rsquo;s data that you might have collected by changing the way things are done or the way things are measured. There&rsquo;s future data that hasn&rsquo;t been collected yet and might never be collected. There&rsquo;s even data that we might have gotten using the exact same strategies we did use, but that would have come out subtly different just due to randomness. Whatever data you have, it is just a snapshot or &quot;sample&quot; of what might be out there. This leads us to the conclusion that we can never, ever 100% trust the data we have. We must always hold back and keep in mind that there is always uncertainty in data. A lot of the power and goodness in statistics comes from the capabilities that people like Fisher developed to help us characterize and quantify that uncertainty and for us to know when to guard against putting too much stock in what a sample of data have to say. So remember that while we can always </span><span class="c59 c14 c57 c23">describe </span><span class="c5">the sample of data we have, the real trick is to </span><span class="c59 c14 c57 c23">infer </span><span class="c5">what the data may mean when generalized to the larger population of data that we don&rsquo;t have. This is the key distinction between descriptive and inferential statistics. </span></p><p class="c7"><span class="c5">We have already encountered several descriptive statistics in previous chapters, but for the sake of practice here they are again, this time with the more detailed definitions: </span></p><ul class="c32 lst-kix_dhe74h5eopku-0 start"><li class="c43 c13 li-bullet-0"><span class="c5">The </span><span class="c59 c14 c23 c68">mean </span><span class="c5">(technically the arithmetic mean), a measure of central tendency that is calculated by adding together all of the observations and dividing by the number of observations. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">The </span><span class="c59 c14 c23 c68">median</span><span class="c5">, another measure of central tendency, but one that cannot be directly calculated. Instead, you make a sorted list of all of the observations in the sample, then go halfway up that list. Whatever the value of the observation is at the halfway point, that is the median. </span></li><li class="c43 c13 li-bullet-0"><span class="c5">The </span><span class="c59 c14 c23 c68">range</span><span class="c5">, which is a measure of &quot;dispersion&quot; how spread out a bunch of numbers in a sample are calculated by subtracting the lowest value from the highest value. </span></li></ul><p class="c7"><span class="c5">To this list we should add three more that you will run into in a variety of situations: </span></p><ul class="c32 lst-kix_rkgjrmjh1cjp-0 start"><li class="c7 c13 li-bullet-0"><span class="c5">The </span><span class="c59 c14 c23 c68">mode</span><span class="c5">, another measure of central tendency. The mode is the value that occurs most often in a sample of data. Like the median, the mode cannot be directly calculated. You just have to count up how many of each number there are and then pick the category that has the most. </span></li><li class="c0 li-bullet-0"><span class="c5">The </span><span class="c59 c14 c23 c68">variance</span><span class="c5">, a measure of dispersion. Like the range, the variance describes how spread out a sample of numbers is. Unlike the range, though, which just uses two numbers to calculate dispersion, the variance is obtained from all of the numbers through a simple calculation that compares each number to the mean. If you remember the ages of the family members from the previous chapter and the mean age of 22, you will be able to make sense out of the following table: </span></li></ul><table class="c126"><tr class="c51"><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">WHO</span></p></td><td class="c60 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">AGE</span></p></td><td class="c55 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">AGE-MEAN</span></p></td><td class="c79 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">(AGEMEAN)</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Dad</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c27"><span class="c18">43</span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c27"><span class="c18">43-22=21</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c27"><span class="c18">21*21=441</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Mom</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c27"><span class="c18">42</span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c27"><span class="c18">42-22=20</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c27"><span class="c18">20*20=400</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Sis</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c27"><span class="c18">12</span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c27"><span class="c18">12-22=-10</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c27"><span class="c18">-10*-10=100</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Bro</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c27"><span class="c18">8</span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c27"><span class="c18">8-22=-14</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c27"><span class="c18">-14*-14=196</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">Dog</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c27"><span class="c18">5</span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c27"><span class="c18">5-22=-17</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c27"><span class="c18">-17*-17=289</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c19 c52"><span class="c15"></span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c19 c52"><span class="c15"></span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c27"><span class="c18 c68">Total:</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c27"><span class="c18 c68">1426</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c19 c52"><span class="c15"></span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c19 c52"><span class="c15"></span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c27"><span class="c18 c68">Total/4:</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c27"><span class="c18 c68">356.5</span></p></td></tr></table><p class="c19 c52"><span class="c38 c57 c23 c18"></span></p><p class="c19"><span class="c5">This table shows the calculation of the variance, which begins by obtaining the &quot;deviations&quot; from the mean and then &quot;squares&quot; them (multiply each times itself) to take care of the negative deviations (for example, -14 from the mean for Bro). We add up all of the squared deviations and then divide by the number of observations to get a kind of &quot;average squared deviation.&quot; Note that it was not a mistake to divide by 4 instead of 5 the reasons for this will become clear later in the book when we examine the concept of degrees of freedom. This result is the </span><span class="c59 c14 c23 c68">variance</span><span class="c5">, a very useful mathematical concept that appears all over the place in statistics. While it is mathematically useful, it is not too nice to look at. For instance, in this example we are looking at the 356.5 squared-years of deviation from the mean. Who measures anything in squared years? Squared feet maybe, but that&rsquo;s a different discussion. So, to address this weirdness, statisticians have also provided us with: </span></p><ul class="c32 lst-kix_uccty6vpnn5d-0 start"><li class="c0 li-bullet-0"><span class="c5">The </span><span class="c59 c14 c23 c68">standard deviation</span><span class="c5">, another measure of dispersion, and a cousin to the variance. The standard deviation is simply the square root of the variance, which puts us back in regular units like &quot;years.&quot; In the example above, the standard deviation would be about 18.88 years (rounding to two decimal places, which is plenty in this case). </span></li><li class="c0 li-bullet-0"><span class="c5">Standard deviation has a complicated formula, but you can think of it as &ldquo;the average distance from the mean.&rdquo; In the example above, the average distance to the mean is about 18.88 years. </span></li><li class="c0 li-bullet-0"><span class="c5">Intuitively, you can think about both the standard deviation and the variance as measuring how spread out the data is. When these numbers are 0 or very small compared to the mean, the data is not very spread out.</span></li></ul><p class="c19"><span class="c5">Now let&rsquo;s have R calculate some statistics for us: </span></p><p class="c19"><span class="c1">&gt; var(</span><span class="c14 c17">myFam</span><span class="c1">$</span><span class="c14 c17">myFamAge</span><span class="c1">) </span></p><p class="c19"><span class="c1">[1] 356.5 </span></p><p class="c19"><span class="c1">&gt; sd(</span><span class="c14 c17">myFam</span><span class="c1">$</span><span class="c14 c17">myFamAge</span><span class="c1">) </span></p><p class="c19"><span class="c1">[1] 18.88121 </span></p><p class="c19"><span class="c5">Note that these commands carry on using the data used in the previous chapter, including the use of the $ to address variables within a dataframe. If you do not have the data from the previous chapter you can also do this: </span></p><p class="c19"><span class="c1">&gt; var(c(43,42,12,8,5)) </span></p><p class="c19"><span class="c1">[1] 356.5 </span><span class="c14 c17"><br></span></p><p class="c19"><span class="c1">&gt; sd(c(43,42,12,8,5)) </span></p><p class="c10"><span class="c1">[1] 18.88121 </span></p><p class="c7"><span class="c5">This is a pretty boring example, though, and not very useful for the rest of the chapter, so here&rsquo;s the next step up in looking at data. We will use the Windows or Mac clipboard to cut and paste a larger data set into R. Go to the U.S. Census website where they have stored population data:</span><span class="c14">&nbsp;</span><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html&amp;sa=D&amp;source=editors&amp;ust=1751550345041106&amp;usg=AOvVaw3Wk6lYWI1YO7QcrhKPm1yr">US Census Bureau: State Population Totals: 2010-2019</a></span></p><p class="c7"><span class="c5">Assuming you have a spreadsheet program available, click on the XLS link for &quot;</span><span class="c14"><a class="c3" href="https://www.google.com/url?q=https://www2.census.gov/programs-surveys/popest/tables/2010-2019/state/totals/nst-est2019-01.xlsx&amp;sa=D&amp;source=editors&amp;ust=1751550345041599&amp;usg=AOvVaw1ez1oXePBQA1t0zS6S5fym">Annual Estimates of the Resident Population for the United States, Regions, States, and Puerto Rico: April 1, 2010 to July 1, 2019 (NST-EST2019-01) </a></span><span class="c5">&quot; When the spreadsheet is open, select the population estimates for the fifty states. The first few looked like this in the 2011 data: </span></p><p class="c42"><span class="c59 c17 c23 c18 c71">.Alabama </span><span class="c59 c17 c23 c18 c71">4,799,069</span></p><p class="c42 c52"><span class="c59 c17 c23 c18 c71"></span></p><p class="c42"><span class="c59 c17 c23 c18 c71">.Alaska </span><span class="c59 c17 c23 c18 c71">722,128</span></p><p class="c42 c52"><span class="c59 c17 c23 c18 c71"></span></p><p class="c42"><span class="c59 c17 c23 c18 c71">.Arizona </span><span class="c59 c17 c23 c18 c71">6,472,643</span></p><p class="c42"><span class="c59 c17 c23 c18 c71">&nbsp;</span></p><p class="c42"><span class="c59 c17 c23 c18 c71">.Arkansas </span><span class="c59 c17 c23 c18 c71">2,940,667</span></p><p class="c42"><span class="c59 c17 c23 c18 c71">&nbsp;</span></p><p class="c72"><span class="c5">Before you copy the numbers, take out the commas by switching the cell type to &quot;General.&quot; This can usually be accomplished under the Format menu, but you might also have a toolbar button to do the job. </span></p><p class="c27 c62"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 186.00px; height: 298.96px;"><img alt="" src="images/image34.png" style="width: 186.00px; height: 298.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span class="c5">Generally, you want to keep the names of columns for the data, however, for the purposes of learning just delete everything and only &nbsp;keep population data for the 50 states (and Washington DC). You should only have the numeric data, occupying a total of 51 lines:</span></p><p class="c19 c62"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 557.50px; height: 410.48px;"><img alt="" src="images/image3.png" style="width: 557.50px; height: 410.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span class="c14">Then open R Studio Cloud and </span><span class="c5">Exploring Populations Homework. In the bottom right window click on the Upload button:</span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 221.33px;"><img alt="" src="images/image30.png" style="width: 624.00px; height: 221.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span class="c5">Select the XSL file that you downloaded and edited and upload it. It should now appear in the Files window. Next, click on the file and select Import Dataset</span></p><p class="c29"><span class="c5">You might be prompted to a page that requires installation of extensions to your Cloud environment. Click yes.</span></p><p class="c29"><span class="c5">After installation of readxl and Rcpp packages for smooth integration of XSL files in our environment, you will be prompted to the Import Excel Data window. There are three changes that you will need to make 1. Change the name of the variable to which you are assigning the data from this excel sheet from its original name to USstatePops. 2. Uncheck &ldquo;First Row as Names&rdquo; box as we do not want the population of Alabama over different years to become the name for our columns 3. (optional) Uncheck Open Data Viewer box unless of course you are curious to see the Data Viewer window.</span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 328.00px;"><img alt="" src="images/image41.png" style="width: 624.00px; height: 328.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span class="c5">This is what the Import Excel Data window should look like after:</span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 328.00px;"><img alt="" src="images/image14.png" style="width: 624.00px; height: 328.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c29"><span class="c5">Nice work, you have successfully uploaded your first data set into R Studio Cloud.</span></p><p class="c29"><span class="c5">Only the first three observations are shown in order to save space on this page. Your output R should show the whole list. </span></p><p class="c7"><span class="c5">This would be a great moment to practice your skills from the previous chapter by using the </span><span class="c1">str() </span><span class="c5">and </span><span class="c1">summary()</span><span class="c5">&nbsp;functions on our new data object called </span><span class="c1">USstatePops</span><span class="c5">. Did you notice anything interesting from the results of these functions? One thing you might have noticed is that there are 51 observations instead of 50. Can you guess why? If not, go back and look at your original data from the spreadsheet or the U.S. Census site. The other thing you may have noticed is that USstatePops is a dataframe, and not a plain vector of numbers. You can actually see this in the output above: In the second command line where we request that R reveal what is stored in </span><span class="c1">USstatePops</span><span class="c5">, it responds with a column topped by the designation &quot;</span><span class="c1">V1</span><span class="c5">&quot; or </span><span class="c14">&ldquo;...</span><span class="c14 c17">1</span><span class="c14">&rdquo;</span><span class="c5">. Because we did not give R any information about the numbers it read in from the clipboard, it called them &quot;</span><span class="c1">V1</span><span class="c5">&quot; or </span><span class="c14">&ldquo;...</span><span class="c14 c17">1</span><span class="c14">&rdquo;</span><span class="c5">, short for Variable One, by default. So anytime we want to refer to our list of population numbers we actually have to use the name </span><span class="c1">USstatePops$V1</span><span class="c5">. If this sounds unfamiliar, take another look at the previous &quot;Rows and Columns&quot; chapter for more information on addressing the columns in a dataframe. </span></p><p class="c7"><span class="c5">Now we&rsquo;re ready to have some fun with a good sized list of numbers. Here are the basic descriptive statistics on the population of the states: </span></p><p class="c10"><span class="c1">&gt; mean(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1) </span></p><p class="c10"><span class="c1">[1] 6053834 </span></p><p class="c10"><span class="c1">&gt; median(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1) </span></p><p class="c2"><span class="c1">[1] 4339367 </span></p><p class="c10"><span class="c1">&gt; mode(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1) </span></p><p class="c10"><span class="c1">[1] &quot;numeric&quot; </span></p><p class="c10"><span class="c1">&gt; var(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1) </span></p><p class="c10"><span class="c1">[1] 4.656676e+13 </span></p><p class="c10"><span class="c1">&gt; sd(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1) </span></p><p class="c10"><span class="c1">[1] 6823984 </span></p><p class="c29"><span class="c5">Some great summary information there, but wait, a couple things have gone awry: </span><span class="c24 c23 c18">&nbsp;</span></p><ul class="c32 lst-kix_u1fdsix2vcok-0 start"><li class="c0 li-bullet-0"><span class="c5">The</span><span class="c1">&nbsp;mode() </span><span class="c5">function has returned the data type of our vector of numbers instead of the statistical mode. This is weird but true: the basic R package does not have a statistical mode function! This is partly due to the fact that the mode is only useful in a very limited set of situations, but we will find out in later chapters how add-on packages can be used to get new functions in R including one that calculates the statistical mode. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">The variance is reported as 4.656676e+13. This is the first time that we have seen the use of scientific notation in R. If you haven&rsquo;t seen this notation before, the way you interpret it is to imagine 4.656676 multiplied by 10,000,000,000,000 (also known as 10 raised to the 13</span><span class="c44 c14 c138">th</span><span class="c5">&nbsp;power). You can see that this is ten trillion, a huge and unwieldy number, and that is why scientific notation is used. If you would prefer not to type all of that into a calculator, another trick to see what number you are dealing with is just to move the decimal point 13 digits to the right. </span></li></ul><p class="c7"><span class="c5">Other than these two issues, we now know that the average population of a U.S. state is 6,053,834 with a standard deviation of 6,823,984. You may be wondering, though, what does it mean to have a standard deviation of almost seven million? The mean and standard deviation are OK, and they certainly are mighty precise, but for most of us, it would make much more sense to have a </span><span class="c14 c46 c23 c64 c98 c97">picture </span><span class="c5">that shows the central tendency and the dispersion of a large set of numbers. So here we go. Run this command: </span></p><p class="c10"><span class="c1">hist(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1) </span></p><p class="c7"><span class="c5">Here&rsquo;s the output you should get: </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 523.00px; height: 521.00px;"><img alt="" src="images/image33.png" style="width: 523.00px; height: 521.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of USstatePops$V1"></span></p><p class="c19"><span class="c38 c57 c23 c114">Histogram of USstatePops$</span><span class="c68 c114">...</span><span class="c38 c57 c23 c114">1 </span></p><p class="c153"><span class="c5">A histogram is a specialized type of bar graph designed to show &quot;frequencies.&quot; Frequencies means how often a particular value or range of values occurs in a dataset. This histogram shows a very interesting picture. There are nearly 30 states with populations under five million, another 10 states with populations under 10 million, and then a very small number of states with populations greater than 10 million. Having said all that, how do we glean this kind of information from the graph? First, look along the Y-axis (the vertical axis on the left) for an indication of how often the data occur. The tallest bar is just to the right of this and it is nearly up to the 30 mark. To know what this tall bar represents, look along the X-axis (the horizontal axis at the bottom) and see that there is a tick mark for every two bars. We see scientific notation under each tick mark. The first tick mark is 1e+07, which translates to 10,000,000. So each new bar (or an empty space where a bar would go) goes up by five million in population. With these points in mind it should now be easy to see that there are nearly 30 states with populations under five million. </span></p><p class="c7"><span class="c5">If you think about presidential elections, or the locations of schools and businesses, or how a single U.S. state might compare with other countries in the world, it is interesting to know that there are two really giant states and then lots of much smaller states. Once you have some practice reading histograms, all of the knowledge is available at a glance. </span></p><p class="c7"><span class="c5">On the other hand there is something unsatisfying about this diagram. With over forty of the states clustered into the first couple of bars, there might be some more details hiding in there that we would like to know about. This concern translates into the number of bars shown in the histogram. There are eight shown here, so why did R pick eight? </span></p><p class="c7"><span class="c5">The answer is that the </span><span class="c1">hist()</span><span class="c5">&nbsp;function has an algorithm or recipe for deciding on the number of categories/bars to use by default. The number of observations and the spread of the data and the amount of empty space there would be are all taken into account. Fortunately it is possible and easy to ask R to use more or fewer categories/bars with the &quot;breaks&quot; parameter, like this: </span></p><p class="c2"><span class="c1">hist(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1, breaks=20) </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 523.00px; height: 516.00px;"><img alt="" src="images/image21.png" style="width: 523.00px; height: 516.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of USstatePops$V1 "></span></p><p class="c19"><span class="c38 c57 c23 c114">Histogram of USstatePops$</span><span class="c68 c114">...</span><span class="c38 c57 c23 c114">1 </span></p><p class="c88"><span class="c5">This gives us five bars per tick mark or about two million for each bar. So the new histogram above shows very much the same pattern as before: 15 states with populations under two million. The pattern that you see here is referred to as a distribution. This is a distribution that starts off tall on the left and swoops downward quickly as it moves to the right. You might call this a &quot;reverse-J&quot; distribution because it looks a little like the shape a J makes, although flipped around vertically. More technically this could be referred to as a Pareto distribution (named after the economist Vilfredo P</span><span class="c14">a</span><span class="c5">reto). We don&rsquo;t have to worry about why it may be a Pareto distribution at this stage, but we can speculate on why the distribution looks the way it does. First, you can&rsquo;t have a state with no people in it, or worse yet negative population. It just doesn&rsquo;t make any sense. So a state has to have at least a few people in it, and if you look through U.S. history every state began as a colony or a territory that had at least a few people in it. On the other hand, what does it take to grow really large in population? You need a lot of land, first of all, and then a good reason for lots of people to move there or lots of people to be born there. So there are lots of limits to growth: Rhode Island is too small </span><span class="c5">to</span><span class="c5">&nbsp;have a bazillion people in it and Alaska, although it has tons of land, is too cold for lots of people to want to move there. So all states probably started small and grew, but it is really difficult to grow really huge. As a result we have a distribution where most of the cases are clustered near the bottom of the scale and just a few push up higher and higher. But as you go higher, there are fewer and fewer states that can get that big, and by the time you are out at the end, just shy of 40 million people, there&rsquo;s only one state that has managed to get that big. By the way, do you know or can you guess what that humongous state is? </span></p><p class="c7"><span class="c5">There are lots of other distribution shapes. The most common one that almost everyone has heard of is sometimes called the &quot;bell&quot; curve because it is shaped like a bell. The technical name for this is the normal distribution. The term &quot;normal&quot; was first introduced by Carl Friedrich Gauss (1777-1855), who supposedly called it that in a belief that it was the most typical distribution of data that one might find in natural phenomena. The following histogram depicts the typical bell shape of the normal distribution. </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 523.00px; height: 524.00px;"><img alt="" src="images/image15.png" style="width: 523.00px; height: 524.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of rnorm(51, 6053834, 6823984) "></span></p><p class="c19"><span class="c38 c57 c23 c114">Histogram of </span><span class="c38 c23 c80 c68 c114">rnorm(51, 6053834, 6823984) </span></p><p class="c19 c52"><span class="c38 c57 c23 c114"></span></p><p class="c19"><span class="c5">If you are curious, you might be wondering how R generated the histogram above, and, if you are alert, you might notice that the histogram that appears above has the word &quot;</span><span class="c1">rnorm</span><span class="c5">&quot; in a couple of places. Here&rsquo;s another of the cool features in R: it is incredibly easy to generate &quot;fake&quot; data to work with when solving problems or giving demonstrations. The data in this histogram were generated by R&rsquo;s </span><span class="c1">rnorm()</span><span class="c5">&nbsp;function, which generates a random data set that fits the normal distribution (more closely if you generate a lot of data, less closely if you only have a little). Some further explanation of the </span><span class="c1">rnorm() </span><span class="c5">command will make sense if you remember that the state population data we were using had a mean of 6,053,834 and a standard deviation of 6,823,984. The command used to generate this histogram was: </span></p><p class="c2"><span class="c1">hist(rnorm(51, 6043834, 6823984)) </span></p><p class="c29"><span class="c5">There are two very important new concepts introduced here. The first is a nested function call: The </span><span class="c1">hist()</span><span class="c5">&nbsp;function that generates the graph &quot;surrounds&quot; the </span><span class="c1">rnorm() </span><span class="c5">function that generates the new fake data. (Pay close attention to the parentheses!) The inside function,</span><span class="c1">&nbsp;rnorm()</span><span class="c5">, is run by R first, with the results of that sent directly and immediately into the </span><span class="c1">hist()</span><span class="c5">&nbsp;function. </span></p><p class="c7"><span class="c5">The other important thing is the &quot;arguments that&quot; were &quot;passed&quot; to the </span><span class="c1">rnorm()</span><span class="c5">&nbsp;function. &quot;Argument&quot; is a term used by computer scientists to refer to some extra information that is sent to a function to help it know how to do its job. In this case we passed three arguments to </span><span class="c1">rnorm()</span><span class="c5">&nbsp;that it was expecting in this order: the number of observations to generate in the fake dataset, the mean of the distribution, and the standard deviation of the distribution. The </span><span class="c1">rnorm()</span><span class="c5">&nbsp;function used these three numbers to generate 51 random data points that, roughly speaking, fit the normal distribution. So the data shown in the histogram above are an approximation of what the distribution of state populations might look like if, </span></p><p class="c19"><span class="c5">instead of being reverse-J-shaped (Pareto distribution), they were normally distributed. </span></p><p class="c7"><span class="c5">The normal distribution is used extensively through applied statistics as a tool for making comparisons. For example, look at the rightmost bar in the previous histogram. The label just to the right of that bar is 3e+07, or 30,000,000. We already know from our real state population data that there is only one actual state with a population in excess of 30 million (if you didn&rsquo;t look it up, it is California). So if all of a sudden, someone mentioned to you that he or she lived in a state, </span><span class="c14 c46 c23 c64 c98 c97">other than </span><span class="c5">California, that had 30 million people, you would automatically think to yourself, &quot;Wow, that&rsquo;s unusual and I&rsquo;m not sure I believe it.&quot; And the reason that you found it hard to believe was that you had a distribution to compare it to. Not only did that distribution have a characteristic shape (for example, J-shaped, or bell shaped, or some other shape), it also had a center point, which was the mean, and a &quot;spread,&quot; which in this case was the standard deviation. Armed with those three pieces of information, the type/shape of distribution, an anchoring point, and a spread (also known as the amount of variability), you have a powerful tool for making comparisons. </span></p><p class="c7"><span class="c5">In the next chapter we will conduct some of these comparisons to see what we can infer about the ways things are in general, based on just a subset of available data, or what statisticians call a sample. </span></p><p class="c29"><span class="c59 c14 c57 c23">Chapter Challenge </span></p><p class="c7"><span class="c5">In this chapter, we used </span><span class="c1">rnorm()</span><span class="c5">&nbsp;to generate random numbers that closely fit a normal distribution. We also learned that the state population data was a &quot;Pareto&quot; distribution. Do some research to find out what R function generates random numbers using the Pareto distribution. Then run that function with the correct parameters to generate 51 random numbers (hint: experiment with different probability values). Create a histogram of these random numbers and describe the shape of the distribution. </span></p><p class="c19"><span class="c59 c14 c57 c23">Sources </span></p><ul class="c32 lst-kix_2wuboluazjjj-0 start"><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Carl_Friedrich_Gauss&amp;sa=D&amp;source=editors&amp;ust=1751550345062434&amp;usg=AOvVaw2Ayh71WHoXJV4WwxbmOb-U">Carl Friedrich Gauss </a></span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Francis_Galton&amp;sa=D&amp;source=editors&amp;ust=1751550345062616&amp;usg=AOvVaw2hfpMj7lxIFb1VzzUCSEtC">Francis Galton </a></span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Pareto_distribution&amp;sa=D&amp;source=editors&amp;ust=1751550345062828&amp;usg=AOvVaw3GRik4tzmUmn5U8MWkgeL2">Pareto distribution</a></span><span class="c11">&nbsp;</span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Karl_Pearson&amp;sa=D&amp;source=editors&amp;ust=1751550345063022&amp;usg=AOvVaw3doeiEnJ_HW4cmOXgWGAiD">Karl Pearson</a></span><span class="c11">&nbsp;</span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Ronald_Fisher&amp;sa=D&amp;source=editors&amp;ust=1751550345063208&amp;usg=AOvVaw2SpA2yF9B_IHGvu7SJe0nA">Ronald Fisher </a></span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/William_Sealy_Gosset&amp;sa=D&amp;source=editors&amp;ust=1751550345063420&amp;usg=AOvVaw22QHqPCzpSGCXK5wsx-Rmh">William Sealy Gosset </a></span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Normal_distribution&amp;sa=D&amp;source=editors&amp;ust=1751550345063576&amp;usg=AOvVaw1fNjmYV1raI4T8PJ3XqSYC">Normal Distribution </a></span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html&amp;sa=D&amp;source=editors&amp;ust=1751550345063836&amp;usg=AOvVaw3SjusHfVmwPJ7G01Z2S6a_">US Census: State Population Totals and Components of Change: 2010-2019</a></span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.r-tutor.com/sitemap&amp;sa=D&amp;source=editors&amp;ust=1751550345064008&amp;usg=AOvVaw27pXu_dWixh7B2zf9aBRX9">R Tutorial: Site Map</a></span></li></ul><h3 class="c39" id="h.3dz11dmcd35h"><span class="c38 c57 c23 c18">R Functions Used in This Chapter </span></h3><ul class="c32 lst-kix_vyyozwbuipsu-0 start"><li class="c7 c13 li-bullet-0"><span class="c1">mean() </span><span class="c5">Calculate arithmetic mean </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">median()</span><span class="c14">&nbsp;</span><span class="c5">Locate the median </span></li><li class="c7 c13 li-bullet-0"><span class="c1">mode() </span><span class="c5">Tells the data type/mode of a data object</span><span class="c14">.</span><span class="c14 c68">&nbsp;<br>Important note</span><span class="c14">: Surprisingly, t</span><span class="c5">his </span><span class="c14 c17">mode()</span><span class="c14">&nbsp;is </span><span class="c5">NOT </span><span class="c14">a </span><span class="c5">statistical mode. I</span><span class="c14">n fact, R does not have a standard in-built function to calculate the statistical mode.</span></li><li class="c7 c13 li-bullet-0"><span class="c1">var() </span><span class="c5">Calculate the sample variance </span></li><li class="c7 c13 li-bullet-0"><span class="c1">sd() </span><span class="c5">Calculate the sample standard deviation </span></li><li class="c7 c13 li-bullet-0"><span class="c1">hist() </span><span class="c5">Produces a histogram graphic </span></li></ul><h3 class="c41" id="h.ft4cnhjvasjq"><span class="c38 c57 c23 c18">If All Else Fails </span></h3><p class="c7"><span class="c5">In case you have difficulty with the </span><span class="c1">read.DIF() </span><span class="c5">or </span><span class="c1">read.table() </span><span class="c5">functions, the code shown below can be copied and pasted (or, in the worst case scenario, typed) into the R console to create the data set used in this chapter. </span></p><p class="c19"><span class="c1">V1 &lt;- c(4779736,710231,6392017,2915918,37253956, 5029196,3574097,897934,601723,18801310,9687653, 1360301,1567582,12830632,6483802,3046355,2853118, 4339367,4533372,1328361,5773552,6547629,9883640, 5303925,2967297,5988927,989415,1826341,2700551, 1316470,8791894,2059179,19378102,9535483,672591, 11536504,3751351,3831074,12702379,1052567, 4625364,814180,6346105,25145561,2763885,625741, 8001024,6724540,1852994,5686986,563626) </span></p><p class="c10"><span class="c1">USstatePops &lt;- data.frame(V1)</span><span class="c1">&nbsp;</span></p><p class="c19 c52"><span class="c59 c14 c57 c23"></span></p><p class="c19"><span class="c14 c68">Question: </span><span class="c14">A bar graph that displays the frequencies of occurrence for a numeric variable is called a </span></p><ol class="c32 lst-kix_rf8s22pxlb23-0 start" start="1"><li class="c0 li-bullet-0"><span class="c24 c53 c23">Histogram </span></li><li class="c0 li-bullet-0"><span class="c24 c23 c53">Pictogram </span></li><li class="c0 li-bullet-0"><span class="c24 c53 c23">Bar Graph </span></li><li class="c0 li-bullet-0"><span class="c53">Bar Chart </span></li></ol><p class="c81 c62 title" id="h.einqfoo0ojkf"><span class="c21">CHAPTER 7 </span><span class="c23"><br>Sample in a Jar</span></p><p class="c19 c62"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 196.00px; height: 398.00px;"><img alt="" src="images/image53.png" style="width: 196.00px; height: 398.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="gumball machine"></span></p><p class="c19 c62"><span class="c46 c40 c23">Sampling distributions are the conceptual key to statistical inference. Many approaches to understanding sampling distributions use examples of drawing marbles or gumballs from a large jar to illustrate the influences of randomness on sampling. Using the list of U.S. states illustrates how a non-normal distribution nonetheless has a normal sampling distribution of means. </span></p><p class="c19 c62 c52"><span class="c25 c23"></span></p><p class="c19"><span class="c5">Imagine a gum ball jar full of gumballs of two different colors, red and blue. The jar was filled from a source that provided 100 red gum balls and 100 blue gum balls, but when these were poured into the jar they got all mixed up. If you drew eight gumballs from the jar at random, what colors would you get? If things worked out perfectly, which they never do, you would get four red and four blue. This is half and half, the same ratio of red and blue that is in the jar as a whole. Of course, it rarely works out this way, does it? Instead of getting four red and four blue you might get three red and five blue or any other mix you can think of. In fact, it would be possible, though perhaps not likely, to get eight red gumballs. The basic situation, though, is that we really don&rsquo;t know what mix of red and blue we will get with one draw of eight gumballs. That&rsquo;s uncertainty for you, the forces of randomness affecting our sample of eight gumballs in unpredictable ways. </span></p><p class="c7"><span class="c5">Here&rsquo;s an interesting idea, though, that is no help at all in predicting what will happen in any one sample, but is great at showing what will occur </span><span class="c14 c46 c23 c64 c98 c97">in the long run</span><span class="c5">. Pull eight gumballs from the jar, count the number of red ones and then throw them back. We do not have to count the number of blue because 8 = &nbsp;#red </span><span class="c14">+</span><span class="c5">&nbsp;#blue. Mix up the jar again and then draw eight more gumballs and count the number of red. Keep doing this many times. Here&rsquo;s an example of what you might get: </span><span class="c14"><br></span></p><table class="c103"><tr class="c51"><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">DRAW</span></p></td><td class="c30 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">#RED</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">1</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">5</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">2</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">3</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">3</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">6</span></p></td></tr><tr class="c51"><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">4</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c27"><span class="c18">2</span></p></td></tr></table><p class="c19"><span class="c18 c68"><br></span><span class="c5">Notice that the left column is just counting up the number of sample draws we have done. The right column is the interesting one because it is the count of the number of red gumballs in each particular sample draw. In this example, things are all over the place. In sample draw 4 we only have two red gumballs, but in sample draw 3 we have 6 red gumballs. But the most interesting part of this example is that if you </span><span class="c14 c46 c23 c64 c98 c97">average </span><span class="c5">the number of red gumballs over all of the draws, the average comes out to </span><span class="c14 c46 c23 c64 c98 c97">exactly four red gumballs </span><span class="c5">per draw, which is what we would expect in a jar that is half and half. Now this is a contrived example and we won&rsquo;t always get such a perfect result so quickly, but if you did four thousand draws instead of four, you would get pretty close to the perfect result. </span></p><p class="c29"><span class="c5">This process of repeatedly drawing a subset from a &quot;population&quot; is called &quot;</span><span class="c14">S</span><span class="c59 c14 c23">ampling</span><span class="c5">,&quot; and the end result of doing lots of sampling is a </span><span class="c14">S</span><span class="c5">ampling </span><span class="c14">D</span><span class="c5">istribution. Note that we are using the word population in the previous sentence in its statistical sense to refer to the totality of units from which a sample can be drawn. It is just a coincidence that our dataset contains the number of people in each state and that this value is also referred to as &quot;population.&quot; Next we will get R to help us draw lots of samples from our U.S. state dataset. </span></p><p class="c7"><span class="c5">Conveniently, R has a function called </span><span class="c1">sample()</span><span class="c5">, that will draw a random sample from a data set with just a single call. We can try it now with our state data: </span></p><p class="c10"><span class="c1">&gt; sample(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1,size=16,replace=TRUE) </span></p><p class="c10"><span class="c1">[1] 4533372 19378102 897934 1052567 672591 18801310 2967297 </span></p><p class="c10"><span class="c1">[8] 5029196 </span><span class="c14 c17"><br><br></span><span class="c5">As a matter of practice, note that we called the </span><span class="c1">sample()</span><span class="c5">&nbsp;function with three arguments. The first argument was the data source. For the second and third arguments, rather than rely on the order in which we specify the arguments, we have used &quot;named arguments&quot; to make sure that R does what we wanted. The </span><span class="c1">size=16</span><span class="c5">&nbsp;argument asks R to draw a sample of 16 state data values. The </span><span class="c1">replace=TRUE</span><span class="c5">&nbsp;argument specifies a style of sampling which statisticians use very often to simplify the mathematics of their proofs. For us, sampling with or without replacement does not usually have any practical effects, so we will just go with what the statisticians typically do. </span></p><p class="c43"><span class="c5">When we&rsquo;re working with numbers such as these state values, instead of counting gumball colors, we&rsquo;re more interested in finding out the average, or what you now know as the mean. So we could also ask R to calculate a </span><span class="c1">mean() </span><span class="c5">of the sample for us: </span></p><p class="c2"><span class="c1">&gt; mean(sample(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1,size=16, + replace=TRUE)) </span></p><p class="c10"><span class="c1">[1] 8198359 </span></p><p class="c29"><span class="c5">There&rsquo;s the nested function call again. The output no longer shows the 16 values that R has sampled from the list of 51. Instead it used those 16 values to calculate the mean and display that for us. If you have a good memory, or merely took the time to look in the last chapter, you will remember that the actual mean of our 51 observations is 6,053,834. So the mean that we got from this one sample of 16 states is really not even close to the true mean value of our 51 observations. Are we worried? Definitely not! We know that when we draw a sample, whether it is gumballs or states, we will never hit the true population mean right on the head. We&rsquo;re interested not in any one sample, but in what happens over the long haul. So now we&rsquo;ve got to get R to repeat this process for us, not once, not four times, but four hundred times or four thousand times. Like most programming languages, R has a variety of ways of repeating an activity. One of the easiest ones to use is the</span><span class="c1">&nbsp;replicate() </span><span class="c5">function. To start, let&rsquo;s just try four replications: </span></p><p class="c2"><span class="c1">&gt; replicate(4, </span><span class="c59 c14 c23 c80 c68">mean(sample(USstatePops$</span><span class="c14 c80 c68">...</span><span class="c59 c14 c23 c80 c68">1,+ size=16,replace=TRUE))</span><span class="c1">,simplify=TRUE) </span></p><p class="c10"><span class="c1">[1] 10300486 11909337 8536523 5798488 </span></p><p class="c29"><span class="c5">Couldn&rsquo;t be any easier. We took the exact same command as before, which was a nested function to calculate the </span><span class="c1">mean()</span><span class="c5">&nbsp;of a random sample of 16 states (shown above in bold). This time, we put that command inside the </span><span class="c1">replicate() </span><span class="c5">function so we could run it over and over again. The</span><span class="c1">&nbsp;simplify=TRUE</span><span class="c5">&nbsp;argument asks R to return the results as a simple vector of means, perfect for what we are trying to do. We only ran it four times, so that we would not have a big screen full of numbers. From here, though, it is easy to ramp up to repeating the process four hundred times. You can try that and see the output, but for here in the book we will encapsulate the whole replicate function inside another </span><span class="c1">mean()</span><span class="c5">, so that we can get the average of all 400 of the sample means. Here we go: </span></p><p class="c2"><span class="c1">&gt; </span><span class="c59 c14 c23 c80 c68">mean(</span><span class="c1">replicate(400, mean( + sample(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1,size=16,replace=TRUE)),+ simplify=TRUE)</span><span class="c59 c14 c23 c80 c68">) </span></p><p class="c10"><span class="c1">[1] 5958336 </span></p><p class="c7"><span class="c5">In the command above, the outermost </span><span class="c1">mean()</span><span class="c5">command is bolded to show what is different from the previous command. So, put into</span></p><p class="c19"><span class="c5">that words, this deeply nested command accomplishes the following: a) Draw 400 samples of size n=8 from our full data set of 51 states; b) </span></p><p class="c19"><span class="c5">Calculate the mean from each sample and keep it in a list; c) When finished with the list of 400 of these means, calculate the mean of that list of means. </span><span class="c14">You can see that the mean of four hundred sample means is 5,958,336. </span><span class="c5">Now that is still not the exact value of the whole data set, but it is getting close. We&rsquo;re off by about 95,000, which is roughly an error of about 1.6% (more precisely, 95,498/ 6,053,834 = 1.58%. You may have also noticed that it took a little while to run that command, even if you have a fast computer. There&rsquo;s a lot of work going on there! Let&rsquo;s push it a bit further and see if we can get closer to the true mean for all of our data: </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c1">&gt; mean(replicate(</span><span class="c59 c14 c23 c80 c68">4000</span><span class="c1">, mean( + sample(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1,size=16,replace=TRUE)),+ simplify=TRUE)) </span></p><p class="c19 c52"><span class="c1"></span></p><p class="c19"><span class="c1">[1] 6000972 </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">Now we are even closer! We are now less than 1% away from the true population mean value. Note that the results you get may be a bit different, because when you run the commands, each of the 400 or 4000 samples that is drawn will be slightly different than the ones that were drawn for the commands above. What will not be much different is the overall level of accuracy. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">We&rsquo;re ready to take the next step. Instead of summarizing our whole sampling distribution in a single average, let&rsquo;s look at the distribution of means using a histogram. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">The histogram displays the complete list of 4000 means as frequencies. Take a close look so that you can get more practice reading frequency histograms. This one shows a very typical configuration that is almost bell-shaped, but still has a bit of &quot;skewness&quot; off to the right.The tallest, and therefore most frequent range of values is right near the true mean of 6,053,834.</span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 541.00px; height: 541.00px;"><img alt="" src="images/image16.png" style="width: 541.00px; height: 541.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of replicate(4000, mean(sample(USstatePops$V1, size = 16, replace = TRUE)), simplify = = TRUE) "></span></p><p class="c19 c52"><span class="c24 c23 c132"></span></p><p class="c19 c52"><span class="c24 c23 c132"></span></p><p class="c19"><span class="c5">By the way, were you able to figure out the command to generate this histogram on your own? All you had to do was substitute</span><span class="c1">&nbsp;hist() </span><span class="c5">for the outermost </span><span class="c1">mean()</span><span class="c5">&nbsp;in the previous command. In case you struggled, here it is: </span></p><p class="c19 c62 c52"><span class="c24 c23 c54"></span></p><p class="c19 c62"><span class="c17 c40 c23">hist(replicate(4000, mean( + sample(USstatePops$</span><span class="c17 c40">...</span><span class="c38 c17 c40 c23">1,size=16,replace=TRUE)), + simplify=TRUE)) </span></p><p class="c29"><span class="c5">This is a great moment to take a deep breath. We&rsquo;ve just covered a couple hundred years of statistical thinking in just a few pages. In fact, there are two big ideas, &quot;the law of large numbers&quot; and the </span><span class="c14">&quot;central </span><span class="c5">limit theorem&quot; that we have just partially demonstrated. These two ideas literally took mathematicians like Gerolamo Cardano (1501-1576) and Jacob Bernoulli (1654-1705) several centuries to figure out. If you look these ideas up, you may find a lot of bewildering mathematical details, but for our purposes, there are two really important take-away messages. First, if you run a statistical process a large number of times, it will converge on a stable result. For us, we knew what the average population was of the 50 states plus the District of Columbia. These 51 observations were our population, and we wanted to know how many smaller subsets, or samples, of size n=16 we would have to draw before we could get a good approximation of that true value. We learned that drawing one sample provided a poor result. Drawing 400 samples gave us a mean that was off by 1.5%. Drawing 4000 samples gave us a mean that was off by less than 1%. If we had kept going to 40,000 or 400,000 repetitions of our sampling process, we would have come extremely close to the actual average of 6,053,384. </span></p><p class="c7"><span class="c5">Second, when we are looking at sample means, and we take the law of large numbers into account, we find that the distribution of sampling means starts to create a bell-shaped or normal distribution, and the center of that distribution, the mean of all of those sample means gets really close to the actual population mean. It gets closer faster for larger samples, and in contrast, for smaller samples you have to draw lots and lots of them to get really close. Just for fun, </span><span class="c14">let&#39;s</span><span class="c5">&nbsp;illustrate this with a sample size that is larger than 16. Here&rsquo;s a run that only repeats 100 times, but each time draws a sample of n=51 (equal in size to the population): </span></p><p class="c2"><span class="c1">&gt; mean(replicate(100, mean( + sample(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1,size=51,replace=TRUE)),+ simplify=TRUE)) </span></p><p class="c10"><span class="c1">[1] 6114231</span><span class="c5">&nbsp;</span></p><p class="c7"><span class="c5">Now, we&rsquo;re only off from the true value of the population mean by about one tenth of one percent. You might be scratching your head now, saying, &quot;Wait a minute, isn&rsquo;t a sample of 51 the same thing as the whole list of 51 observations?&quot; This is confusing, but it goes back to the question of sampling with replacement that we examined a couple of pages ago (and that appears in the command above as replace=TRUE). Sampling with replacement means that as you draw out one value to include in your random sample, you immediately chuck it back into the list so that, potentially, it could get drawn again either immediately or later. As mentioned before, this practice simplifies the underlying proofs, and it does not cause any practical problems, other than head scratching. In fact, we could go even higher in our sample size with no trouble: </span></p><p class="c10"><span class="c1">&gt; mean(replicate(100, mean( + sample(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1,size=120,replace=TRUE)), + simplify=TRUE)) </span></p><p class="c10"><span class="c1">[1] 6054718 </span></p><p class="c29"><span class="c5">That command runs 100 replications using samples of size n=120. Look how close the mean of the sampling distribution is to the</span><span class="c14">&nbsp;</span><span class="c5">population mean now! Remember that this result will change a little bit every time you run the procedure, because different random samples are being drawn for each run. But the rule of thumb is that the bigger your sample size, what statisticians call n, the closer your estimate will be to the true value. Likewise, the more trials you run, the closer your population estimate will be. </span></p><p class="c7"><span class="c5">So, if you&rsquo;ve had a chance to catch your breath, let&rsquo;s move on to making use of the sampling distribution. First, let&rsquo;s save one distribution of sample means so that we have a fixed set of numbers to work with: </span></p><p class="c10"><span class="c59 c14 c23 c80 c68">SampleMeans &lt;- </span><span class="c1">replicate(10000, mean(sample(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1,size=5,+ replace=TRUE)</span><span class="c14 c17">)</span><span class="c1">,simplify=TRUE) </span></p><p class="c29"><span class="c5">The bolded part is new. We&rsquo;re saving a distribution of sample means to a new vector called &quot;SampleMeans&quot;. We should have 10,000 of them: </span></p><p class="c2"><span class="c1">&gt; length(SampleMeans) </span></p><p class="c10"><span class="c1">[1] 10000 </span></p><p class="c29"><span class="c5">And the mean of all of these means should be pretty close to our population mean of 6,053,384: </span></p><p class="c2"><span class="c1">&gt; mean(SampleMeans) </span></p><p class="c10"><span class="c1">[1] 6065380 </span></p><p class="c90"><span class="c5">You might also want to run a histogram on SampleMeans and see what the frequency distribution looks like. Right now, all we need to look at is a summary of the list of sample means: </span></p><p class="c90 c52"><span class="c5"></span></p><p class="c19"><span class="c1">&gt; summary(SampleMeans) </span></p><p class="c10"><span class="c1">Min. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1st Qu. Median &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mean &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3rd Qu. Max. </span><span class="c14 c17"><br></span><span class="c1">799100</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">3853000</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">5370000</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">6065000</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">7622000</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">25030000 </span></p><p class="c7"><span class="c5">If you need a refresher on the median and quartiles, take a look back at Chapter 3 Rows and Columns. </span></p><p class="c7"><span class="c5">This summary is full of useful information. First, take a look at the max and the min. The minimum sample mean in the list was 799,100. Think about that for a moment. How could a sample have a mean that small when we know that the true mean is much higher? Rhode Island must have been drawn several times in that sample! The answer comes from the randomness involved in sampling. If you run a process 10,000 times you are definitely going to end up with a few weird examples. Its almost like buying a lottery ticket. The vast majority of tickets are the usual not a winner. Once in a great while, though, there is a very unusual ticket a winner. Sampling is the same: The extreme events are unusual, but they do happen if you run the process enough times. The same goes for the maximum: at 25,030,000 the maximum sample mean is much higher than the true mean. </span></p><p class="c7"><span class="c5">At 5,370,000 the median is quite close to the mean, but not exactly the same because we still have a little bit of rightward skew (the &quot;tail&quot; on the high side is slightly longer than it should be because of the reverse J-shape of the original distribution). The median is very useful because it divides the sample exactly in half: 50%, or exactly 5000 of the sample means are larger than 5,370,000 and the other 50% are lower. So, if we were to draw one more sample from the population it would have a fifty-fifty chance of being above the median. The quartiles help us to cut things up even more finely. The third quartile divides up the bottom 75% from the top 25%. So only 25% of the sample means are higher than 7,622,000. That means if we drew a new sample from the population that there is only a 25% chance that it will be larger than that. Likewise, in the other direction, the first quartile tells us that there is only a 25% chance that a new sample would be less than 3,853,000. </span></p><p class="c7"><span class="c5">There is a slightly different way of getting the same information from R that will prove more flexible for us in the long run. The </span><span class="c1">quantile() </span><span class="c5">function can show us the same information as the median and the quartiles, like this: </span></p><p class="c10"><span class="c1">&gt; quantile(SampleMeans, probs=c(0.25,0.50,0.75)) </span></p><p class="c2"><span class="c1">25%</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">50%</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">75% </span></p><p class="c10"><span class="c1">3853167 5370314 7621871 </span></p><p class="c29"><span class="c5">You will notice that the values are just slightly different, by less than one tenth of one percent, than those produced by the</span><span class="c1">&nbsp;summary() </span><span class="c5">function. These are actually more precise, although the less precise ones from </span><span class="c1">summary() </span><span class="c5">are fine for most purposes. One reason to use </span><span class="c1">quantile() </span><span class="c5">is that it lets us control exactly where we make the cuts. To get quartiles, we cut at 25% (0.25 in the command just above), at 50%, and at 75%. But what if we wanted instead to cut at 2.5% and 97.5%? Easy to do with </span><span class="c1">quantile(): </span></p><p class="c2"><span class="c1">&gt; quantile(SampleMeans, probs=c(0.025,0.975)) </span></p><p class="c10"><span class="c1">2.5%</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">97.5% </span></p><p class="c10"><span class="c1">2014580</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">13537085 </span></p><p class="c29"><span class="c5">So this result shows that, if we drew a new sample, there is only a 2.5% chance that the mean would be lower than 2,014,580. Like</span></p><p class="c19"><span class="c5">wise, there is only a 2.5% chance that the new sample mean would be higher than 13,537,085 (because 97.5% of the means in the sampling distribution are lower than that value). </span></p><p class="c7"><span class="c5">Now let&rsquo;s put this knowledge to work. Here is a sample of the number of people in a certain area, where each of these areas is some kind of a unit associated with the U.S.: </span></p><p class="c7"><span class="c5">3,706,690 159,358 106,405 55,519 53,883 </span></p><p class="c7"><span class="c5">We can easily get these into R and calculate the sample mean: </span></p><p class="c10"><span class="c1">&gt; MysterySample &lt;- c(3706690, 159358, 106405, + </span></p><p class="c33"><span class="c1">55519, 53883) </span></p><p class="c10"><span class="c1">&gt; mean(MysterySample) </span></p><p class="c2"><span class="c1">[1] 816371 </span></p><p class="c29"><span class="c5">The mean of our mystery sample is 816,371. The question is, is this a sample of U.S. states or is it something else? Just on its own it would be hard to tell. The first observation in our sample has more people in it than Kansas, Utah, Nebraska, and several other states. We also know from looking at the distribution of raw population data from our previous example that there are many, many states that are quite small in the number of people. Thanks to the work we&rsquo;ve done earlier in this chapter, however, we have an excellent basis for comparison. We have the sampling distribution of means, and it is fair to say that if we get a new mean to look at, and the new mean is way out in the extreme areas of the sample distribution, say, below the 2.5% mark or above the 97.5% mark, then it seems much less likely that our MysterySample is a sample of states. </span></p><p class="c7"><span class="c5">In this case, we can see quite clearly that 816,371 is on the extreme low end of the sampling distribution. Recall that when we ran the </span><span class="c1">quantile()</span><span class="c5">&nbsp;command we found that only 2.5% of the sample means in the distribution were smaller than 2,014,580. </span></p><p class="c7"><span class="c5">In fact, we could even play around with a more stringent criterion: </span></p><p class="c2"><span class="c1">&gt; quantile(SampleMeans, probs=c(0.005,0.995)) </span></p><p class="c10"><span class="c1">0.5%</span><span class="c14 c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">99.5% </span></p><p class="c10"><span class="c1">1410883 16792211 </span></p><p class="c29"><span class="c5">This </span><span class="c1">quantile()</span><span class="c5">&nbsp;command shows that only 0.5% of all the sample means are lower than 1,410,883. So our </span><span class="c1">MysterySample</span><span class="c5">&nbsp;mean of 816,371 would definitely be a very rare event, if it were truly a sample of states. From this we can infer, tentatively but based on good statistical evidence, that our </span><span class="c1">MysterySample</span><span class="c5">&nbsp;is </span><span class="c14 c46 c23 c64 c98 c97">not </span><span class="c5">a sample of states. The mean of </span><span class="c1">MysterySample</span><span class="c5">&nbsp;is just too small to be very likely to be a sample of states. </span></p><p class="c29"><span class="c5">And this is in fact correct: </span><span class="c1">MysterySample</span><span class="c5">&nbsp;contains the number of people in five different U.S. territories, including Puerto Rico in the Caribbean and Guam in the Pacific. These territories are land masses and groups of people associated with the U.S., but they are not states and they are different in many ways than states. For one thing they are all islands, so they are limited in land mass. Among the U.S. states, only Hawaii is an island, and it is actually bigger than 10 of the states in the continental U.S. The </span><span class="c14">key </span><span class="c5">thing to take away is that </span><span class="c5">the</span><span class="c5">&nbsp;mean of this sample was sufficiently different from a known distribution of means that we could make an inference that the sample </span><span class="c14 c46 c23 c64 c97 c98">was not drawn from the original population of data</span><span class="c5">. </span></p><p class="c7"><span class="c5">This reasoning is the basis for virtually all statistical inference. You construct a comparison distribution, you mark off a zone of extreme values, and you compare any new sample of data you get to the distribution to see if it falls in the extreme zone. If it does, you tentatively conclude that the new sample was obtained from some other source than what you used to create the comparison distribution. </span></p><p class="c7"><span class="c5">If you feel a bit confused, take heart. There&rsquo;s 400-500 years of mathematical developments represented in that one preceding paragraph. Also, before we had cool programs like R that could be used to create and analyze actual sample distributions, most of the material above was taught as a set of formulas and proofs. Yuck! Later in the book we will come back to specific statistical procedures that use the reasoning described above. For now, we just need to take note of three additional pieces of information. </span></p><p class="c7"><span class="c5">First, we looked at the mean of the sampling distribution with </span><span class="c1">mean() </span><span class="c5">and we looked at its shaped with </span><span class="c1">hist()</span><span class="c5">, but we never quantified the spread of the distribution: </span></p><p class="c2"><span class="c1">&gt; sd(SampleMeans) </span></p><p class="c10"><span class="c1">[1] 3037318 </span></p><p class="c29"><span class="c5">This shows us the standard deviation of the distribution of sampling means. Statisticians call this the &quot;standard error of the mean.&quot; This chewy phrase would have been clearer, although longer, if it had been something like this: &quot;the standard deviation of the distribution of sample means for samples drawn from a population.&quot; Unfortunately, statisticians are not known for giving things clear labels. Suffice to say that when we are looking at a distribution and each data point in that distribution is itself a representation of a sample (for example, a mean), then the standard deviation is referred to as the standard error. </span></p><p class="c7"><span class="c5">Second, there is a shortcut to finding out the standard error that does not require actually constructing an empirical distribution of 10,000 (or any other number) of sampling means. It turns out that the standard deviation of the original raw data and the standard error are closely related by a simple bit of algebra: </span></p><p class="c10"><span class="c1">&gt; sd(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1)/sqrt(5) </span></p><p class="c10"><span class="c1">[1] 3051779 </span></p><p class="c7"><span class="c5">The formula in this command takes the standard deviation of the original state data and divides it by the square root of the sample size. Remember three of four pages ago when we created the SampleMeans vector by using the </span><span class="c1">replicate() </span><span class="c5">and </span><span class="c1">sample() </span><span class="c5">commands, a</span><span class="c14">nd </span><span class="c5">that we used a sample size of </span><span class="c1">n=5</span><span class="c5">. That&rsquo;s what you see in the formula above, inside of the </span><span class="c1">sqrt() </span><span class="c5">function. In R, and other software </span><span class="c1">sqrt()</span><span class="c5">&nbsp;is the abbreviation for &quot;square root&quot; and not for &quot;squirt&quot; as you might expect. So if you have a set of observations and you calculate their standard deviation, you can also calculate the standard error for a distribution of means (each of which has the same sample size), just by dividing by the square root of the sample size. You may notice that the number we got with the shortcut was slightly larger than the number that came from the distribution itself, but the difference is not meaningful (and only </span><span class="c14">arises</span><span class="c5">&nbsp;because of randomness in the distribution). Another thing you may have noticed is that the larger the sample size, the smaller the standard error. This leads to an important rule for working with samples: the bigger the better. </span></p><p class="c7"><span class="c5">The last thing is another shortcut. We found out the 97.5% cut point by constructing the sampling distribution and then using quantile to tell us the actual cuts. You can also cut points just using the mean and the standard error. Two standard errors down from the mean is the 2.5% cut point and two standard errors up from the mean is the 97.5% cut point. </span></p><p class="c2"><span class="c1">&gt; StdError&lt;-sd(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1)/sqrt(5) </span></p><p class="c10"><span class="c1">&gt; CutPoint975&lt;-mean(USstatePops$</span><span class="c14 c17">...</span><span class="c1">1)+(2 * StdError) </span></p><p class="c10"><span class="c1">&gt; CutPoint975 </span></p><p class="c10"><span class="c1">[1] 12157391 </span></p><p class="c29"><span class="c5">You will notice again that this value is different from what we calculated with the </span><span class="c1">quantile()</span><span class="c5">&nbsp;function using the empirical distribution. The differences arise because of the randomness in the distribution that we constructed. The value above is an estimate that is based on statistical proofs, whereas the empirical SampleMeans list that we constructed is just one of a nearly infinite range of such lists that we could create. We could easily reduce the discrepancy between the two methods by using a larger sample size and by having more replications included in the sampling distribution. </span></p><p class="c7"><span class="c5">To summarize, with a data set that includes 51 data points with the numbers of people in states, and a bit of work using R to construct a distribution of sampling means, we have learned the following: <br></span></p><ul class="c32 lst-kix_r1iuztannmt0-0 start"><li class="c0 li-bullet-0"><span class="c5">Run a statistical process a large number of times and you get a consistent pattern of results. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Taking the means of a large number of samples and plotting them on a histogram shows that the sample means are fairly well normally distributed and that the center of the distribution is very, very close to the mean of the original raw data. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">This resulting distribution of sample means can be used as a basis for comparisons. By making cut points at the extreme low and high ends of the distribution, for example 2.5% and 97.5%, we have a way of comparing any new information we get. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">If we get a new sample mean, and we find that it is in the extreme zone defined by our cut points, we can tentatively conclude that the sample that made that mean is a different kind of thing than the samples that made the sampling distribution. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">A shortcut and more accurate way of figuring the cut points involves calculating the &quot;standard error&quot; based on the standard deviation of the original raw data. </span></li></ul><p class="c7"><span class="c5">We&rsquo;re not statisticians at this point, but the process of reasoning based on sampling distributions is at the heart of inferential statistics, so if you have followed the logic presented in this chapter, you have made excellent progress towards being a competent user of applied statistics. </span></p><h3 class="c20" id="h.yfzs0si0nwri"><span class="c38 c57 c23 c18">Chapter Challenge </span></h3><p class="c29"><span class="c5">Collect a sample consisting of at least 20 data points and construct a sampling distribution. Calculate the standard error and use this </span></p><p class="c19"><span class="c5">to calculate the 2.5% and 97.5% distribution cut points. The data points you collect should represent instances of the same phenomenon. For instance, you could collect the prices of 20 textbooks, or count the number of words in each of 20 paragraphs. </span></p><h3 class="c94" id="h.qj0jydo1jif8"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_2w83y07jwups-0 start"><li class="c7 c13 li-bullet-0"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Central_limit_theorem&amp;sa=D&amp;source=editors&amp;ust=1751550345110077&amp;usg=AOvVaw3QHQrVPuJjxxy3mY5F2k0K">http://en.wikipedia.org/wiki/Central_limit_theorem </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Gerolamo_Cardano&amp;sa=D&amp;source=editors&amp;ust=1751550345110398&amp;usg=AOvVaw22_RfICV34GUIcVQxDTWXd">http://en.wikipedia.org/wiki/Gerolamo_Cardano</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Jacob_Bernoulli&amp;sa=D&amp;source=editors&amp;ust=1751550345110673&amp;usg=AOvVaw3-BeQzsyK8Cw0A1rl1kg5s">http://en.wikipedia.org/wiki/Jacob_Bernoulli </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Law_of_large_numbers&amp;sa=D&amp;source=editors&amp;ust=1751550345110947&amp;usg=AOvVaw3zI2iPHncA8eT2fVuAiSwX">http://en.wikipedia.org/wiki/Law_of_large_numbers </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/List_of_U.S._states_and_territories&amp;sa=D&amp;source=editors&amp;ust=1751550345111282&amp;usg=AOvVaw2-vlyFXH3vNCP57cun8mH1">http://en.wikipedia.org/wiki/List_of_U.S._states_and_territories _by_population </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://www.khanacademy.org/math/statistics/v/central-limit-th&amp;sa=D&amp;source=editors&amp;ust=1751550345111679&amp;usg=AOvVaw0t1j7N6k6v9lBgWzzjfrnf">http://www.khanacademy.org/math/statistics/v/central-limit-th eorem</a></span><span class="c11">&nbsp;</span></li></ul><h3 class="c20" id="h.vl79rh4bnsg4"><span class="c38 c57 c23 c18">R Commands Used in This Chapter </span></h3><ul class="c32 lst-kix_4koh223rnoq-0 start"><li class="c7 c13 li-bullet-0"><span class="c1">length() </span><span class="c5">The number of elements in a vector </span></li><li class="c7 c13 li-bullet-0"><span class="c1">mean() </span><span class="c5">The arithmetic mean or average of a set of values </span></li><li class="c7 c13 li-bullet-0"><span class="c1">quantile() </span><span class="c5">Calculates cut points based on</span><span class="c14">&nbsp;</span><span class="c5">percents/proportions </span></li><li class="c105 c13 li-bullet-0"><span class="c1">sample() </span><span class="c5">Chooses elements at random from a vector </span></li><li class="c7 c13 li-bullet-0"><span class="c1">sd() </span><span class="c5">Calculates standard deviation </span></li><li class="c0 li-bullet-0"><span class="c1">replicate() </span><span class="c5">Runs an expression/calculation many times </span></li><li class="c0 li-bullet-0"><span class="c1">sqrt() </span><span class="c5">Calculates square root </span></li><li class="c7 c13 li-bullet-0"><span class="c1">summary() </span><span class="c5">Summarizes contents of a vector </span></li></ul><p class="c19 c52"><span class="c44 c76 c23"></span></p><p class="c81 c62 title" id="h.l2axcvjdrknt"><span class="c21">CHAPTER 8 </span><span class="c23">&nbsp;<br>Big Data? Big Deal<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 480.00px; height: 320.00px;"><img alt="" src="images/image1.png" style="width: 480.00px; height: 320.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c19 c62"><span class="c46 c40">T</span><span class="c46 c40 c23">he technology press contain</span><span class="c46 c40">s</span><span class="c25 c23">&nbsp;many headlines about big data. What makes data big, and why is this bigness important? In this chapter, we discuss some of the real issues behind these questions. Armed with information from the previous chapter concerning sampling, we can give more thought to how the size of a data set affects what we do with the data. </span></p><p class="c19"><span class="c18"><br></span><span class="c5">MarketWatch (a Wall Street Journal Service) published an article with the title, &quot;Big Data Equals Big Business Opportunity Say Global IT and Business Professionals,&quot; and the subtitle, &quot;70 Percent of Organizations Now Considering, Planning or Running Big Data Projects According to New Global Survey.&quot; The technology news has been full of similar articles for several years. Given the number of such articles it is hard to resist the idea that &quot;big data&quot; represents some kind of revolution that has turned the whole world of information and technology topsy-turvy. But is this really true? Does &quot;big data&quot; change everything? </span></p><p class="c7"><span class="c5">Business analyst Doug Laney suggested that three characteristics make &quot;big data&quot; different from what came before: volume, velocity, and variety. Volume refers to the sheer amount of data. Velocity focuses on how quickly data arrives as well as how quickly those data become &quot;stale.&quot; Finally, Variety reflects the fact that there may be many different kinds of data. Together, these three characteristics are often referred to as the &quot;three Vs&quot; model of big data. Note, however, that even before the dawn of the computer age we&rsquo;ve had a variety of data, some of which arrives quite quickly, and that can add up to quite a lot of total storage over time (think, for example, of the large variety and volume of data that has arrived annually at Library of Congress since the 1800s!). So it is difficult to tell, just based on someone saying that they have a high volume, high velocity, and high variety data problem, that big data is fundamentally a brand new thing. </span></p><p class="c7"><span class="c5">With that said, there are certainly many changes afoot that make data problems qualitatively different today as compared with a few years ago. Let&rsquo;s list a few things which are pretty accurate: <br></span></p><ol class="c32 lst-kix_wfbxhn4b3sc5-0 start" start="1"><li class="c0 li-bullet-0"><span class="c5">The decline in the price of sensors (like barcode readers) and other technology over recent decades has made it cheaper and easier to collect a lot more data. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Similarly, the declining cost of storage has made it practical to keep lots of data hanging around, regardless of its quality or usefulness. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Many people&rsquo;s attitudes about privacy seem to have</span><span class="c14">&nbsp;</span><span class="c5">accommodated the use of Facebook and other platforms where we reveal lots of information about ourselves. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Researchers have made significant advances in the &quot;machine learning&quot; algorithms that form the basis of many data mining techniques. </span></li><li class="c29 c13 li-bullet-0"><span class="c5">When a data set gets to a certain size (into the range of thousands of rows), conventional tests of statistical significance are meaningless, because even the most tiny and trivial results (or effect sizes, as statisticians call them) are statistically significant. </span></li></ol><p class="c7"><span class="c5">Keeping these points in mind, there are also a number of things that have not changed throughout the years: </span></p><ol class="c32 lst-kix_dy505sosxomp-0 start" start="1"><li class="c7 c13 li-bullet-0"><span class="c5">Garbage in, garbage out: The usefulness of data depends heavily upon how carefully and well it was collected. After data were collected, the quality depends upon how much attention was paid to suitable pre-processing: data cleaning and data screening. </span></li><li class="c43 c13 li-bullet-0"><span class="c14">B</span><span class="c5">igger equals weirder: If you are looking for </span><span class="c14">anomalies, rare</span><span class="c5">&nbsp;events that break the rules</span><span class="c14">, </span><span class="c5">then larger is better. Low frequency events often do not appear until a data collection goes on for a long time and/or encompasses a large enough group of instances to contain one of the bizarre cases. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Linking adds potential: Standalone datasets are inherently limited by whatever variables are available. But if those data can be linked to some other data, all of a sudden new vistas may open up. No guarantees, but the more you can connect records here to other records over there, the more potential findings you have. </span></li></ol><p class="c7"><span class="c5">Items on both of the lists above are considered pretty commonplace and uncontroversial. Taken together, however, they do shed some light on the question of how important &quot;big data&quot; might be. We have had lots of historical success using conventional statistics to examine modestly sized (i.e., 1000 rows or less) datasets for statistical regularities. Everyone&rsquo;s favorite basic statistic, the Student&rsquo;s t-test, is essential a test for differences in the central tendency of two groups. If the data contain regularities such that one group is notably different from another group, a t-test shows it to be so. </span></p><p class="c7"><span class="c5">Big data does not help us with these kinds of tests. We don&rsquo;t even need a thousand records for many conventional statistical comparisons, and having a million or a hundred million records won&rsquo;t make our job any easier (it will just take more computer memory and storage). Think about what you read in the previous chapter: We were able to start using a basic form of statistical inference with a data set that contained a population with only 51 elements. In fact, many of the most commonly used statistical techniques, like the Student&rsquo;s t-test, were designed specifically to work with very small samples. </span></p><p class="c7"><span class="c5">On the other hand, if we are looking for needles in haystacks, it makes sense to look (as efficiently as possible) through the biggest </span></p><p class="c19"><span class="c5">possible haystack we can find, because it is much more likely that a big haystack will contain at least one needle and maybe more. Keeping in mind the advances in machine learning that have occurred over recent years, we begin to have an idea that good tools together with big data and interesting questions about unusual patterns could indeed provide some powerful new insights. </span></p><p class="c7"><span class="c5">Let&rsquo;s couple this optimism with three very important cautions. The first caution is that the more complex our data are, the more difficult it will be to ensure that the data are &quot;clean&quot; and suitable for the purpose we plan for them. A dirty data set is worse in some ways than no data at all because we may put a lot of time and effort into finding an insight and find nothing. Even more problematic, we may put a lot of time and effort and find a result that is simply wrong! Many analysts believe that cleaning data</span><span class="c14">, </span><span class="c5">getting it ready for analysis, weeding out the anomalies, organizing the data into a suitable configuration actually takes up most of the time and effort of the analysis process. </span></p><p class="c7"><span class="c5">The second caution is that rare and unusual events or patterns are almost always by their nature highly unpredictable. Even with the best data we can imagine and plenty of variables, we will almost always have a lot of trouble accurately enumerating all of the causes of an event. The data mining tools may show us a pattern, and we may even be able to replicate the pattern in some new data, but we may never be confident that we have understood the pattern to the point where we believe we can isolate, control, or understand the causes. Predicting the path of hurricanes provides a great example here: despite decades of advances in weather instrumentation, forecasting, and number crunching, meteorologists still have great difficulty predicting where a hurricane will make landfall or how hard the winds will blow when it gets there. The complexity and unpredictability of the forces at work make the task exceedingly difficult. </span></p><p class="c7"><span class="c5">The third caution is about linking data sets. Item C above suggests that linkages may provide additional value. With every linkage to a new data set, however, we also increase the complexity of the data and the likelihood of dirty data and resulting spurious patterns. In addition, although many companies seem less and less concerned about the idea, the more we link data about living people (e.g., consumers, patients, voters, etc.) the more likely we are to cause a catastrophic loss of privacy. Even if you are not a big fan of the importance of privacy on principle, it is clear that security and privacy failures have cost companies dearly both in money and reputation. Today&rsquo;s data innovations for valuable and acceptable purposes maybe tomorrow&rsquo;s crimes and scams. The greater the amount of linkage between data sets, the easier it is for those people with malevolent intentions to exploit it. </span></p><p class="c7"><span class="c5">Putting this </span><span class="c14">all together</span><span class="c5">, we can take a sensible position that </span><span class="c59 c14 c57 c23">high quality </span><span class="c5">data, in abundance, together with tools used by intelligent analysts in a secure environment, may provide worthwhile benefits in the commercial sector, in education, in government, and in other areas. The focus of our efforts as data scientists, however, should not be on achieving the largest possible data sets, but rather on getting the right data and the right amount of data for the purpose we intend. There is no special virtue in having a lot of data if those data are unsuitable to the conclusions that we want to draw. Likewise, simply getting data more quickly does not guarantee that what we get will be highly relevant to our problems. Finally, although it is said that variety is the spice of life, complexity is often a danger to reliability and trustworthiness: the more complex the linkages among our data the more likely it is that problems may crop up in making use of those data or keeping them safe. </span></p><h2 class="c94" id="h.cf97gdjxdbaz"><span class="c38 c57 c23 c117">The Tools of Data Science </span></h2><p class="c7"><span class="c5">Over the past few chapters, we&rsquo;ve gotten a pretty quick jump start on an analytical tool used by thousands of data analysts worldwide the open source R system for data analysis and visualization. Despite the many capabilities of R, however, there are hundreds of other tools used by data scientists, depending on the particular aspects of the data problem they focus on. </span></p><p class="c7"><span class="c5">The single most popular and powerful tool, outside of R, is a proprietary statistical system called SAS (pronounced &quot;sass&quot;). SAS contains a powerful programming language that provides access to many data types, functions, and language features. Learning SAS is arguably as difficult (or as easy, depending upon your perspective) as learning R, but SAS is used by many large corporations because, unlike R, there is extensive technical and product support </span><span class="c14">offered</span><span class="c5">. Of course, this support does not come cheap, so most SAS users work in large organizations that have sufficient resources to purchase the necessary licenses and support plans. </span></p><p class="c7"><span class="c5">Next in line in the statistics realm is SPSS, a package used by many scientists (the acronym used to stand for Statistical Package for the Social Sciences). SPSS is much friendlier than SAS, in the opinion of many analysts, but not quite as flexible and powerful. </span></p><p class="c29"><span class="c5">R, SPSS, and SAS grew up as statistics packages, but there are also many general purpose programming languages that incorporate features valuable to data scientists. One very exciting development in programming languages has the odd name of &quot;Processing.&quot; Processing is a programming language specifically geared toward creating data visualizations. Like R, Processing is an open source project, so it is freely available at </span><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://processing.org/&amp;sa=D&amp;source=editors&amp;ust=1751550345130533&amp;usg=AOvVaw0N1ZYk5Zrl644qcDCyBYat">http://processing.org/</a></span><span class="c5">. Also like R, Processing is a cross-platform program, so it will run happily on Mac, Windows, and Linux. There are lots of books available for learning Processing and the website contains lots of examples for getting started. Besides R, Processing might be one of the most important tools in the data scientist&rsquo;s toolbox, at least for those who need to use data to draw conclusions and communicate with others. </span></p><h3 class="c94" id="h.1rxkt5xka6mp"><span class="c38 c57 c23 c18">Chapter Challenge </span></h3><p class="c29"><span class="c5">Look over the various websites connected with &quot;Data.gov&quot; to find the largest and/or most complex data set that you can. Think about (and perhaps write about) one or more of the ways that those data could potentially be misused by analysts. Download a data set that you find interesting and read it into R to see what you can do with it. </span></p><p class="c7"><span class="c5">For a super extra challenge, go to this website: </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://teamwpc.co.uk/products/wps&amp;sa=D&amp;source=editors&amp;ust=1751550345131647&amp;usg=AOvVaw02nBE5I8wt78jYn9OWcI-7">WPS Analytics Version 4 Released</a></span></p><p class="c7"><span class="c5">and download a trial version of the &quot;World Programming System&quot; (WPS). WPS can read SAS code, so you could easily look up the code that you would need in order to read in your Data.gov dataset. </span></p><h3 class="c124" id="h.yu3b9slbiar7"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_q8jk3tahpxqe-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://aqua.nasa.gov/sites/default/files/references/Wx_Forecasting.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550345132170&amp;usg=AOvVaw0dsVhk6s0m7am9ZCXTtN1r">NASA Weather Forecasting Through the Ages</a></span></li><li class="c0 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Big_data&amp;sa=D&amp;source=editors&amp;ust=1751550345132319&amp;usg=AOvVaw26c0iINnah67FMWiRJM0Od">Wikipedia.org: Big_data </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Data.gov&amp;sa=D&amp;source=editors&amp;ust=1751550345132541&amp;usg=AOvVaw3S7rSflt6OjElDnsR6FRpg">Wikipedia.org: Data.gov</a></span><span class="c11">&nbsp;</span></li></ul><p class="c7 c52 c110"><span class="c44 c21 c54"></span></p><p class="c81 c62 title" id="h.azpgpk2f2iyy"><span class="c21">CHAPTER 9 </span><span class="c23"><br>Onward</span><span>&nbsp;</span><span class="c38 c57 c23 c84">with R-Studio</span></p><p class="c7"><span class="c5">When you run R-Studio Cloud, you will see three or four subwindows. Use the File menu to click &quot;New File&quot; and in the sub-menu for &quot;New File&quot; click &quot;R Script.&quot; This should give you a screen that looks something like this: </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 341.33px;"><img alt="" src="images/image40.png" style="width: 624.00px; height: 341.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c5">The upper left hand &quot;pane&quot; (another name for a sub-window) displays a blank space under the tab title &quot;Untitled1.&quot; Click in that pane and type the following: <br></span></p><p class="c42"><span class="c1">MyMode &lt;- function(myVector) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">return(myVector) </span></p><p class="c42"><span class="c1">}</span></p><p class="c10"><span class="c14">You have just created your first &quot;function&quot; in R. A function is a block of R code that can be used over and over again without having to retype it. </span><span class="c5">Other programming languages also have functions. Other words for function are &quot;procedure&quot; and &quot;subroutine,&quot; although these terms can have a slightly different meaning in other languages. We have called our function &quot;</span><span class="c1">MyMode</span><span class="c5">.&quot; You may remember from a couple of chapters that the basic setup of R does not have a statistical mode function in it, even though it does have functions for the two other other common central tendency statistics, </span><span class="c1">mean() </span><span class="c5">and </span><span class="c1">median()</span><span class="c5">. We&rsquo;re going to fix that problem by creating our own mode function. Recall that the mode function should count up how many of each value is in a list and then return the value that occurs most frequently. That is the definition of the statistical mode: the most frequently occurring item in a vector of numbers. </span></p><p class="c7"><span class="c5">A couple of other things to note: The first is the &quot;</span><span class="c1">myVector</span><span class="c5">&quot; in parentheses on the first line of our function. This is the &quot;argument&quot; or input to the function. We have seen arguments before when we called functions like </span><span class="c1">mean()</span><span class="c5">&nbsp;and </span><span class="c1">median()</span><span class="c5">. Next, note the curly braces that are used on the second and final lines. These curly </span></p><p class="c19"><span class="c5">braces hold together all of the code that goes in our function. Finally, look at the </span><span class="c1">return()</span><span class="c5">&nbsp;right near the end of our function. This </span><span class="c1">return() </span><span class="c5">is where we send back the result of what our function accomplished. Later on when we &quot;call&quot; our new function from the R console, the result that we get back will be whatever is in the parentheses in the </span><span class="c1">return()</span><span class="c5">. </span></p><p class="c7"><span class="c5">Based on that explanation, can you figure out what </span><span class="c1">MyMode() </span><span class="c5">does in this primitive initial form? All it does is return whatever we give it in myVector, completely unchanged. By the way, this is a common way to write code, by building up bit by bit. We can test out what we have each step of the way. Let&rsquo;s test out what we have accomplished so far. First, let&rsquo;s make a very small vector of data to work with. In the lower left hand pane of R-studio you will notice that we have a regular R console running. You can type commands into this console, just like we did in previous chapters just using R: </span></p><p class="c2"><span class="c1">&gt; tinyData &lt;- c(1,2,1,2,3,3,3,4,5,4,5) </span></p><p class="c10"><span class="c1">&gt; tinyData </span></p><p class="c10"><span class="c1">[1] 1 2 1 2 3 3 3 4 5 4 5 </span></p><p class="c29"><span class="c5">Then we can try out our new </span><span class="c1">MyMode()</span><span class="c5">&nbsp;function: </span></p><p class="c2"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c10"><span class="c1">Error: could not find function &quot;MyMode&quot; </span></p><p class="c7"><span class="c5">Oops! R doesn&rsquo;t know about our new function yet. We typed our </span><span class="c1">MyMode()</span><span class="c5">&nbsp;function into the code window, but we didn&rsquo;t tell R about it. If you look in the upper left pane, you will see the code for </span><span class="c1">MyMode() </span><span class="c5">and just above that a few small buttons on a </span><span class="c14">toolbar</span><span class="c5">. One of the buttons looks like a little right pointing arrow with the word &quot;Run&quot; next to it. First, use your mouse to select all of the code for </span><span class="c1">MyMode()</span><span class="c5">, from the first M all the way to the last curly brace. Then click the Run button. You will immediately see the same code appear in the R console window just below. If you have typed everything correctly, there should be no errors or warnings. Now R knows about our </span><span class="c1">MyMode() </span><span class="c5">function and is ready to use it. Now we can type: </span></p><p class="c2"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c10"><span class="c1">[1] 1 2 1 2 3 3 3 4 5 4 5 </span></p><p class="c29"><span class="c5">This did exactly what we expected: it just echoed back the contents of tinyData. You can see from this example how parameters work, too. in the command just above, we passed in tinyData as the input to the function. While the function was working, it took what was in tinyData and copied it into myVector for use inside the function. Now we are ready to add the next command to our function: <br></span></p><p class="c42"><span class="c1">MyMode &lt;- function(myVector) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">uniqueValues &lt;- unique(myVector) </span></p><p class="c42"><span class="c1">return(uniqueValues) </span></p><p class="c42"><span class="c1">}</span></p><p class="c10"><span class="c44 c96 c36">Because we made a few changes, the whole function appears again </span><span class="c5">above. Later, when the code gets a little more complicated, we will just provide one or two lines to add. Let&rsquo;s see what this code does. First, don&rsquo;t forget to select the code and click on the Run button. Then, in the R console, try the </span><span class="c1">MyMode() </span><span class="c5">command again: </span></p><p class="c19"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c10"><span class="c1">[1] 1 2 3 4 5 </span></p><p class="c7"><span class="c5">Pretty easy to see what the new code does, right? We called the </span><span class="c1">unique()</span><span class="c5">&nbsp;function, and that returned a list of unique values that appeared in </span><span class="c1">tinyData</span><span class="c5">. Basically, </span><span class="c1">unique() </span><span class="c5">took out all of the redundancies in the vector that we passed to it. Now let&rsquo;s build a little more: </span></p><p class="c42"><span class="c1">MyMode &lt;- function(myVector) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">uniqueValues &lt;- unique(myVector) </span></p><p class="c42"><span class="c1">uniqueCounts &lt;- tabulate(myVector) </span></p><p class="c42"><span class="c1">return(uniqueCounts) </span></p><p class="c42"><span class="c1">}</span></p><p class="c2"><span class="c5">Don&rsquo;t forget to select all of this code and Run it before testing i</span><span class="c14">t</span><span class="c5">&nbsp;out. &nbsp;This time when we pass </span><span class="c1">tinyData </span><span class="c5">to our function we get back another list of five elements, but this time it is the count of how many times each value occurred: </span></p><p class="c10"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c10"><span class="c1">[1] 2 2 3 2 2 </span></p><p class="c43"><span class="c5">Now we&rsquo;re basically ready to finish our </span><span class="c1">MyMode() </span><span class="c5">function, but let&rsquo;s make sure we understand the two pieces of data we have in </span><span class="c1">uniqueValues </span><span class="c5">and </span><span class="c1">uniqueCounts</span><span class="c5">: </span><span class="c5"><br></span></p><table class="c103"><tr class="c140"><td class="c133 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">INDEX</span></p></td><td class="c116 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">1</span></p></td><td class="c116 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">2</span></p></td><td class="c116 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">3</span></p></td><td class="c116 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">4</span></p></td><td class="c116 c108" colspan="1" rowspan="1"><p class="c27"><span class="c16">5</span></p></td></tr><tr class="c140"><td class="c133" colspan="1" rowspan="1"><p class="c27"><span class="c18">uniqueValues</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">1</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">2</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">3</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">4</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">5</span></p></td></tr><tr class="c140"><td class="c133" colspan="1" rowspan="1"><p class="c27"><span class="c18">uniqueCounts</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">2</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">2</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">3</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">2</span></p></td><td class="c116" colspan="1" rowspan="1"><p class="c27"><span class="c18">2</span></p></td></tr></table><p class="c7"><span class="c5">In the table </span><span class="c14">above </span><span class="c5">we have lined up a row of the elements of </span><span class="c1">uniqueValues </span><span class="c5">just above a row of the counts of how many of each of those values we have. Just for illustration purposes, in the top/label row we have also shown the &quot;index&quot; number. This index number is the way that we can &quot;address&quot; the elements in either of the variables that are shown in the rows. For instance, element number 4 (index 4) for </span><span class="c1">uniqueValues </span><span class="c5">contains the number four, whereas element number four for </span><span class="c1">uniqueCounts </span><span class="c5">contains the number two. So if we&rsquo;re looking for the most frequently occurring item, we should look along the bottom row for the largest number. When we get there, we should look at the index of that cell. Whatever that index is, if we look in the same cell in </span><span class="c1">uniqueValues</span><span class="c5">, we will have the value that occurs most frequently in the original list. </span><span class="c14">I</span><span class="c5">n R, it is easy to accomplish what was described in the last sentence with a single line of code: </span></p><p class="c2"><span class="c1">uniqueValues[which.max(uniqueCounts)] </span></p><p class="c29"><span class="c5">The </span><span class="c1">which.max()</span><span class="c5">&nbsp;function finds the index of the element of uniqueCounts that is the largest. Then we use that index to address uniqueValues with square braces. The square braces let us get at any of the elements of a vector. For example, if we asked for </span><span class="c1">uniqueValues[5],</span><span class="c5">we would get the number 5. If we add this one list of code to our return statement, our function will be finished: <br></span></p><p class="c42"><span class="c1">MyMode &lt;- function(myVector) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">uniqueValues &lt;- unique(myVector) </span></p><p class="c42"><span class="c1">uniqueCounts &lt;- tabulate(myVector) </span></p><p class="c42"><span class="c1">return(uniqueValues[which.max(uniqueCounts)]) </span></p><p class="c42"><span class="c1">}</span></p><p class="c10"><span class="c44 c96 c36">We&rsquo;re now ready to test out our function. Don&rsquo;t forget to select the </span><span class="c5">whole thing and run it! Otherwise R will still be remembering our old one. Let&rsquo;s ask R what tinyData contains, just to remind ourselves, and then we will send tinyData to our </span><span class="c1">MyMode() </span><span class="c5">function: </span></p><p class="c2"><span class="c1">&gt; tinyData </span></p><p class="c10"><span class="c1">[1] 1 2 1 2 3 3 3 4 5 4 5 </span></p><p class="c10"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c45"><span class="c1">[1] 3 </span></p><p class="c45"><span class="c5">Hooray! It works. Three is the most frequently occurring value in </span><span class="c1">tinyData</span><span class="c5">. Let&rsquo;s keep testing and see what happens: </span></p><p class="c45"><span class="c1">&gt; tinyData&lt;-c(tinyData,5,5,5) </span></p><p class="c45"><span class="c1">&gt; tinyData </span></p><p class="c45"><span class="c1">[1] 1 2 1 2 3 3 3 4 5 4 5 5 5 5 </span></p><p class="c45"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c45"><span class="c1">[1] 5 </span><span class="c14 c17"><br></span></p><p class="c19"><span class="c5">It still works! We added three more fives to the end of the </span><span class="c1">tinyData </span><span class="c5">vector. Now </span><span class="c1">tinyData </span><span class="c5">contains five fives. </span><span class="c1">MyMode() </span><span class="c5">properly reports the mode as five. Hmm, now let&rsquo;s try to break it: </span></p><p class="c2"><span class="c1">&gt; tinyData </span></p><p class="c10"><span class="c1">[1] 1 2 1 2 3 3 3 4 5 4 5 5 5 5 1 1 1 </span></p><p class="c10"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c10"><span class="c1">[1] 1 </span></p><p class="c7"><span class="c5">This is interesting: Now </span><span class="c1">tinyData </span><span class="c5">contains five ones and five fives. </span><span class="c1">MyMode() </span><span class="c5">now reports the mode as one. This turns out to be no surprise. In the documentation for </span><span class="c1">which.max()</span><span class="c5">&nbsp;it says that this function will return the first maximum it finds. So this behavior is to be expected. Actually, this is always a problem with the statistical mode: there can be more than one mode in a data set. Our </span><span class="c1">MyMode() </span><span class="c5">function is not smart enough to realize this, </span><span class="c14">nor</span><span class="c5">&nbsp;does it give us any kind of warning that there are multiple modes in our data. It just reports the first mode that it finds. </span></p><p class="c7"><span class="c5">Here&rsquo;s another problem: </span></p><p class="c2"><span class="c1">&gt; tinyData &lt;- c(tinyData,9,9,9,9,9,9,9) </span></p><p class="c10"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c10"><span class="c1">[1] NA </span></p><p class="c10"><span class="c1">&gt; tabulate(tinyData) </span></p><p class="c10"><span class="c1">[1] 5 2 3 2 5 0 0 0 7 </span></p><p class="c29"><span class="c5">In the first line, we stuck a bunch of nines on the end of </span><span class="c1">tinyData</span><span class="c5">. Remember that we had no sixes, sevens, or eights. Now when we </span></p><p class="c19"><span class="c5">run </span><span class="c1">MyMode()</span><span class="c5">&nbsp;it says &quot;</span><span class="c1">NA</span><span class="c5">,&quot; which is R&rsquo;s way of saying that something went wrong and you are getting back an empty value. It is probably not obvious why things went whacky until we look at the last command above, </span><span class="c1">tabulate(tinyData)</span><span class="c5">. Here we can see what happened: when it was run inside of the </span><span class="c1">MyMode()</span><span class="c5">&nbsp;function, </span><span class="c1">tabulate()</span><span class="c5">&nbsp;generated a longer list than we were expecting, because it added zeroes to cover the sixes, sevens, and eights that were not there. The maximum value, out at the end is 7, and this refers to the number of nines in </span><span class="c1">tinyData</span><span class="c5">. But look at what the </span><span class="c1">unique()</span><span class="c5">&nbsp;function produces: </span></p><p class="c10"><span class="c1">&gt; unique(tinyData) </span></p><p class="c2"><span class="c1">[1] 1 2 3 4 5 9 </span></p><p class="c29"><span class="c5">There are only six elements in this list, so it doesn&rsquo;t match up as it should (take another look at the table on the previous page and imagine if the bottom row stuck out further than the row just above it). We can fix this with the addition of the </span><span class="c1">match() </span><span class="c5">function to our code: </span></p><p class="c42"><span class="c1">MyMode</span><span class="c14 c17">F</span><span class="c1">&lt;-function(myVector) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">uniqueValues &lt;- unique(myVector) </span></p><p class="c42"><span class="c1">uniqueCounts &lt;- tabulate( + </span></p><p class="c42"><span class="c59 c14 c23 c80 c68">match(myVector,uniqueValues)</span><span class="c1">) </span></p><p class="c42"><span class="c1">return(uniqueValues[which.max(uniqueCounts)]) </span></p><p class="c42"><span class="c1">} </span></p><p class="c19"><span class="c18"><br></span><span class="c5">The new part of the code is in bold. Now instead of tabulating every possible value, including the ones for which we have no data, we only tabulate those items where there is a &quot;match&quot; between the list of unique values and what is in </span><span class="c1">myVector</span><span class="c5">. Now when we ask </span><span class="c1">MyMode() </span><span class="c5">for the mode of </span><span class="c1">tinyData </span><span class="c5">we get the correct result: </span></p><p class="c2"><span class="c1">&gt; MyMode(tinyData) </span></p><p class="c10"><span class="c1">[1] 9 </span></p><p class="c29"><span class="c5">Aha, now it works the way it should. After our last addition of seven nines to the data set, the mode of this vector is correctly reported as nine. </span></p><p class="c7"><span class="c5">Before we leave this activity, make sure to save your work. Click anywhere in the code window and then click on the File menu and then on Save. You can call the file MyMode, if you like. Note that R adds the &quot;R&quot; extension to the filename so that it is saved as MyMode.R. You can open this file at any time and rerun the </span><span class="c1">MyMode() </span><span class="c5">function in order to define the function in your current working version of R. </span></p><p class="c7"><span class="c5">A couple of other points deserve attention. First, notice that when we created our own function, we had to do some testing and repairs to make sure it ran the way we wanted it to. This is a common situation when working on anything related to computers, including spreadsheets, macros, and pretty much anything else that requires precision and accuracy. Second, we introduced at least four new functions in this exercise, including </span><span class="c1">unique(), tabulate(), match()</span><span class="c5">, and </span><span class="c1">which.max()</span><span class="c5">. Where did </span></p><p class="c19"><span class="c5">these come from and how did we know? R has so many functions that it is very difficult to memorize them all. There&rsquo;s almost always more than one way to do something, as well. So it can be quite confusing to create a new function, if you don&rsquo;t know all of the ingredients and there&rsquo;s no one way to solve a particular problem. This is where the community comes in. Search online and you will fin</span><span class="c14">d </span><span class="c5">dozens of instances where people have tried to solve similar problems to the one you are solving, and you will also find that they have posted the R code for their solutions. These code fragments are free to borrow and test. In fact, learning from other people&rsquo;s examples is a great way to expand your horizons and learn new techniques. </span></p><p class="c7"><span class="c5">The last point leads into the next key topic. We had to do quite a bit of work to create our </span><span class="c1">MyMode </span><span class="c5">function, and we are still not sure that it works perfectly on every variation of data it might encounter. Maybe someone else has already solved the same problem. If they did, we might be able to find an existing &quot;package&quot; to add onto our copy of R to extend its functions. In fact, for the statistical mode, there is an existing package that does just about everything you could imagine doing with the mode. The package is called modeest, a not very good abbreviation for mode-estimator. To install this package look in the lower right hand pane of Rstudio. There are several tabs there, and one of them is &quot;Packages.&quot; Click on this and you will get a list of every package that you already have available in your copy of R (it may be a short list) with checkmarks for the ones that are ready to use. It is unlikely that modeest is already on this list, so click on the button that says &quot;Install Packages. This will give a dialog that looks like what you see on the screenshot above. Type the beginning of the package name in the appropriate area, and R-studio will start to prompt you with matching choices. Finish typing </span><span class="c1">modeest </span><span class="c5">or choose it off of the list. There may be a check box for &quot;Install Dependencies,&quot; and if so leave this checked. In some cases an R package will depend on other packages and R will install all of the necessary packages in the correct order if it can. Once you click the &quot;Install&quot; button in this dialog, you will see some commands running on the R console (the lower left pane). Generally, this works without a hitch and you should not see any warning messages. Once the installation is complete you will see </span><span class="c1">modeest </span><span class="c5">added to the list in the lower right pane (assuming you have clicked the &quot;Packages&quot; tab). One last step is to </span></p><p class="c19"><span class="c5">click the check box next to it. This runs the </span><span class="c1">library() </span><span class="c5">function on the package, which prepares it for further use. </span></p><p class="c7"><span class="c5">Let&rsquo;s try out the</span><span class="c1">&nbsp;mfv() </span><span class="c5">function. This function returns the &quot;most frequent value&quot; in a vector, which is generally what we want in a mode function: </span></p><p class="c2"><span class="c1">&gt; mfv(tinyData) </span></p><p class="c10"><span class="c1">[1] 9 </span></p><p class="c29"><span class="c5">So far so good! This seems to do exactly what our </span><span class="c1">MyMode()</span><span class="c5">&nbsp;function did, though it probably uses a different method. In fact, it is easy to see what strategy the authors of this package used just by typing the name of the function at the R command line: </span></p><p class="c42"><span class="c1">&gt; mfv </span></p><p class="c42"><span class="c1">function (x, ...) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">f &lt;- factor(x) </span></p><p class="c42"><span class="c1">tf &lt;- tabulate(f) </span></p><p class="c42"><span class="c1">return(as.numeric(levels(f)[tf == max(tf)])) </span></p><p class="c42"><span class="c1">}</span></p><p class="c42"><span class="c59 c96 c17 c36">&lt;environment: namespace:modeest&gt; </span></p><p class="c7"><span class="c5">This is one of the great things about an open source program: you can easily look under the hood to see how things work. Notice that this is quite different from how we built </span><span class="c1">MyMode()</span><span class="c5">, although it too uses the </span><span class="c1">tabulate()</span><span class="c5">&nbsp;function. The final line, that begins with the word &quot;environment&quot; has importance for more complex feats of programming, as it indicates which variable names </span><span class="c1">mfv()</span><span class="c5">&nbsp;can refer to when it is working. The other aspect of this function which is probably not so obvious is that it will correctly return a list of multiple modes when one exists in the data you send to it: </span></p><p class="c2"><span class="c1">&gt; multiData </span><span class="c1">&lt;- c</span><span class="c1">(1,5,7,7,9,9,10) </span></p><p class="c10"><span class="c1">&gt; mfv(multiData) </span></p><p class="c10"><span class="c1">[1] 7 9 </span></p><p class="c10"><span class="c1">&gt; MyMode(multiData) </span></p><p class="c10"><span class="c1">[1] 7 </span></p><p class="c7"><span class="c5">In the first command line above, we made a small new vector that contains two modes, 7 and 9. Each of these numbers occurs twice, while the other numbers occur only once. When we run </span><span class="c1">mfv()</span><span class="c5">&nbsp;on this vector it correctly reports both 7 and 9 as modes. When we use our function, </span><span class="c1">MyMode()</span><span class="c5">, it only reports the first of the two modes. </span></p><p class="c7"><span class="c5">To recap, this chapter provided a basic introduction to R-studio, an integrated development environment (IDE) for R. An IDE is useful for helping to build reusable components for handling data and conducting data analysis. Among other things, R-studio makes it easy to manage &quot;packages&quot; in R, and packages are the key to R&rsquo;s extensibility. In future chapters we will be routinely using R packages to get access to specialized capabilities. </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 340.00px;"><img alt="" src="images/image23.png" style="width: 624.00px; height: 340.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c5">These specialized capabilities come in the form of extra functions that are created by developers in the R community. By creating our own function, we learn that functions take &quot;arguments&quot; as their inputs and provide a return value. A return value is a data object, so it could be a single number (technically a vector of length one) or it could be a list of values (a vector) or even a more complex data object. We can write and reuse our own functions, which we will do quite frequently later in the book, or we can use other people&rsquo;s functions by installing their packages and using the </span><span class="c1">library()</span><span class="c5">&nbsp;function to make the contents of the package available. Once we have used </span><span class="c1">library()</span><span class="c5">&nbsp;we can inspect how a function works by typing its name at the R command line. (Note that this works for many functions, but there are a few that were created in a different computer language, like C, and for those we will not be able to inspect the code as easily.) </span></p><h3 class="c20" id="h.b1fs1hwykbj8"><span class="c38 c57 c23 c18">R Commands Used in this Chapter </span></h3><ul class="c32 lst-kix_ctb1u95sakhd-0 start"><li class="c7 c13 li-bullet-0"><span class="c14 c17">function()</span><span class="c5">&nbsp;Creates a new function </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">return()</span><span class="c5">&nbsp;Completes a function by returning a value </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">tabulate()</span><span class="c5">&nbsp;Counts occurrences of integer-valued data in a vector </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">unique()</span><span class="c5">&nbsp;Creates a list of unique values in a vector </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">match() </span><span class="c5">Takes two lists and returns values that are in each </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">mfv()</span><span class="c14">&nbsp;Most frequent value (from the </span><span class="c14 c17">modeest </span><span class="c14">package) </span></li></ul><hr style="page-break-before:always;display:none;"><p class="c81 c62 c52 title" id="h.7wq74j3nlzzb"><span class="c44 c21 c84"></span></p><p class="c81 c62 title" id="h.eeo61uux3jvp"><span class="c21 c64">CHAPTER 10</span><span class="c23">&nbsp;<br>Tweet, &nbsp;Twee</span><span>t</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 314.37px; height: 338.50px;"><img alt="" src="images/image49.png" style="width: 314.37px; height: 338.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c19"><span class="c14 c46 c23 c64 c98 c97">We&rsquo;ve come a long way already: Basic skills in controlling R, some exposure to R-studio, knowledge of how to manage add-on packages, experience creating a function, essential descriptive statistics, and a start on sampling distributions and inferential statistics. In this chapter, we use the social media service Twitter to grab some up-to-the minute data and begin manipulating it. </span></p><p class="c19"><span class="c37 c36"><br></span><span class="c5">Prior to this chapter we only worked with toy data sets: some made up data about a fictional family and the census head counts for the 50 states plus the District of Columbia. At this point we have practiced a sufficient range of skills to work with some real data. There are data sets everywhere, thousands of them, many free for the taking, covering a range of interesting topics from psychology experiments to film actors. For sheer immediacy, though, you can&rsquo;t beat the Twitter social media service. As you may know from direct experience, Twitter is a micro-blogging service that allows people all over the world to broadcast brief thoughts (</span><span class="c14">280</span><span class="c5">&nbsp;characters or less) that can then be read by their &quot;followers&quot; (other Twitter users who signed up to receive the sender&rsquo;s messages). The developers of Twitter, in a stroke of genius, decided to make these postings, called tweets, available to the general public through a web page on the Twitter.com site, and additional through what is known as an application programming interface or API. </span></p><p class="c7"><span class="c5">Here&rsquo;s where the natural extensibility of R comes in. An individual named Jeff Gentry who, at this writing, seems to be a data professional in the financial services industry, created an add-on package for R called </span><span class="c1">twitteR </span><span class="c5">(not sure how it is pronounced, but &quot;twit-are&quot; seems pretty close). The </span><span class="c1">twitteR </span><span class="c5">package provides an extremely simple interface for downloading a list of tweets directly from the Twitter service into R. Using the interface functions in </span><span class="c1">twitteR</span><span class="c5">, it is possible to search through Twitter to obtain a list of tweets on a specific topic. Every tweet contains the text of the posting that the author wrote as well as lots of other useful information such as the time of day when a tweet was posted. Put it all together and </span></p><p class="c19"><span class="c5">it makes a fun way of getting up-to-the-minute data on what people are thinking about a wide variety of topics. </span></p><p class="c43"><span class="c5">The other great thing about working with </span><span class="c1">twitteR </span><span class="c5">is that we will use many, if not all of the skills that we have developed earlier in the book to put the interface to use. </span></p><h2 class="c94" id="h.20tv4d9u7bjh"><span class="c38 c57 c23 c117">A Token of Your Esteem: Using OAuth </span></h2><p class="c7"><span class="c5">Before we move forward with creating some code in Rstudio, there&rsquo;s an important set of steps we need to accomplish at the Twitter website. </span></p><p class="c7"><span class="c5">In 2013, Twitter completed a transition to a new version of their application programming interface, or API. This new API requires the use of a technique for authorization, a way of proving to Twitter that you are who you are when you search for (or post) tweets from a software application. The folks at Twitter adopted an industry standard for this process known as OAuth. OAuth provides a method for obtaining two pieces of information: a &quot;secret&quot; and a &quot;key&quot; without which it will be difficult if not downright impossible to work with Twitter (as well as twitteR). Here are the steps: </span></p><ol class="c32 lst-kix_778s5wjh1op-0 start" start="1"><li class="c7 c13 li-bullet-0"><span class="c5">Get a Twitter account at Twitter.com if you don&rsquo;t already</span><span class="c14">&nbsp;</span><span class="c5">have one. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Go to the development page at Twitter</span><span class="c14">&nbsp;</span><span class="c5">(</span><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=https://dev.twitter.com&amp;sa=D&amp;source=editors&amp;ust=1751550345168565&amp;usg=AOvVaw0btTJOz-bOP6abTQwuJAvr">https://dev.twitter.com</a></span><span class="c5">) and sign in with your Twitter credentials. </span></li><li class="c0 li-bullet-0"><span class="c5">Click on &quot;</span><span class="c14">Apps</span><span class="c5">.&quot; The location of this may vary over time, but look for i</span><span class="c14">t near </span><span class="c5">your profile picture on the top right corner of the screen.</span></li><li class="c0 li-bullet-0"><span class="c5">Apply for a permission to have a Developer account. You will need to link your phone number to your account. The purpose for the account will be Educational or Student. Then when prompted only indicate that we will be processing tweets outside of Twitter environment and press no for all others options such as &ldquo;Using Tweet, Retweet and etc.&rdquo; Congratulations, now you have a Twitter developer account.</span></li><li class="c7 c13 li-bullet-0"><span class="c5">Click on &quot;Create a New Application.&quot; Fill in the blanks with some sensible answers. Where it asks for a &ldquo;website&rdquo; you can give your own home page. This is a required response, so you will have to have some kind of web page to point to. In contrast, the &ldquo;Callback URL&rdquo; can be left blank. Click submit. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Check the checkbox specified </span><span class="c14">to allow y</span><span class="c5">our application should be set so that it can be used to sign in with Twitter. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 121.33px;"><img alt="" src="images/image64.png" style="width: 624.00px; height: 121.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Allow this application to sign in with Twitter"></span></li><li class="c13 c66 li-bullet-0"><span class="c5">You will get a screen containing a whole bunch of data. Make sure to save it all, but the part that you will really need is the &quot;Consumer key&quot; and the &quot;Consumer Secret,&quot; both of which are long strings of letters and numbers. These strings will be used later to get your application running in R. The reason these are such long strings of gibberish is that they are encrypted. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">Also take note of the Request Token URL and the Authorize URL. For the most part these are exactly the same across all uses of Twitter, but they may change over time, so you should make sure to stash them away for later. You do not need to click on the &ldquo;Create my Access Token&rdquo; button. </span></li><li class="c0 li-bullet-0"><span class="c5">Go to the Settings tab and make sure that &quot;Read, Write and</span><span class="c14">&nbsp;</span><span class="c5">Access direct messages&quot; is set. </span></li></ol><p class="c7"><span class="c5">You may notice on the </span><span class="c1">Home-&gt;My applications</span><span class="c5">&nbsp;screen in the </span><span class="c4 c65 c23 c64"><a class="c3" href="https://www.google.com/url?q=http://dev.twitter.com&amp;sa=D&amp;source=editors&amp;ust=1751550345170771&amp;usg=AOvVaw2q22vVeb0RFLYQsNF45ZtA">https://dev.twitter.com</a></span><span class="c5">&nbsp;interface that there are additional tabs along the top for different activities and tasks related to OAuth. There is a tab called </span><span class="c14">Apps - details</span><span class="c5">&nbsp;where you can always come back to get your Consumer key and Consumer secret information. Later in the chapter we will come back to the usage of your Consumer key and your Consumer secret but before we get there we have to get the twitteR package ready to go.</span></p><h2 class="c94" id="h.v1y8my37p2ad"><span class="c38 c57 c23 c117">Working with twitteR </span></h2><p class="c7"><span class="c14">Open the Twitter assignment. </span><span class="c5">R-studio will respond by showing a clean console screen and most importantly an R &quot;workspace&quot; that does not contain any of the old variables and data that we created in previous chapters. In order to use twitteR, we need to load several packages that it depends upon. These are called, in order </span><span class="c1">&quot;</span><span class="c14 c17">bitops</span><span class="c1">&quot;, &quot;RCurl&quot;, &quot;RJSONIO&quot;</span><span class="c5">, and once these are all in place </span><span class="c1">&quot;twitteR&quot;</span><span class="c5">&nbsp;itself. Rather than doing all of this by hand with the menus, let&rsquo;s create some functions that will assist us and make the activity more repeatable. First, here is a function that takes as input the name of a package. It tests whether the package has been downloaded &quot;installed&quot; from the R code repository. If it has not yet been downloaded/installed, the function takes care of this. Then we use a new function, called </span><span class="c1">require()</span><span class="c5">, to prepare the package for further use. Let&rsquo;s call our function </span><span class="c1">&quot;EnsurePackage&quot;</span><span class="c5">&nbsp;because it ensures that a package is ready for us to use. If you don&rsquo;t recall this step from the previous chapter, you should click the &quot;File&quot; menu and then click &quot;New&quot; to create a new file of R script. Then, type or copy/paste the following code: <br></span></p><p class="c42"><span class="c1">EnsurePackage&lt;-function(x) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">x &lt;- as.character(x) </span></p><p class="c42"><span class="c1">if (!require(x,character.only=TRUE)) </span></p><p class="c42"><span class="c1">{ </span></p><p class="c42"><span class="c1">install.packages(pkgs=x, </span></p><p class="c42"><span class="c1">repos=&quot;</span><span class="c59 c17 c40 c23 c61">http://cran.r-project.org</span><span class="c1">&quot;) </span></p><p class="c42"><span class="c1">require(x,character.only=TRUE) </span></p><p class="c42"><span class="c1">} </span></p><p class="c42"><span class="c1">}</span></p><p class="c22"><span class="c14">On Windows machines, the folder where new R packages are stored has to be configured to allow R to put new files there (&ldquo;write&rdquo; permissions). </span><span class="c5">In Windows Explorer, you can right click on the folder and choose &ldquo;Properties-&gt;Security&rdquo; then choose your username and user group, click Edit, enable all permissions, and click OK. If you run into trouble, check out the Windows FAQ at CRAN by searching or using this web address: </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/bin/windows/base/rw-FAQ.html&amp;sa=D&amp;source=editors&amp;ust=1751550345173628&amp;usg=AOvVaw2Y2GTYJt0co2wGL62bT3IK">R for Windows FAQ</a></span><span class="c5">. </span></p><p class="c7"><span class="c5">The </span><span class="c1">require()</span><span class="c5">&nbsp;function on the fourth line above does the same thing as </span><span class="c1">library()</span><span class="c5">, which we learned in the previous chapter, but it also returns the value &quot;</span><span class="c1">FALSE</span><span class="c5">&quot; if the package you requested in the argument &quot;x&quot; has not yet been downloaded. That same line of code also contains another new feature, the &quot;if&quot; statement. This is what computer scientists call a conditional. It tests the stuff inside the parentheses to see if it evaluates to </span><span class="c1">TRUE </span><span class="c5">or </span><span class="c1">FALSE</span><span class="c5">. If </span><span class="c1">TRUE</span><span class="c5">, the program continues to run the script in between the curly braces (lines 4 and 8). If </span><span class="c1">FALSE</span><span class="c5">, all the stuff in the curly braces is skipped. Also in the third line, in case you are curious, the arguments to the </span><span class="c1">require() </span><span class="c5">function include &quot;x,&quot; which is the name of the package that was passed into the function, and </span><span class="c1">&quot;character.only=TRUE&quot;</span><span class="c5">&nbsp;which tells the </span><span class="c1">require()</span><span class="c5">&nbsp;function to expect x to be a character </span><span class="c14">string. Last thing to notice about this third line: there is a &quot;!&quot; character that reverses the results of the logical test. Technically, it is the Boolean function </span><span class="c14 c17">NOT</span><span class="c14">. It requires a bit of mental gyration that when </span><span class="c14 c17">require()</span><span class="c14">&nbsp;returns </span><span class="c14 c17">FALSE</span><span class="c14">, the &quot;</span><span class="c14 c17">!</span><span class="c14">&quot; inverts it to </span><span class="c14 c17">TRUE</span><span class="c5">, and that is when the code in the curly braces runs. </span></p><p class="c7"><span class="c5">Once you have this code in a script window, make sure to select the whole function and click Run in the toolbar to make R aware of the function. There is also a checkbox on that same toolbar called, &quot;Source on Save,&quot; that will keep us from having to click on the Run button all the time. If you click the checkmark, then every time you save the source code file, Rstudio will rerun the code. If you get in the habit of saving after every code change you will always be running the latest version of your function. </span></p><p class="c7"><span class="c14">Now we are ready to put </span><span class="c14 c17">EnsurePackage()</span><span class="c14">&nbsp;to work on the packages we need for</span><span class="c14 c17">&nbsp;twitteR</span><span class="c14">. We&rsquo;ll make a new function, </span><span class="c14 c17">&quot;PrepareTwitter,&quot;</span><span class="c5">&nbsp;that will load up all of our packages for us. Here&rsquo;s the code: </span></p><p class="c2"><span class="c1">PrepareTwitter&lt;-function() </span></p><p class="c22"><span class="c1">{ </span></p><p class="c22"><span class="c1">EnsurePackage(&quot;bitops&quot;) </span></p><p class="c22"><span class="c1">EnsurePackage(&quot;RCurl&quot;) </span></p><p class="c22"><span class="c1">EnsurePackage(&quot;RJSONIO&quot;) </span></p><p class="c28"><span class="c1">EnsurePackage(&quot;twitteR&quot;) </span></p><p class="c22"><span class="c1">EnsurePackage(&quot;ROAuth&quot;) </span></p><p class="c19"><span class="c14 c17">}</span><span class="c14"><br></span></p><p class="c19"><span class="c14">This code is quite straightforward: it calls the EnsurePackage() function we created before five times, once to load each of the packages we need. </span><span class="c5">You may get some warning messages and these generally won&rsquo;t cause any harm. If you are on Windows and you get errors about being able to write to your library remember to check the Windows FAQ as noted above. </span></p><p class="c7"><span class="c14">Make sure to save your script file once you have typed this new function in. You can give it any file name that makes sense to you, such as </span><span class="c14 c17">&quot;twitterSupport.&quot;</span><span class="c14">&nbsp;Now is also a good time to start the habit of commenting: Comments are human readable messages that software developers leave for themselves and for others, so that everyone can remember what a piece of code is supposed to do. All computer languages have at least one &quot;comment character&quot; that sets off the human readable stuff from the rest of the code. In R, the comment character is </span><span class="c14 c17">#</span><span class="c5">. For now, just put one comment line above each function you created, briefly describing it, like this: </span></p><p class="c92"><span class="c47 c17 c23 c36"># EnsurePackage(x) Installs and loads a package </span></p><p class="c2"><span class="c47 c17 c23 c36"># if necessary </span></p><p class="c107"><span class="c5">and this: </span></p><p class="c92"><span class="c47 c17 c23 c36"># PrepareTwitter() Load packages for working </span></p><p class="c2"><span class="c47 c17 c23 c36"># with twitteR </span></p><p class="c155"><span class="c14">Later on we will do a better job of commenting, but this gives us the bare minimum we need to keep going with this project. Before we move on, you should run the </span><span class="c14 c17">PrepareTwitter()</span><span class="c5">&nbsp;function on the console command line to actually load the packages we need: </span></p><p class="c2"><span class="c14 c17">&gt; </span><span class="c59 c14 c23 c80 c68">PrepareTwitter() </span></p><p class="c92"><span class="c5">Note the parentheses after the function name, even though there is no argument to this function. What would happen if you left out the parentheses? Try it later to remind yourself of some basic R syntax rules. </span></p><p class="c7"><span class="c14">You may get a lot of output from running </span><span class="c14 c17">PrepareTwitter()</span><span class="c5">, because your computer may need to download some or all of these packages. You may notice the warning message above, for example,, about objects being &quot;masked.&quot; Generally speaking, this message refers to a variable or function that has become invisible because another variable or function with the same name has been loaded. Usually this is fine: the newer thing works the same as the older thing with the same name. </span></p><p class="c29"><span class="c5">Take a look at the four panes in R-Studio, each of which contains something of interest. The upper left pane is the code/ script window, where we should have the code for our two new functions. The lower left pane shows our R console with the results of the most recently run commands. The upper right pane contains our workspace and history of prior commands, with the tab currently set to workspace. As a reminder, in R parlance, workspace represents all of the currently available data objects and functions. Our two new functions which we have defined should be listed there, indicating that they have each run at least once and R is now aware of them. In the lower right pane, we have files, plots, packages, and help, with the tab currently set to packages. <br></span></p><p class="c19"><span class="c14">This window is scrolled to the bottom to show that </span><span class="c14 c17">RCurl, RJSONIO, </span><span class="c14">and </span><span class="c14 c17">twitteR </span><span class="c5">are all loaded and &quot;libraryed&quot; meaning that they are ready to use from the command line or from functions. </span></p><p class="c7"><span class="c59 c14 c57 c23">Getting New SSL Tokens on Windows </span></p><p class="c7"><span class="c5">For Windows users, depending upon which version of operating system software you are using as well as your upgrade history, it may be necessary to provide new SSL certificates. Certificates help to maintain secure communications across the Internet, and most computers keep an up-to-date copy on file, but not all of them do. If you encounter any problems using R to access the Internet, you may need new tokens. </span></p><p class="c92"><span class="c17 c53 c36">download.file(url=&quot;</span><span class="c17 c53 c61">http://curl.haxx.se/ca/cacert.pem</span><span class="c47 c17 c23 c36">&quot;,+ </span></p><p class="c10"><span class="c47 c17 c23 c36">destfile=&quot;cacert.pem&quot;) </span></p><p class="c107"><span class="c14">This statement needs to be run before the R tries to contact Twitter for authentication. This is because </span><span class="c14 c17">twitteR </span><span class="c14">uses RCurl which in turn employs SSL security whenever &ldquo;https&rdquo; appears in a URL. The command above downloads new certificates and saves them within the current working directory for R. You may need to use cacert.pem for many or most of the function calls to </span><span class="c14 c17">twitteR </span><span class="c14">by adding the argument </span><span class="c14 c17">cainfo=</span><span class="c17 c53 c36">&quot;</span><span class="c14 c17">cacert.pem</span><span class="c17 c53 c36">&quot;.</span><span class="c44 c53 c23 c36">&nbsp;</span></p><p class="c29"><span class="c59 c14 c57 c23">Using Your OAuth Tokens </span></p><p class="c7"><span class="c5">Remember at the beginning of the chapter that we went through some rigamarole to get a Consumer key and a Consumer secret from Twitter. Before we can get started in retrieving data from Twitter we need to put those long strings of numbers and letters to use. </span></p><p class="c7"><span class="c14">Begin this process by getting a credential from ROAuth. Remember that in the command below where I have put </span><span class="c14 c17">&quot;lettersAndNumbers&quot;</span><span class="c14">&nbsp;you have to substitute in your </span><span class="c14 c17">ConsumerKey </span><span class="c14">and your </span><span class="c14 c17">ConsumerSecret </span><span class="c14">that you got from Twitter. The </span><span class="c14 c17">ConsumerKey </span><span class="c14">is a string of upper and lowercase letters and digits about 22 characters long. The </span><span class="c14 c17">ConsumerSecret </span><span class="c14">is also letters and digits and it is about twice as long as the </span><span class="c14 c17">ConsumerKey</span><span class="c14">. Make sure to keep these private, especially the </span><span class="c14 c17">ConsumerSecret</span><span class="c5">, and don&rsquo;t share them with others. Here&rsquo;s the command: </span></p><p class="c9"><span class="c38 c17 c23 c67">&gt; credential &lt;OAuthFactory$new(consumerKey=&quot;lettersAndNumbers&quot;, +cons </span></p><p class="c10"><span class="c17 c67">umerSecret=&quot;lettersAndNumbers&quot;, +requestURL=&quot;</span><span class="c17 c61 c67">https://api.twitter.com/oauth/request_token</span><span class="c17 c67">&quot;, +accessURL=&quot;</span><span class="c17 c61 c67">https://api.twitter.com/oauth/access_token</span><span class="c17 c67">&quot;, +authURL=&quot;</span><span class="c17 c61 c67">https://api.twitter.com/oauth/authorize</span><span class="c38 c17 c23 c67">&quot;) </span></p><p class="c135"><span class="c5">This looks messy but is really very simple. If you now type: </span></p><p class="c10"><span class="c1">&gt; credential </span></p><p class="c7"><span class="c14">You will find that the credential data object is just a conglomeration of the various fields that you specified in the arguments to the </span><span class="c14 c17">OAuthFactory$new </span><span class="c5">method. We have to put that data structure to work now with the following function call: </span></p><p class="c2"><span class="c1">&gt; credential$handshake() </span></p><p class="c19"><span class="c5">Or, if you have downloaded new certificates: </span></p><p class="c2"><span class="c1">&gt; credential$handshake(cainfo=&quot;cacert.pem&quot;) </span></p><p class="c29"><span class="c5">You will get a response back that looks like this: </span></p><p class="c7"><span class="c47 c17 c23 c36">When complete, record the PIN given to you and provide it here: </span></p><p class="c7"><span class="c17 c53 c46 c23 c36 c97">To enable the connection, please direct your web browser to: </span></p><p class="c7"><span class="c83 c17 c53 c46"><a class="c3" href="https://www.google.com/url?q=https://api.twitter.com/oauth/authorize?oauth_token%3D&amp;sa=D&amp;source=editors&amp;ust=1751550345185173&amp;usg=AOvVaw1VQXbGB1IoYigp5uHqIDGp">https://api.twitter.com/oauth/authorize?oauth_token=</a></span><span class="c17 c53 c46 c23 c36 c97">... </span></p><p class="c7"><span class="c17 c53 c46 c23 c36 c97">When complete, record the PIN given to you and provide it here: </span></p><p class="c7"><span class="c14">This will be followed by a long string of numbers. Weirdly, you have to go to a web browser and type in exactly what you see in the R-Studio output window (the URL and the long string of numbers). While typing the URL to be redirected to twitter, be sure that you type </span><span class="c40 c149">http:// </span><span class="c14">instead of </span><span class="c40 c149">https:// </span><span class="c14">otherwise Twitter will not entertain the request because the Twitter server invokes SSL security itself. If you type the URL correctly, Twitter will respond in your browser window with a big button that says &quot;Authorize App.&quot; Go ahead and click on that and you will receive a new screen with a PIN on it (my PIN had seven digits). Take those seven digits and type them into the R-Studio console window (the </span><span class="c14 c17">credential$handshake()</span><span class="c5">&nbsp;function will be waiting for them). Type the digits in front of &ldquo;When complete, record the PIN given to you and provide it here:&rdquo; Hit Enter and, assuming you get no errors, you are fully authorized! Hooray! What a crazy process! Thankfully, you should not have to do any of this again as long as you save the credential data object and restore it into future sessions. The credential object, and all of the other active data, will be stored in the default workspace when you exit R-Studio. </span></p><p class="c7"><span class="c59 c14 c57 c23">Ready, Set, Go! </span></p><p class="c7"><span class="c14">Now let&rsquo;s get some data from Twitter. First, tell the </span><span class="c14 c17">twitteR </span><span class="c5">package that you want to use your shiny new credentials: </span></p><p class="c2"><span class="c1">&gt; registerTwitterOAuth(credential) </span></p><p class="c10"><span class="c1">[1] TRUE </span></p><p class="c29"><span class="c14">The return value of </span><span class="c14 c17">TRUE </span><span class="c14">shows that the credential is working and ready to help you get data from Twitter. Subsequent commands using the </span><span class="c14 c17">twitteR </span><span class="c5">package will pass through the authorized application interface. </span></p><p class="c7"><span class="c14">The </span><span class="c14 c17">twitteR </span><span class="c14">package provides a function called </span><span class="c14 c17">searchTwitter() </span><span class="c14">that allows us to retrieve some recent tweets based on a search term. Twitter users have invented a scheme for organizing their tweets based on subject matter. This system is called &quot;hashtags&quot; and is based on the use of the hashmark character (</span><span class="c14 c17">#</span><span class="c14">) followed by a brief text tag. For example, fans of Oprah Winfrey use the tag </span><span class="c14 c17">#oprah</span><span class="c14">&nbsp;to identify their tweets about her. We will use the </span><span class="c14 c17">searchTwitter()</span><span class="c14">&nbsp;function to search for hashtags about global climate change. The website hashtags.org lists a variety of hashtags covering a range of contemporary topics. You can pick any hashtag you like, as long as there are a reasonable number of tweets that can be retrieved. The </span><span class="c14 c17">searchTwitter() </span><span class="c5">function also requires specify</span></p><p class="c19"><span class="c5">ing the maximum number of tweets that the call will return. For now we will use 500, although you may find that your request does not return that many. Here&rsquo;s the command: </span></p><p class="c2"><span class="c1">tweetList &lt;searchTwitter(&quot;#climate&quot;, n=500) </span></p><p class="c29"><span class="c5">As above, if you are on Windows, and you had to get new certificates, you may have to use this command: </span></p><p class="c2"><span class="c1">tweetList &lt;searchTwitter(&quot;#climate&quot;, n=500, cainfo=&quot;cacert.pem&quot;) </span></p><p class="c29"><span class="c5">Depending upon the speed of your Internet connection and the amount of traffic on Twitter&rsquo;s servers, this command may take a short while for R to process. Now we have a new data object, tweetList, that presumably contains the tweets we requested. But what is this data object? Let&rsquo;s use our R diagnostics to explore what we have gotten: </span></p><p class="c2"><span class="c1">&gt; mode(tweetList) </span></p><p class="c10"><span class="c1">[1] &quot;list&quot; </span></p><p class="c29"><span class="c5">Hmm, this is a type of object that we have not encountered before. In R, a list is an object that contains other data objects, and those objects may be a variety of different modes/types. Contrast this definition with a vector: A vector is also a kind of list, but with the requirement that all of the elements in the vector must be in the same mode/type. Actually, if you dig deeply into the definitions of R data objects, you may realize that we have already encountered one type of list: the dataframe. Remember that the dataframe is a list of vectors, where each vector is exactly the same length. So a dataframe is a particular kind of list, but in general lists do not have those two restrictions that dataframes have (i.e., that each element is a vector and that each vector is the same length). </span></p><p class="c7"><span class="c14">So we know that </span><span class="c14 c17">tweetList </span><span class="c14">is a list, but what does that list contain? Let&rsquo;s try using the </span><span class="c14 c17">str()</span><span class="c5">&nbsp;function to uncover the structure of the list: </span></p><p class="c2"><span class="c1">&gt; str(tweetList) </span></p><p class="c29"><span class="c14">The output will scroll right off the screen, but a quick glance would show that it is pretty repetitive, with each 20 line block being quite similar. So let&rsquo;s use the </span><span class="c14 c17">head()</span><span class="c14">&nbsp;function to just examine the first element of the list. The </span><span class="c14 c17">head() </span><span class="c5">function allows you to just look at the first few elements of a data object. In this case we will look just at the first list element of the tweetList list. The command is: </span></p><p class="c10"><span class="c14 c17">str(head(tweetList,1)<br><br></span><span class="c5">This output is also long, but not quite as long.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 549.00px; height: 457.00px;"><img alt="" src="images/image62.png" style="width: 549.00px; height: 457.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="output from str(head(tweetList,1)"></span></p><p class="c10"><span class="c14">The output looks pretty messy, but is simpler than it may first appear. Following the line &quot;</span><span class="c14 c17">List of 1,</span><span class="c14">&quot; there is a line that begins &quot;</span><span class="c14 c17">$ :Reference class</span><span class="c14">&quot; and then the word &lsquo;status&rsquo; in single quotes. In Twitter terminology a &quot;status&quot; is a single tweet posting (it supposedly tells us the &quot;status&quot; of the person who posted it). So the author of the R </span><span class="c14 c17">twitteR </span><span class="c14">package has created a new kind of data object, called a &lsquo;status&rsquo; that itself contains 10 fields. The fields are then listed out. For each line that begins with &quot;</span><span class="c14 c17">..$</span><span class="c5">&quot; there is a field name and then a mode or data type and then a taste of the data that that field contains. </span></p><p class="c7"><span class="c14">So, for example, the first field, called &quot;</span><span class="c14 c17">text</span><span class="c14">&quot; is of type &quot;</span><span class="c14 c17">chr</span><span class="c5">&quot; (which means character/text data) and the field contains the string that starts with, &quot;Get the real facts on gas prices.&quot; You can look through the other fields and see if you can make sense of them. There are two other data types in there: &quot;logi&quot; stands for logical and that is the same as TRUE/FALSE; &quot;POSIXct&quot; is a format for storing the calendar date and time. (If you&rsquo;re curious, POSIX is an old unix style operating system, where the current date and time were stored as the number of seconds elapsed since 12 midnight on January 1, 1970.) You can see in the &quot;created&quot; field that this particular tweet was created on April 5, 2012 one second after 2:10 PM. It does not </span></p><p class="c19"><span class="c5">show what time zone, but a little detective work shows that all Twitter postings are coded with &quot;coordinated universal time&quot; or what is usually abbreviated with UTC. </span></p><p class="c7"><span class="c14">One last thing to peek at in this data structure is about seven lines from the end, where it says, &quot;and 33 methods...&quot; In computer science lingo a &quot;method&quot; is an operation/activity/ procedure that works on a particular data object. The idea of a method is at the heart of so-called &quot;object oriented programming.&quot; One way to think of it is that the data object is the noun, and the methods are all of the verbs that work with that noun. For example you can see the method &quot;</span><span class="c14 c17">getCreated</span><span class="c14">&quot; in the list: If you use the method </span><span class="c14 c17">getCreated() </span><span class="c5">on a reference object of class &lsquo;status&rsquo;, the method will return the creation time of the tweet. </span></p><p class="c7"><span class="c5">If you try running the command: </span></p><p class="c2"><span class="c1">str(head(tweetList,2)) </span></p><p class="c90"><span class="c5">you will find that the second item in the tweetList list is structured exactly like the first time, with the only difference being the specific contents of the fields. You can also run: </span></p><p class="c2"><span class="c1">length(tweetList) </span></p><p class="c29"><span class="c5">to find out how many items are in your list. The list obtained for this exercise was a full 500 items long. Se we have 500 complex items in our list, but every item had exactly the same structure, with 10 fields in it and a bunch of other stuff too. That raises a thought: tweetList could be thought of as a 500 row structure with 10 columns! That means that we could treat it as a dataframe if we wanted to (and we do, because this makes handling these data much more convenient as you found in the &quot;Rows and Columns&quot; chapter). </span></p><p class="c7"><span class="c14">Happily, we can get some help from R in converting this list into a dataframe. Here we will introduce four powerful new R functions:</span><span class="c14 c17">&nbsp;as()</span><span class="c14">,</span><span class="c14 c17">&nbsp;lapply()</span><span class="c14">, </span><span class="c14 c17">rbind()</span><span class="c14">, and </span><span class="c14 c17">do.call()</span><span class="c14">. The first of these, </span><span class="c14 c17">as()</span><span class="c14">, performs a type coercion: in other words it changes one type to another type. The second of these,</span><span class="c14 c17">&nbsp;lapply()</span><span class="c14">, applies a function onto all of the elements of a list. In the command below, lapply(tweetList, as.data.frame), applies the </span><span class="c14 c17">as.data.frame()</span><span class="c14">&nbsp;coercion to each element in tweetList. Next, the </span><span class="c14 c17">rbind()</span><span class="c14">&nbsp;function &quot;binds&quot; together the elements that are supplied to it into a row-by-row structure. Finally, the </span><span class="c14 c17">do.call()</span><span class="c5">&nbsp;function executes a function call, but unlike just running the function from the console, allows for a variable number of arguments to be supplied to the function. The whole command we will use looks like this: </span></p><p class="c2"><span class="c1">tweetDF &lt;- do.call(&quot;rbind&quot;, lapply(tweetList, + </span></p><p class="c6"><span class="c1">as.data.frame)) </span></p><p class="c7"><span class="c14">You might wonder a few things about this command. One thing that looks weird is &quot;rbind&quot; in double quotes. This is the required method of supplying the name of the function to </span><span class="c14 c17">do.call()</span><span class="c14">. You might also wonder why we needed </span><span class="c14 c17">do.call()</span><span class="c14">&nbsp;at all. Couldn&rsquo;t we have just called </span><span class="c14 c17">rbind()</span><span class="c14">&nbsp;directly from the command line? You can try it if you want, and you will find that it does provide a result, but not the one you want. The difference is in how the arguments to </span><span class="c14 c17">rbind()</span><span class="c14">&nbsp;are supplied to it: if you call it directly,</span><span class="c14 c17">&nbsp;lapply()</span><span class="c14">&nbsp;is evaluated first, and it forms a single list that is then supplied to </span><span class="c14 c17">rbind()</span><span class="c14">. In contrast, by using </span><span class="c14 c17">do.call()</span><span class="c14">, all 500 of the results of</span><span class="c14 c17">&nbsp;lapply() </span><span class="c14">are supplied to </span><span class="c14 c17">rbind() </span><span class="c14">as individual arguments, and this allows </span><span class="c14 c17">rbind()</span><span class="c14">&nbsp;to create the nice rectangular dataset that we will need. The advantage of </span><span class="c14 c17">do.call() </span><span class="c5">is that it will set up a function call with a variable number of arguments in cases where we don&rsquo;t know how many arguments will be supplied at the time when we write the code. </span></p><p class="c7"><span class="c14">If you run the command above, you should see in the upper right hand pane of R-studio a new entry in the workspace under the heading of &quot;</span><span class="c14 c17">Data</span><span class="c14">.&quot; For the example we are running here, the entry says, &quot;</span><span class="c14 c17">500 obs. of 10 variables.</span><span class="c5">&quot; This is just what we wanted, a nice rectangular data set, ready to analyze. Later on, we may need more than one of these data sets, so let&rsquo;s create a function to accomplish the commands we just ran: </span></p><p class="c22"><span class="c1"># TweetFrame() Return a dataframe based on a # search of Twitter </span></p><p class="c22"><span class="c1">TweetFrame&lt;-function(searchTerm, maxTweets)</span></p><p class="c22"><span class="c1">{ </span></p><p class="c22"><span class="c1">twtList&lt;searchTwitter(searchTerm,n=maxTweets) </span></p><p class="c22"><span class="c1">return(do.call(&quot;rbind&quot;,+ </span></p><p class="c22"><span class="c1">lapply(twtList,as.data.frame))) </span></p><p class="c22"><span class="c1">}</span></p><p class="c22"><span class="c14">There are three good things about putting this code in a function. First, because we put a comment at the top of the function, we will remember in the future what this code does. Second, if you test this function you will find out that the variable </span><span class="c14 c17">twtList </span><span class="c14">that is created in the code above does not stick around after the function is finished running. This is the result of what computer scientists call &quot;variable scoping.&quot; The variable twtList only exists while the </span><span class="c14 c17">TweetFrame() </span><span class="c5">function is running. Once the function is done, twtList evaporates as if it never existed. This helps us to keep our workspace clean and avoid collecting lots of intermediate variables that are not reused. </span></p><p class="c7"><span class="c14">The last and best thing about this function is that we no longer have to remember the details of the method for using </span><span class="c14 c17">do.call(), rbind()</span><span class="c14">, </span><span class="c14 c17">lapply()</span><span class="c14">, </span><span class="c14">and </span><span class="c14 c17">as.data.frame()</span><span class="c14">&nbsp;because we will not have to retype these commands again: we can just call the function whenever we need it. And we can always go back and look at the code later. In fact, this would be a good reason to put in a comment just above the </span><span class="c14 c17">return()</span><span class="c5">&nbsp;function. Something like this: </span></p><p class="c7"><span class="c14 c17"># as.data.frame()</span><span class="c14">&nbsp;coerces each list element into a row</span><span class="c14 c17">&nbsp;<br># lapply() </span><span class="c14">applies this to all of the elements in </span><span class="c14 c17">twtList <br># rbind() </span><span class="c14">takes all of the rows and puts them together <br></span><span class="c14 c17"># do.call() </span><span class="c14">gives </span><span class="c14 c17">rbind() </span><span class="c5">all the rows as individual elements </span></p><p class="c73"><span class="c5">Now, whenever we want to create a new data set of tweets, we can just call TweetFrame from the R console command line like this: </span></p><p class="c2"><span class="c1">lgData &lt;- TweetFrame(&quot;#ladygaga&quot;, 250) </span></p><p class="c2"><span class="c14">This command would give us a new dataframe &quot;</span><span class="c14 c17">lgData</span><span class="c5">&quot; all ready to analyze, based on the supplied search term and maximum number of tweets. </span></p><p class="c7"><span class="c14">Let&rsquo;s start to play with the </span><span class="c14 c17">tweetDF </span><span class="c14">dataset that we created before. First, as a matter of convenience, let&rsquo;s learn the </span><span class="c14 c17">attach()</span><span class="c14">&nbsp;function. The </span><span class="c14 c17">attach()</span><span class="c5">&nbsp;function saves us some typing by giving one particular dataframe priority over any others that have the same variable names. Normally, if we wanted to access the variables in our dataframe, we would have to use the $ notation, like this: </span></p><p class="c10"><span class="c1">tweetDF$created </span></p><p class="c7"><span class="c14">But if we run </span><span class="c14 c17">attach(tweetDF)</span><span class="c14">&nbsp;first, we can then refer to created directly, without having to type the </span><span class="c14 c17">tweetDF$</span><span class="c5">&nbsp;before it: </span></p><p class="c9"><span class="c26 c17 c23">&gt; attach(tweetDF) </span></p><p class="c10"><span class="c26 c17 c23">&gt; head(created,4) </span></p><p class="c10"><span class="c26 c17 c23">[1] &quot;2012-04-05 14:10:01 UTC&quot; &quot;2012-04-05 14:09:21 UTC&quot; </span></p><p class="c10"><span class="c26 c17 c23">[3] &quot;2012-04-05 14:08:15 UTC&quot; &quot;2012-04-05 14:07:12 UTC&quot; </span></p><p class="c135"><span class="c14">Let&rsquo;s visualize the creation time of the 500 tweets in our dataset. When working with time codes, the </span><span class="c14 c17">hist() </span><span class="c5">function requires us to specify the approximate number of categories we want to see in the histogram: </span></p><p class="c2"><span class="c1">hist(created, breaks=15, freq=TRUE) </span></p><p class="c29"><span class="c14">This command yields the histogram that appears below. If we look along the x-axis (the horizontal), this string of tweets starts at about 4:20 AM and goes until about 10:10 AM, a span of roughly six hours. There are 22 different bars so each bar represents about 16 minutes for casual purposes we&rsquo;ll call it a quarter of an hour. It looks like there are something like 20 tweets per bar, so we are looking at roughly 80 tweets per hour with the hashtag &quot;</span><span class="c14 c17">#climate</span><span class="c14">&quot;. This is obviously a pretty popular topic. This distribution does not really have a discernible shape, although it seems like there might be a bit of a growth trend as time goes on, particularly starting at about 7:40 AM. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image56.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of Created"></span></p><p class="c19"><span class="c5">Take note of something very important about these data: It doesn&rsquo;t make much sense to work with a measure of central tendency. Remember a couple of chapters ago when we were looking at the number of people who resided in different U.S. states? In that case it made sense to say that if State A had one million people and State B had three million people, then the average of these two states was two million people. When you&rsquo;re working with timestamps, it doesn&rsquo;t make a whole lot of sense to say that one tweet arrived at 7 AM and another arrived at 9 AM so the average is 8 AM. Fortunately, there&rsquo;s a whole area of statistics concerned with &quot;arrival&quot; times and similar phenomena, dating back to a famous study by Ladislaus von Bortkiewicz of horsemen who died after being kicked by their horses. von Bortkiewicz studied each of 14 cavalry corps over a period of 20 years, noting when horsemen died each year. The distribution of the &quot;arrival&quot; of kickdeaths turns out to have many similarities to other arrival time data, such as the arrival of buses or subway cars at a station, the arrival of customers at a cash register, or the occurrence of telephone calls at a particular exchange. All of these kinds of events fit what is known as a &quot;Poisson Distribution&quot; (named after Simeon Denis Poisson, who published it about half a century before von Bortkiewicz found a use for it). Let&rsquo;s find out if the arrival times of tweets comprise a Poisson distribution. </span></p><p class="c7"><span class="c14">Right now we have the actual times when the tweets were posted, coded as a POSIX date and time variable. Another way to think about these data is to think of each new tweet as arriving a certain amount of time after the previous tweet. To figure that out, we&rsquo;re going to have to &quot;look back&quot; a row in order to subtract the creation time of the previous tweet from the creation time of the current tweet. In order to be able to make this calculation, we have to make sure that our data are sorted in ascending order of arrival in other words the earliest one first and the latest one last. To accomplish this, we will use the </span><span class="c14 c17">order()</span><span class="c5">&nbsp;function together with R&rsquo;s built-in square bracket notation. </span></p><p class="c7"><span class="c14">As mentioned briefly in the previous chapter, in R, square brackets allow &quot;indexing&quot; into a list, vector, or data frame. For example, </span><span class="c14 c17">myList[3] </span><span class="c14">would give us the third element of </span><span class="c14 c17">myList</span><span class="c14">. Keeping in mind that a dataframe is a rectangular structure, really a two dimensional structure, we can address any element of a dataframe with both a row and column designator: </span><span class="c14 c17">myFrame[4,1] </span><span class="c14">would give the fourth row and the first column. A shorthand for taking the whole column of a dataframe is to leave the row index empty: <br></span><span class="c14 c17">myFrame[ , 6] </span><span class="c14">would give every row in the sixth column. Likewise, a shorthand for taking a whole row of a dataframe is to leave the column index empty: </span><span class="c14 c17">myFrame[10, ] </span><span class="c14">would give every column in the tenth row. We can also supply a list of rows instead of just one row, like this: </span><span class="c14 c17">myFrame[ c(1,3,5), ] </span><span class="c14">would return rows 1, 3, 5 (including the data for all columns, because we left the column index blank). We can use this feature to reorder the rows, using the </span><span class="c14 c17">order() </span><span class="c14">function. We tell </span><span class="c14 c17">order() </span><span class="c5">which variable we want to sort on, and it will give back a list of row indices in the order we requested. Putting it all together yields this command: </span></p><p class="c2"><span class="c1">tweetDF[order(as.integer(created)), ] </span></p><p class="c2"><span class="c14">Working our way from the inside to the outside of the expression above, we want to sort in the order that the tweets were created. We first coerce the variable &quot;created&quot; to integer it will then truly be expressed in the number of seconds since 1970 just in case there are operating system differences in how POSIX dates are sorted. We wrap this inside the </span><span class="c14 c17">order() </span><span class="c14">function. The </span><span class="c14 c17">order() </span><span class="c14">function will provide a list of row indices that reflects the time ordering we want. We use the square brackets notation to address the rows in </span><span class="c14 c17">tweetDF</span><span class="c5">, taking all of the columns by leaving the index after the comma empty. </span></p><p class="c7"><span class="c5">We have a choice of what to do with the dataframe that is returned from this command. We could assign it back to tweetDF, which would overwrite our original dataframe with the sorted version. Or we could create a new sorted dataframe and leave the original data alone, like so: </span></p><p class="c73"><span class="c26 c17 c23">sortweetDF &lt;- tweetDF[order(as.integer(created)), ] </span></p><p class="c86"><span class="c14">If you choose this method, make sure to </span><span class="c14 c17">detach() tweetDF </span><span class="c14">and </span><span class="c14 c17">attach() sortweetDF</span><span class="c5">&nbsp;so that later commands will work smoothly with the sorted dataframe: </span></p><p class="c2"><span class="c1">&gt; detach(tweetDF) </span></p><p class="c10"><span class="c1">&gt; attach(sortweetDF) </span></p><p class="c29"><span class="c14">Another option, which seems better than creating a new dataframe, would be to build the sorting into the </span><span class="c14 c17">TweetFrame()</span><span class="c14">&nbsp;function that we developed at the beginning of the chapter. Let&rsquo;s leave that to the chapter challenge. For now, we can keep working with </span><span class="c14 c17">sortweetDF</span><span class="c5">. </span></p><p class="c19"><span class="c14">Technically, what we have with our created variable now is a time series, and because statisticians like to have convenient methods for dealing with time series, R has a built-in function, called </span><span class="c14 c17">diff()</span><span class="c5">, that allows us to easily calculate the difference in seconds between each pair of neighboring values. Try it:</span></p><p class="c19"><span class="c59 c96 c17 c36">&gt; diff(created) </span></p><p class="c29"><span class="c5">You should get a list of time differences, in seconds, between neighboring tweets. The list will show quite a wide range of intervals, perhaps as long as several minutes, but with many intervals near or at zero. You might notice that there are only 499 values and not 500: This is because you cannot calculate a time difference for the very first tweet, because we have no data on the prior tweet. Let&rsquo;s visualize these data and see what we&rsquo;ve got: </span></p><p class="c2"><span class="c14 c17">&gt; hist(as.integer(diff(created))) <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image35.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of hist(as.integer(diff(created)))"></span></p><p class="c2"><span class="c14">As with earlier commands, we use </span><span class="c14 c17">as.integer()</span><span class="c14">&nbsp;to coerce the time differences into plain numbers, otherwise </span><span class="c14 c17">hist() </span><span class="c5">does not know how to handle the time differences. This histogram shows that the majority of tweets in this group come within 50 seconds or less of the previous tweets. A much smaller number of tweets arrive within somewhere between 50 and 100 seconds, and so on down the line. This is typical of a Poisson arrival time distribution. Unlike the raw arrival time data, we could calculate a mean on the time differences: </span></p><p class="c2"><span class="c1">&gt; mean(as.integer(diff(created))) </span></p><p class="c19"><span class="c1">[1] 41.12826 </span></p><p class="c19 c52"><span class="c1"></span></p><p class="c19"><span class="c14">We have to be careful though, in using measures of central tendency on this positively skewed distribution, that the value we get from the </span><span class="c14 c17">mean() </span><span class="c5">is a sensible representation of central tendency. Remembering back to the previous chapter, and our discussion of the statistical mode (the most frequently occurring value), we learn that the mean and the mode are very different: </span></p><p class="c2"><span class="c1">&gt; library(&quot;modeest&quot;) </span></p><p class="c10"><span class="c1">&gt; mfv(as.integer(diff(created))) </span></p><p class="c10"><span class="c14 c17">[1] 0</span><span class="c5">&nbsp;</span></p><p class="c19"><span class="c14">We use the </span><span class="c14 c17">library() </span><span class="c14">function to make sure that the add on package with the </span><span class="c14 c17">mfv()</span><span class="c14">&nbsp;function is ready to use. The results of the </span><span class="c14 c17">mfv()</span><span class="c5">&nbsp;function show that the most commonly occurring time interval between neighboring tweets is zero! </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">Likewise the median shows that half of the tweets have arrival times of under half a minute: </span></p><p class="c2"><span class="c1">&gt; median(as.integer(diff(created))) </span></p><p class="c10"><span class="c1">[1] 28 </span></p><p class="c29"><span class="c5">In the next chapter we will delve more deeply into what it means when a set of data are shaped like a Poisson distribution and what that implies about making use of the mean. </span></p><p class="c7"><span class="c5">One last way of looking at these data before we close this chapter. If we choose a time interval, such as 10 seconds, or 30 seconds, or 60 seconds, we can ask the question of how many of our tweet arrivals occurred within that time interval. Here&rsquo;s code that counts the number of arrivals that occur within certain time intervals: </span></p><p class="c2"><span class="c1">&gt; sum((as.integer(diff(created)))&lt;60) </span></p><p class="c10"><span class="c1">[1] 375 </span></p><p class="c10"><span class="c1">&gt; sum((as.integer(diff(created)))&lt;30) </span></p><p class="c10"><span class="c1">[1] 257 </span></p><p class="c10"><span class="c1">&gt; sum((as.integer(diff(created)))&lt;10) </span></p><p class="c10"><span class="c1">[1] 145 </span></p><p class="c7"><span class="c5">You could also think of these as ratios, for example 145/500 = 0.29.</span></p><p class="c7"><span class="c14">And where we have a ratio, we often can think about it as a probability: There is a 29% probability that the next tweet will arrive in 10 seconds or less. You could make a function to create a whole list of these probabilities. Some sample code for such a function appears at the end of the chapter. Some new scripting skills that we have not yet covered (for example, the &quot;for loop&quot;) appear in this function, but try making sense out of it to stretch your brain. Output from this function created the plot that appears below. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image46.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Poisson distribution of arrival probabilities. "></span></p><p class="c19"><span class="c5">This is a classic Poisson distribution of arrival probabilities. The x-axis contains 10 second intervals (so by the time you see the number 5 on the x-axis, we are already up to 50 seconds). This is called a cumulative probability plot and you read it by talking about the probability that the next tweet will arrive in the amount of time indicated on the x-axis or less. For example, the number five on the x-axis corresponds to about a 60% probability on the y-axis, so there is a 60% probability that the next tweet will arrive in 50 seconds or less. Remember that this estimate applies only to the data in this sample! </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">In the next chapter we will reexamine sampling in the context of Poisson and learn how to compare two Poisson distributions to find out which hashtag is more popular. </span></p><p class="c7"><span class="c14">Let&rsquo;s recap what we learned from this chapter. First, we have begun to use the project features of R-studio to establish a clean environment for each R project that we build. Second, we used the source code window of R-studio to build two or three very useful functions, ones that we will reuse in future chapters. Third, we practiced the skill of installing packages to extend the capabilities of R. Specifically, we loaded Jeff Gentry&rsquo;s </span><span class="c14 c17">twitteR </span><span class="c14">package and the other three packages it depends upon. Fourth, we put the </span><span class="c14 c17">twitteR </span><span class="c5">package to work to obtain our own fresh data right from the web. Fifth, we started to condition that data, for example by creating a sorted list of tweet arrival times. And finally, we started to analyze and visualize those data, by conjecturing that this sample of arrival times fitted the classic Poisson distribution. </span></p><p class="c7"><span class="c59 c14 c57 c23">Chapter Challenge </span></p><p class="c7"><span class="c14">Modify the </span><span class="c14 c17">TweetFrame() </span><span class="c14">function created at the beginning of this chapter to sort the dataframe based on the creation time of the tweets. This will require taking the line of code from a few pages ago that has the </span><span class="c14 c17">order() </span><span class="c14">function in it and adding this to the </span><span class="c14 c17">TweetFrame() </span><span class="c14">function with a few minor modifications. Here&rsquo;s a hint: Create a temporary dataframe inside the function and don&rsquo;t attach it while you&rsquo;re working with it. You&rsquo;ll need to use the </span><span class="c14 c17">$</span><span class="c5">&nbsp;notation to access the variable you want to use to order the rows. </span></p><h3 class="c39" id="h.lnv8nd3csybh"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_ciwimf4uiie7-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/web/packages/twitteR/twitteR.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550345218078&amp;usg=AOvVaw1ko3CB1JzfPM_Lga3h_IfN">http://cran.r-project.org/web/packages/twitteR/twitteR.pdf </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/web/packages/twitteR/vignettes/twitte&amp;sa=D&amp;source=editors&amp;ust=1751550345218370&amp;usg=AOvVaw3opjNjr3EM6Y35QSA4Jt71">http://cran.r-project.org/web/packages/twitteR/vignettes/twitte R.pdf </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Ladislaus_Bortkiewicz&amp;sa=D&amp;source=editors&amp;ust=1751550345218556&amp;usg=AOvVaw2k73D_co_xm99dLRhmzv-B">http://en.wikipedia.org/wiki/Ladislaus_Bortkiewicz </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Poisson_distribution&amp;sa=D&amp;source=editors&amp;ust=1751550345218790&amp;usg=AOvVaw2G7fNMLEBrM3yZVrIFncbP">http://en.wikipedia.org/wiki/Poisson_distribution</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://hashtags.org/&amp;sa=D&amp;source=editors&amp;ust=1751550345218927&amp;usg=AOvVaw25otQFQY592QcsKMozOoRA">http://hashtags.org/ </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.inside-r.org/packages/cran/twitteR/docs/example&amp;sa=D&amp;source=editors&amp;ust=1751550345219162&amp;usg=AOvVaw1YKpzJs6S9aajrWwvNTWEz">http://www.inside-r.org/packages/cran/twitteR/docs/example Oauth</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.khanacademy.org/math/probability/v/poisson-proc&amp;sa=D&amp;source=editors&amp;ust=1751550345219389&amp;usg=AOvVaw0651Mb3tGyVN-mGn7YeZCv">http://www.khanacademy.org/math/probability/v/poisson-proc ess-1</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.khanacademy.org/math/probability/v/poisson-proc&amp;sa=D&amp;source=editors&amp;ust=1751550345219573&amp;usg=AOvVaw1ObIQrufgdn4cnd4TrJzSX">http://www.khanacademy.org/math/probability/v/poisson-proc ess-2</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://support.twitter.com/articles/49309&amp;sa=D&amp;source=editors&amp;ust=1751550345219724&amp;usg=AOvVaw05xHT2s8H2zqRWNMJ2hg8c">https://support.twitter.com/articles/49309</a></span><span class="c40 c61">&nbsp;</span><span class="c5">(hashtags explained) </span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.rdatamining.com/examples/text-mining&amp;sa=D&amp;source=editors&amp;ust=1751550345219903&amp;usg=AOvVaw19phV9Iuvh3PozBBkqiey0">http://www.rdatamining.com/examples/text-mining </a></span></li></ul><h3 class="c94" id="h.euoyul971wtc"><span class="c38 c57 c23 c18">R Script Create Vector of Probabilities From Arrival Times </span></h3><p class="c10"><span class="c50 c17 c23 c36"># ArrivalProbability Given a list of arrival times </span></p><p class="c31"><span class="c50 c17 c23 c36"># calculates the delays between them using lagged differences </span></p><p class="c31"><span class="c50 c17 c23 c36"># then computes a list of cumulative probabilities of arrival </span></p><p class="c31"><span class="c50 c17 c23 c36"># for the sequential list of time increments </span></p><p class="c31"><span class="c50 c17 c23 c36"># times A sorted, ascending list of arrival times in POSIXct </span></p><p class="c31"><span class="c50 c17 c23 c36"># increment the time increment for each new slot, e.g. 10 sec </span></p><p class="c31"><span class="c50 c17 c23 c36"># max the highest time increment, e.g., 240 sec </span></p><p class="c31"><span class="c17 c36">#</span><span class="c59 c17 c147 c36 c163"># Returns an ordered list of probabilities in a numeric vector </span></p><p class="c31"><span class="c50 c17 c23 c36"># suitable for plotting with plot() </span></p><p class="c31"><span class="c50 c17 c23 c36">ArrivalProbability&lt;-function(times, increment, max) </span></p><p class="c31"><span class="c50 c17 c23 c36">{ </span></p><p class="c31"><span class="c50 c17 c23 c36"># Initialize an empty vector </span></p><p class="c31"><span class="c50 c17 c23 c36">plist &lt;NULL </span></p><p class="c95"><span class="c50 c17 c23 c36"># Probability is defined over the size of this sample </span></p><p class="c31"><span class="c50 c17 c23 c36"># of arrival times </span></p><p class="c31"><span class="c50 c17 c23 c36">timeLen &lt;length(times) </span></p><p class="c95"><span class="c50 c17 c23 c36"># May not be necessary, but checks for input mistake </span></p><p class="c31"><span class="c50 c17 c23 c36">if (increment&gt;max) {return(NULL)} </span></p><p class="c95"><span class="c50 c17 c23 c36">for (i in seq(increment, max, by=increment)) </span></p><p class="c31"><span class="c17 c23 c36 c50">{ </span></p><p class="c31"><span class="c50 c17 c23 c36"># diff() requires a sorted list of times </span></p><p class="c19"><span class="c50 c17 c23 c36"># diff() calculates the delays between neighboring times </span></p><p class="c31"><span class="c50 c17 c23 c36"># the logical test &lt;i provides a list of TRUEs and FALSEs </span></p><p class="c31"><span class="c50 c17 c23 c36"># of length = timeLen, then sum() counts the TRUEs. </span></p><p class="c31"><span class="c50 c17 c23 c36"># Divide by timeLen to calculate a proportion </span></p><p class="c31"><span class="c50 c17 c23 c36">plist&lt;-c(plist,(sum(as.integer(diff(times))&lt;i))/timeLen) </span></p><p class="c31"><span class="c50 c17 c23 c36">} </span></p><p class="c31"><span class="c50 c17 c23 c36">return(plist) </span></p><p class="c31"><span class="c44 c23 c54 c36">}</span></p><p class="c31"><span class="c59 c96 c57 c36">R Functions Used in This Chapter </span></p><ul class="c32 lst-kix_l3gnvclgwx98-0 start"><li class="c7 c13 li-bullet-0"><span class="c14 c17">attach() </span><span class="c14">Makes the variables of a dataset available without </span><span class="c14 c17">$</span><span class="c5">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">as.integer() </span><span class="c5">Coerces data into integers </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">detach() </span><span class="c14">Undoes an </span><span class="c14 c17">attach() </span><span class="c5">function </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">diff() </span><span class="c5">Calculates differences between neighboring rows </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">do.call() </span><span class="c5">Calls a function with a variable number of arguments </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">function() </span><span class="c5">Defines a function for later use</span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">install.packages() </span><span class="c5">Downloads and prepares a package for use </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">lapply() </span><span class="c5">Applies a function to a list </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">library() </span><span class="c14">Loads a package for use; like </span><span class="c1">require() </span></li><li class="c29 c13 li-bullet-0"><span class="c14 c17">mean() </span><span class="c5">Calculates the arithmetic mean of a vector </span></li><li class="c29 c13 li-bullet-0"><span class="c14 c17">hist() </span><span class="c5">Plots a histogram from a list of data</span></li><li class="c0 li-bullet-0"><span class="c14 c17">median() </span><span class="c5">Finds the statistical center point of a list of numbers </span></li><li class="c13 c113 li-bullet-0"><span class="c14 c17">mfv() </span><span class="c14">Most frequent value; part of the </span><span class="c14 c17">modeest() </span><span class="c5">package </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">mode() </span><span class="c5">Shows the basic data type of an object </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">order() </span><span class="c5">Returns a sorted list of index numbers </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">rbind() </span><span class="c5">Binds rows into a dataframe object </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">require() </span><span class="c5">Tests if a package is loaded and loads it if needed </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">searchTwitter() </span><span class="c14">Part of the </span><span class="c14 c17">twitteR </span><span class="c5">package </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">str() </span><span class="c5">Describes the structure of a data object </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">sum() </span><span class="c14">Adds up a list of numbers </span></li></ul><hr style="page-break-before:always;display:none;"><p class="c81 c62 c52 title" id="h.57wh8s89lkon"><span class="c59 c21 c57 c84"></span></p><p class="c81 c62 title" id="h.40ryukfdav8a"><span class="c70">CHAPTER 11 </span><span><br>Popularity Contest<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 168.00px;"><img alt="" src="images/image59.png" style="width: 602.00px; height: 168.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="A Viral Tweet"></span><span class="c59 c57 c23 c37 c36">In the previous chapter we found that arrival times of tweets on a given topic seem to fit a Poisson distribution, which is a distribution of probabilities that a certain number of unrelated events (like Tweets) occur within a certain period of time when they occur with a known average rate. Armed with that knowledge we can now develop a test to compare two different Twitter topics to see which one is more popular (or at least which one has a higher posting rate). We will use our knowledge of sampling distributions to understand the logic of the test.</span></p><p class="c19"><span class="c37 c36"><br></span><span class="c5">Which topic on Twitter is more popular, Lady Gaga or Oprah Winfrey? This may not seem like an important question, depending upon your view of older popular culture, but if we can make the comparison for these two topics, we can make it for any two topics. Certainly in the case of the next presidential election, or a corruption scandal in the local news, or an international crisis, it could be a worthwhile goal to be able to analyze social media in a systematic way. And on the surface, the answer to the question seems trivial: Just add up who has more tweets. Surprisingly, in order to answer the question in an accurate and reliable way, this won&rsquo;t work, at least not very well. Instead, one must consider many of the vexing questions that made inferential statistics necessary. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">Recall from the last chapter that there&rsquo;s a whole area of statistics concerned with &quot;arrival&quot; times and similar phenomena, dating back to a famous study by Ladislaus von Bortkiewicz of horsemen who died after being kicked by their horses. Von Bortkiewicz studied each of 14 cavalry corps over a period of 20 years, noting when horsemen died each year. The distribution of the &quot;arrival&quot; of kickdeaths turns out to have many similarities to other arrival time data, such as the arrival of buses or subway cars at a station, the arrival of customers at a cash register, or the occurrence of telephone calls at a particular exchange. All of these kinds of events fit what is known as a &quot;Poisson Distribution&quot; (named after Simeon Denis Poisson, who published it). Let&rsquo;s find out if the arrival times of tweets comprise a Poisson distribution. </span></p><p class="c29"><span class="c5">Let&rsquo;s say we retrieved one hour&rsquo;s worth of Lady Gaga tweets and a similar amount of Oprah Winfrey tweets and just counted them up. What if it just happened to be a slow news day for Oprah? It really wouldn&rsquo;t be a fair comparison. What if most of Lady Gaga&rsquo;s tweets happen at midnight or on Saturdays? We could expand our sampling time, maybe to a day or a week. This could certainly help: Generally speaking, the bigger the sample, the more representative it is of the whole population, assuming it is not collected in a biased way. This approach defines popularity as the number of tweets over a fixed period of time. Its success depends upon the choice of a sufficiently large period of time, that the tweets are collected for the two topics at the same time, and that the span of time chosen happens to be equally favorable for both two topics. </span></p><p class="c7"><span class="c5">Another approach to the popularity comparison would build upon what we learned in the previous chapter about how arrival times (and the delays between them) fit into the Poisson distribution. In this alternative definition of the popularity of a topic, we could suggest that if the arrival curve is &quot;steeper&quot; for the first topic in contrast to the second topic, then the first topic is more active and therefore more popular. Another way of saying the same thing is that for the more popular topic, the likely delay until the arrival of the next tweet is shorter than for the less popular topic. You could also say that for a given interval of time, say ten minutes, the number of arrivals for the first topic would be higher than for the second topic. Assuming that the arrival delays fit a Poisson distribution, these are all equivalent ways of capturing the comparison between the two topics. </span></p><p class="c7"><span class="c14">Just as we did in the chapter entitled, &quot;Sample in a Jar,&quot; we can use a random number generator in R to illustrate these kinds of differences more concretely. The relevant function for the Poisson distribution is </span><span class="c14 c17">rpois()</span><span class="c14">, &quot;random poisson.&quot; The </span><span class="c14 c17">rpois()</span><span class="c14">&nbsp;function will generate a stream of random numbers that roughly fit the Poisson distribution. The fit gets better as you ask for a larger and larger sample. The first argument to </span><span class="c14 c17">rpois() </span><span class="c5">is how many random numbers you want to generate and the second number is the average delay between arrivals that you want the random number generator to try to come close to. We can look at a few of these numbers and then use a histogram function to visualize the results: </span></p><p class="c2"><span class="c1">&gt; rpois(10,3) </span></p><p class="c10"><span class="c1">[1] 5 4 4 2 0 3 6 2 3 3 </span></p><p class="c10"><span class="c1">&gt; mean(rpois(100,3)) </span></p><p class="c10"><span class="c1">[1] 2.99 </span></p><p class="c10"><span class="c1">&gt; var(rpois(100,3)) </span></p><p class="c10"><span class="c14 c17">[1] 3.028182 </span></p><p class="c19"><span class="c1">&gt; hist(rpois(1000,3))</span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image27.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="hist(rpois(1000,3))"></span><span class="c14 c17"><br></span><span class="c14">In the first command above, we generate a small sample of n=10 arrival delays, with a hoped-for mean of 3 seconds of delay, just to see what kind of numbers we get. You can see that all of the numbers are small integers, ranging from 0 to 6. In the second command we double check these results with a slightly larger sample of n=100 to see if </span><span class="c14 c17">rpois() </span><span class="c5">will hit the mean we asked for. In that run it came out to 2.99, which was pretty darned close. If you run this command yourself you will find that your result will vary a bit each time: it will sometimes be slightly larger than three and occasionally a little less than three (or whatever mean you specify). </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c14">This is expected because of the random number generator. In the third command we run yet another sample of 100 random data points, this time analyzing them with the </span><span class="c14 c17">var() </span><span class="c5">function (which calculates the variance; see the chapter entitled &quot;Beer, Farms, and Peas&quot;). It is a curious fact of Poisson distributions that the mean and the variance of the &quot;ideal&quot; (i.e., the theoretical) distribution are the same. In practice, for a small sample, they may be different. </span></p><p class="c7"><span class="c5">In the final command, we ask for a histogram of an even larger sample of n=1000. The histogram shows the most common value hanging right around three seconds of delay with a nice tail that points rightwards and out to about 10 seconds of delay. You can think of this as one possible example of what you might observe of the average delay time between tweets was about three seconds. Note how similar the shape of this histogram is to what we observed with real tweets in the last chapter. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">Compare the histogram on the previous page to the one on the next page that was generated with this command: </span></p><p class="c2"><span class="c1">hist(rpois(1000,10)) </span></p><p class="c2"><span class="c14">It is pretty easy to see the different shape and position of this histogram, which has a mean arrival delay of about ten seconds. First of all, there are not nearly as many zero length delays. Secondly, the most frequent value is now about 10 (as opposed to two in the previous histogram). Finally, the longest delay is now over 20 seconds (instead of 10 for the previous histogram). One other thing to try is this:</span><span class="c1">&nbsp;</span></p><p class="c2"><span class="c1">&gt; sum(rpois(1000,10)&lt;=10) </span></p><p class="c2"><span class="c1">[1] 597 </span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image38.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of hist(rpois(1000,10))"></span><span class="c5"><br></span></p><p class="c19"><span class="c14">This command generated 1000 new random numbers, using the </span><span class="c14">Poisson distribution and also with a hoped-for mean of 10, just like in the histogram on the next page. Using the &quot;</span><span class="c14 c17">&lt;=</span><span class="c14">&quot; inequality test and the </span><span class="c14 c17">sum() </span><span class="c14">function, we then counted up how many events were less than or equal to 10, and this turned out to be 597 events. As a fraction of the total of n=1000 data points that </span><span class="c14 c17">rpois() </span><span class="c5">generated, that is 0.597, or 59.7%.</span></p><p class="c42 c52"><span class="c24 c53 c23"></span></p><p class="c19"><span class="c14">We can look at the same kind of data in terms of the probability of arrival within a certain amount of time. Because </span><span class="c14 c17">rpois() </span><span class="c14">generates delay times directly (rather than us having to calculate them from neighboring arrival times), we will need a slightly different function than the </span><span class="c14 c17">ArrivalProbabilities() </span><span class="c14">that we wrote and used in the previous chapter. We&rsquo;ll call this function &quot;</span><span class="c14 c17">DelayProbability</span><span class="c5">&quot; (the code is at the end of this chapter): </span></p><p class="c2"><span class="c1">&gt; DelayProbability(rpois(100,10),1,20) </span></p><p class="c10"><span class="c1">[1] 0.00 0.00 0.00 0.03 0.06 0.09 0.21 0.33 0.48 0.61 0.73 0.82 0.92 </span></p><p class="c10"><span class="c1">[14] 0.96 0.97 0.98 0.99 1.00 1.00 1.00 </span></p><p class="c29"><span class="c14">At the heart of that command is the </span><span class="c14 c17">rpois()</span><span class="c14">&nbsp;function, requesting 100 points with a mean of 10. The other two parameters are the increment, in this case one second, and the maximum delay time, in this case 20 seconds. The output from this function is a sorted list of cumulative probabilities for the times ranging from 1 second to 20 seconds. Of course, what we would really like to do is compare these probabilities to those we would get if the average delay was three seconds instead of ten seconds. We&rsquo;re going to use two cool tricks for creating this next plot. First, we will use the </span><span class="c14 c17">points() </span><span class="c14">command to add points to an existing plot. Second, we will use the </span><span class="c14 c17">col= </span><span class="c5">parameter to specify two different colors for the points that we plot. Here&rsquo;s the code that creates a plot and then adds more points to it: </span></p><p class="c9"><span class="c26 c17 c23">&gt; plot(DelayProbability(rpois(100,10),1,20), col=2) </span></p><p class="c2"><span class="c26 c17 c23">&gt; points(DelayProbability(rpois(100,3),1,20), col=3) </span></p><p class="c19"><span class="c14"><br></span><span class="c14">Again, the heart of each of these lines of code is the </span><span class="c14 c17">rpois() </span><span class="c14">function that is generating random Poisson arrival delays for us. Our parameters for increment (1 second) and maximum (20 seconds) are the same for both lines. The first line uses </span><span class="c14 c17">col=2</span><span class="c14">, which gives us red points, and the second gives us </span><span class="c14 c17">col=3</span><span class="c5">, which yields green points: </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="&gt; plot(DelayProbability(rpois(100,10),1,20), col=2) 
&gt; points(DelayProbability(rpois(100,3),1,20), col=3)
" src="images/image4.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="2 color plot of probabilities"></span></p><p class="c19"><span class="c14">This plot clearly shows that the green points have a &quot;steeper&quot; profile. We are more likely to have earlier arrivals for the 3-second delay data than we are for the 10-second data. If these were real tweets, the green tweets would be piling in much faster than the red tweets. Here&rsquo;s a reminder on how to read this plot: Look at a value on the X-axis, for example &quot;5.&quot; Then look where the dot is and trace leftward to the Y-axis. For the red dot, the probability value at time (x) equal 4 is about 0.10. So for the red data there is about a 10% chance that the next event will occur within five time units (we&rsquo;ve been calling them seconds, but they could really be anything, as long as you use the units consistently throughout the whole example). For the green data there is about a 85% chance that the next event will occur within four time units. The fact that the green curve rises more steeply than the red curve means that </span><span class="c14 c46">for these two samples only </span><span class="c14">the green stuff is arriving </span><span class="c14 c46">much more often </span><span class="c5">than the red stuff. </span></p><p class="c7"><span class="c5">The reason we emphasized the point &quot;for these samples only&quot; is that we know from prior chapters that every sample of data you collect varies by at least a little bit and sometimes by quite a lot. A sample is just a snapshot, after all, and things can and do change from sample to sample. We can illustrate this by running and plotting multiple samples, much as we did in the earlier chapter: </span></p><p class="c2"><span class="c1">&gt; plot(DelayProbability(rpois(100,10),1,20)) </span></p><p class="c10"><span class="c1">&gt; for (i in 1:15) {points(DelayProbability(rpois(100,10),1,20))} </span></p><p class="c29"><span class="c5">This is the first time we have used the &quot;for loop&quot; in R, so let&rsquo;s walk through it. A &quot;for loop&quot; is one of the basic constructions that computer scientists use to &quot;iterate&quot; or repeatedly run a chunk of code. In R, a for loop runs the code that is between the curly braces a certain number of times. The number of times R runs the code depends on the expression inside the parentheses that immediately follow the &quot;for.&quot; </span></p><p class="c7"><span class="c14">In the example above, the expression &quot;</span><span class="c14 c17">i in 1:15</span><span class="c14">&quot; creates a new data object, called </span><span class="c14 c17">i</span><span class="c14">, and then puts the number </span><span class="c14 c17">1</span><span class="c5">&nbsp;in it. Then, the &ldquo;for loop&rdquo; keeps adding one to the value of i, until i reaches 15. Each time that it does this, it runs the code between the curly braces. The expression &quot;in 1:15&quot; tells R to start with one and count up to 15. The data object i, which is just a plain old integer, could also have been used within the curly braces if we had needed it, but it doesn&rsquo;t have to be used within the curly braces if it is not needed. In this case we didn&rsquo;t need it. The code inside the curly braces just runs a new random sample of 100 Poisson points with a hoped for mean of 10. </span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image39.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="plot using for loop"></span></p><p class="c19"><span class="c14">When you consider the two command lines on the previous page together you can see that we initiate a </span><span class="c14 c17">plot() </span><span class="c14">on the first line of code, using similar parameters to before (random poisson numbers with a mean of 10, fed into our probability calculator, which goes in increments of 1 second up to 20 seconds). In the second line we add more points to the same plot, by running exactly 15 additional copies of the same code. Using </span><span class="c14 c17">rpois() </span><span class="c5">ensures that we have new random numbers each time: </span></p><p class="c7"><span class="c5">Now instead of just one smooth curve we have a bunch of curves, and that these curves vary quite a lot. In fact, if we take the example of 10 seconds (on the X-axis), we can see that in one case the probability of a new event in 10 seconds could be as low as 0.50, while in another case the probability is as high as about 0.70. </span></p><p class="c7"><span class="c14">This shows why we can&rsquo;t just rely on one sample for making our judgments. We need to know something about the uncertainty that surrounds a given sample. Fortunately, R gives us additional tools to help us figure this situation out. First of all, even though we had loads of fun programming the </span><span class="c14 c17">DelayProbability() </span><span class="c14">function, there is a quicker way to get information about what we ideally </span><span class="c14 c46">expect </span><span class="c14">from a Poisson distribution. The function </span><span class="c14 c17">ppois() </span><span class="c14">gives us the </span><span class="c14 c46">theoretical </span><span class="c5">probability of observing a certain delay time, given a particular mean. For example: </span></p><p class="c2"><span class="c1">&gt; ppois(3, lambda=10) </span></p><p class="c10"><span class="c1">[1] 0.01033605 </span></p><p class="c29"><span class="c14">So you can read this as: There is a 1% chance of observing a delay of 3 or less in a Poisson distribution with mean equal to 10. Note that in statistical terminology, &quot;</span><span class="c14 c17">lambda</span><span class="c14">&quot; is the term used for the mean of a Poisson distribution. We&rsquo;ve provided the named parameter &quot;</span><span class="c14 c17">lambda=10</span><span class="c5">&quot; in the example above just to make sure that R does not get confused about what parameter we are controlling </span></p><p class="c19"><span class="c14">when we say &quot;10.&quot; The </span><span class="c14 c17">ppois() </span><span class="c5">function does have other parameters that we have not used here. Now, using a for loop, we could get a list of several of these theoretical probabilities: </span></p><p class="c2"><span class="c1">&gt; plot(1,20,xlim=c(0,20),ylim=c(0,1)) </span></p><p class="c10"><span class="c1">&gt; for (i in 1:20) {points(i,ppois(i,lambda=10)) } </span></p><p class="c29"><span class="c14">We are using a little code trick in the first command line above by creating a nearly empty set of axes with the </span><span class="c14 c17">plot() </span><span class="c14">function, and then filling in the points in the second line using the </span><span class="c14 c17">points() </span><span class="c5">function. This gives the following plot: </span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image9.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c19"><span class="c5">You may notice that this plot looks a lot like the ones earlier in this chapter as well as somewhat similar to the probability plot in the previous chapter. When we say the &quot;theoretical distribution&quot; we are talking about the ideal Poisson distribution that would be generated by the complex equation that Mr. Poisson invented a couple of centuries ago. Another way to think about it is this: Instead of just having a small sample of points, which we know has a lot of randomness in it, what if we had a truly humongous sample with zillions of data points? The curve in the plot above is just about what we would observe for a truly humongous sample (where most of the biases up or down cancel themselves out because of the large number of points). </span></p><p class="c7"><span class="c5">So this is the ideal, based on the mathematical theory of the Poisson distribution, or what we would be likely to observe if we created a really large sample. We know that real samples, of reasonable amounts of data, like 100 points or 1000 points or even 10,000 points, will not hit the ideal exactly, because some samples will come out a little higher and others a little lower. </span></p><p class="c7"><span class="c14">We also know, from the histograms and output earlier in the chapter, that we can look at the mean of a sample, or the count of events less than or equal to the mean, or the arrival probabilities in the graph on this page, and in </span><span class="c14 c46">each case we are looking at different versions of the same information</span><span class="c5">. Check out these five commands: </span></p><p class="c10"><span class="c1">&gt; mean(rpois(100000,10)) </span></p><p class="c28"><span class="c1">[1] 10.01009 </span></p><p class="c22"><span class="c1">&gt; var(rpois(100000,10)) </span></p><p class="c22"><span class="c1">[1] 10.02214 </span></p><p class="c22"><span class="c1">&gt; sum(rpois(100000,10)&lt;=10)/100000 </span></p><p class="c19"><span class="c1">[1] 0.58638 </span></p><p class="c22"><span class="c1">&gt; ppois(10,lambda=10) </span></p><p class="c28"><span class="c1">[1] 0.58303 </span></p><p class="c22"><span class="c1">&gt; qpois(0.58303,lambda=10) </span></p><p class="c22"><span class="c1">[1] 10 </span></p><p class="c92"><span class="c14">In the first command, we confirm that for a very large random sample of n=100,000 with a desired mean of 10, the actual mean of the random sample is almost exactly 10. Likewise, for another large random sample with a desired mean of 10, the variance is 10. In the next command, we use the inequality test and the </span><span class="c14 c17">sum() </span><span class="c14">function again to learn that the probability of observing a value of 10 or less in a very large sample is about 0.59 (note that the </span><span class="c14 c17">sum() </span><span class="c14">function yielded 58,638 and we divided by 100,000 to get the reported value of 0.58638). Likewise, when we ask for the theoretical distribution with </span><span class="c14 c17">ppois() </span><span class="c14">of observing 10 or less in a sample with a mean of 10, we get a probability of 0.58303, which is darned close to the empirical result from the previous command. Finally, if we ask the &nbsp;</span><span class="c14 c17">qpois() </span><span class="c14">function w</span><span class="c14">hat is the </span><span class="c14 c46">threshold value </span><span class="c14">for a probability of 0.58303 is in a Poisson sample with mean of 10, we get back the answer: 10. You may see that </span><span class="c14 c17">qpois() </span><span class="c14">does the reverse of what </span><span class="c14 c17">ppois() </span><span class="c5">does. For fun, try this formula on the R command line: </span></p><p class="c99"><span class="c1">&gt; qpois(ppois(10, lambda=10), lambda=10) </span></p><p class="c130"><span class="c14">Here&rsquo;s one last point to cap off this thinking. Even with a sample of 100,000 there is some variation in samples. That&rsquo;s why the 0.58638 from the </span><span class="c14 c17">sum() </span><span class="c14">function above does not exactly match the theoretical 0.58303 from the </span><span class="c14 c17">ppois() </span><span class="c14">function above. We can ask R to tell us how much variation there is around one of these probabilities using the </span><span class="c14 c17">poisson.test() </span><span class="c14">function like this: <br></span></p><p class="c19"><span class="c1">&gt; poisson.test(58638, 100000) </span></p><p class="c10"><span class="c1">95 percent confidence interval: </span></p><p class="c10"><span class="c1">0.5816434 0.5911456 </span></p><p class="c7"><span class="c14">We&rsquo;ve truncated a little of the output in the interests of space: What you have left is the upper and lower bounds on a 95% confidence interval. Here&rsquo;s what a confidence interval is: For 95% of the samples that we could generate using </span><span class="c14 c17">rpois()</span><span class="c14">, using a sample size of 100,000, and a desired mean of 10, we will get a result that lies between 0.5816434 and 0.5911456 (remember that this resulting proportion is calculated as the total number of events whose delay time is 10 or less). So we know what would happen for 95% of the </span><span class="c14 c17">rpois() </span><span class="c14">samples, but the assumption that statisticians also make is that </span><span class="c14 c46">if </span><span class="c5">a natural phenomenon, like the arrival time of tweets, also fits the Poisson distribution, that this same confidence interval would be operative. So while we know that we got 0.58638 in one sample on the previous page, it is likely that future samples will vary by a little bit (about 1%). Just to get a feel for what happens to the confidence interval with smaller samples, look at these: </span></p><p class="c10"><span class="c14 c17">&gt; poisson.test(</span><span class="c14 c80 c68">5863, 10000</span><span class="c1">) </span></p><p class="c10"><span class="c1">95 percent confidence interval: </span></p><p class="c10"><span class="c1">0.5713874 0.6015033 </span></p><p class="c2"><span class="c14 c17">&gt; poisson.test(</span><span class="c14 c80 c68">586, 1000</span><span class="c1">) </span></p><p class="c10"><span class="c1">95 percent confidence interval: </span></p><p class="c10"><span class="c1">0.5395084 0.6354261 </span></p><p class="c10"><span class="c14 c17">&gt; poisson.test(</span><span class="c14 c68 c80">58, 100</span><span class="c1">) </span></p><p class="c19"><span class="c1">95 percent confidence interval: </span></p><p class="c10"><span class="c1">0.4404183 0.7497845 </span></p><p class="c7"><span class="c14">We&rsquo;ve bolded the parameters that changed in each of the three commands above, just to emphasize that in each case we&rsquo;ve reduced the sample size by a factor of 10. By the time we get to the bottom look how wide the confidence interval gets. With a sample of 100 events, of which 58 had delays of 10 seconds or less, the confidence interval around the proportion of 0.58 ranges from a low of 0.44 to a high of 0.75! That&rsquo;s huge! The confidence interval gets wider and wider as we get </span><span class="c14 c46">less and less confident about the accuracy of our estimate. </span><span class="c14">In the case of a small sample of 100 events, the confidence interval is very wide, showing that we have a lot of uncertainty about our estimate that 58 events out of 100 will have arrival delays of 10 or less. Note that you can filter out the rest of the stuff that </span><span class="c14 c17">poisson.test() </span><span class="c14">generates by asking specifically for the &quot;</span><span class="c14 c17">conf.int</span><span class="c5">&quot; in the output that is returned: </span></p><p class="c10"><span class="c14 c17">&gt; poisson.test(58, 100)</span><span class="c59 c14 c23 c80 c68">$conf.int </span></p><p class="c10"><span class="c1">[1] 0.4404183 0.7497845 </span></p><p class="c10"><span class="c1">attr(,&quot;conf.level&quot;) </span></p><p class="c2"><span class="c1">[1] 0.95 </span></p><p class="c29"><span class="c14">The bolded part of the command line above shows how we used the $ notation to get a report of just the bit of output that we wanted from </span><span class="c14 c17">poisson.test()</span><span class="c5">. This output reports the exact same confidence interval that we saw on the previous page, along with a reminder in the final two lines that we are looking at a 95% confidence interval. </span></p><p class="c29"><span class="c5">At this point we have all of the knowledge and tools we need to compare two sets of arrival rates. Let&rsquo;s grab a couple of sets of tweets and extract the information we need. Open the HWK Assignment for this Chapter in R-Studio. First, we will import the dataset GagaData.xlsl into the environment. Name the variable for importing tweetDF. </span></p><p class="c29"><span class="c14">Next, we need to sort the tweets by arrival time, That is, of course, unless you accepted the Chapter Challenge in the previous chapter and built the sorting into your </span><span class="c14 c17">TweetFrame() </span><span class="c5">function. </span></p><p class="c2"><span class="c1">sortweetDF&lt;-tweetDF[order(as.integer(tweetDF$created)), ] </span></p><p class="c29"><span class="c14">Now, we&rsquo;ll extract a vector of the time differences. In the previous chapter the use of the </span><span class="c14 c17">diff() </span><span class="c14">function occurred within the </span><span class="c14 c17">ArrivalProbability() </span><span class="c5">function that we developed. Here we will use it directly and save the result in a vector: </span></p><p class="c2"><span class="c1">eventDelays &lt;- as.integer(diff(sortweetDF$created)) </span></p><p class="c29"><span class="c5">Now we can calculate a few of the things we need in order to get a picture of the arrival delays for Lady Gaga&rsquo;s tweets: </span></p><p class="c2"><span class="c1">&gt; mean(eventDelays) </span></p><p class="c10"><span class="c1">[1] 30.53707 </span></p><p class="c10"><span class="c1">&gt; sum(eventDelays&lt;=31) </span></p><p class="c10"><span class="c1">[1] 333 </span></p><p class="c29"><span class="c5">So, for Lady Gaga tweets, the mean arrival delay for the next tweet is just short of 31 seconds. Another way of looking at that same sta</span></p><p class="c19"><span class="c14">tistic is that 333 out of 500 tweets (0.666, about two thirds) arrived within 31 seconds of the previous tweet. We can also ask </span><span class="c14 c17">poisson.test() </span><span class="c5">to show us the confidence interval around that value: </span></p><p class="c2"><span class="c1">&gt; poisson.test(333,500)$conf.int </span></p><p class="c10"><span class="c1">[1] 0.5963808 0.7415144 </span></p><p class="c10"><span class="c1">attr(,&quot;conf.level&quot;) </span></p><p class="c10"><span class="c1">[1] 0.95 </span></p><p class="c7"><span class="c5">So, this result suggests that for 95% of the Lady Gaga samples of tweets that we might pull from the Twitter system, the proportion arriving in 31 seconds or less would fall in this confidence band. In other words, we&rsquo;re not very likely to see a sample with a proportion under 59.6% or over 74.1%. That&rsquo;s a pretty wide band, so we do not have a lot of exactitude here. </span></p><p class="c7"><span class="c5">Now let&rsquo;s get the same data for Oprah: </span></p><p class="c2"><span class="c14">Import the OprahData.xlxs file into a variable named tweetDF</span></p><p class="c10"><span class="c1">&gt; sortweetDF&lt;- + tweetDF[order(as.integer(tweetDF$created)), ] </span></p><p class="c10"><span class="c1">&gt; eventDelays &lt;- + as.integer(diff(sortweetDF$created)) </span></p><p class="c10"><span class="c1">&gt; mean(eventDelays) </span></p><p class="c10"><span class="c1">[1] 423.01 </span></p><p class="c7"><span class="c5">Hmm, I guess we know who is the boss here! Now let&rsquo;s finish the job: </span></p><p class="c10"><span class="c1">&gt; sum(eventDelays&lt;=31) </span></p><p class="c10"><span class="c14 c17">[1] 73<br></span></p><p class="c19"><span class="c1">&gt; poisson.test(73,500)$conf.int </span></p><p class="c10"><span class="c1">[1] 0.1144407 0.1835731 </span></p><p class="c10"><span class="c1">attr(,&quot;conf.level&quot;) </span></p><p class="c2"><span class="c1">[1] 0.95 </span></p><p class="c29"><span class="c14">The </span><span class="c14 c17">sum() </span><span class="c14">function, above, calculates that only 73 out of Oprah&rsquo;s sample of 500 tweets arrive in an interval of 31 or less. We use 31, the mean of the Lady Gaga sample, because we need to have a common basis of comparison. So for Oprah, the proportion of events that occur in the 31 second time frame is, 73/500 = 0.146, or about 14.6%. That&rsquo;s a lot lower than the 66.6% of Lady Gaga tweets, for sure, but we need to look at the confidence interval around that value. So the </span><span class="c14 c17">poisson.test() </span><span class="c14">function just above for Oprah reports that the 95% confidence interval runs from about 11.4% to 18.4%. Note that this confidence interval does not overlap at all with the confidence interval for Lady Gaga, so we have a very strong sense that these two rates are statistically quite distinctive in other words, this is a difference that was not caused by the random influences that sampling always creates. We can make a bar graph to summarize these differences. We&rsquo;ll use the </span><span class="c14 c17">barplot2() </span><span class="c14">function, which is in a package called </span><span class="c14 c17">gplots()</span><span class="c14">. If you created the </span><span class="c14 c17">EnsurePackage()</span><span class="c5">&nbsp;function a couple of chapters ago, you can use that. Otherwise make sure to load gplots manually: </span></p><p class="c2"><span class="c1">&gt; EnsurePackage(&quot;gplots&quot;) </span></p><p class="c10"><span class="c1">&gt; barplot2(c(0.666,0.146), + </span></p><p class="c142"><span class="c1">ci.l=c(0.596,0.114), + ci.u=c(0.742,0.184), + </span></p><p class="c19"><span class="c1">plot.ci=TRUE, + names.arg=c(&quot;Gaga&quot;,&quot;Oprah&quot;)) </span></p><p class="c85"><span class="c14">This is not a particularly efficient way to use the </span><span class="c14 c17">barplots() </span><span class="c14">function, because we are supplying our data by typing it in, using the </span><span class="c14 c17">c() </span><span class="c14">function to create short vectors of values on the command line. On the first line,, we supply a list of the means from the two samples, expressed as proportions. On the next two lines we first provide the lower limits of the confidence intervals and then the upper limits. The </span><span class="c14 c17">plot.ci=TRUE</span><span class="c14">&nbsp;parameter asks </span><span class="c14 c17">barplot2()</span><span class="c14">&nbsp;to put confidence interval whiskers on each bar. The final line provides labels to put underneath the bars. What we get is below.</span></p><p class="c85"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 489.50px; height: 346.43px;"><img alt="" src="images/image52.png" style="width: 489.50px; height: 346.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Gaga vs Obra Tweets"></span></p><p class="c85"><span class="c5">This is not an especially attractive bar plot, but it does represent the information we wanted to display accurately. And with the assistance of this plot, it is easy to see both the substantial difference between the two bars and the fact that the confidence intervals do not overlap. </span></p><p class="c7"><span class="c14">For one final confirmation of our results, we can ask the </span><span class="c14 c17">poisson.text() </span><span class="c14">function to evaluate our two samples together. This code provides the same information to </span><span class="c14 c17">poisson.test() </span><span class="c5">as before, but now provides the event counts as short lists describing the two samples, with 333 events (under 31 seconds) for Lady Gaga and 73 events for Oprah, in both cases out of 500 events: <br></span></p><p class="c19"><span class="c1">&gt; poisson.test(c(333,73),c(500,500)) </span></p><p class="c42 c111"><span class="c1">Comparison of Poisson rates </span></p><p class="c42"><span class="c1">data: c(333, 73) time base: c(500, 500) </span></p><p class="c42"><span class="c1">count1 = 333, expected count1 = 203, p-value &lt; 2.2e-16 </span></p><p class="c42"><span class="c1">alternative hypothesis: true rate ratio is not equal to 1 </span></p><p class="c42"><span class="c1">95 percent confidence interval: </span></p><p class="c42"><span class="c1">3.531401 5.960511 </span></p><p class="c42"><span class="c1">sample estimates: </span></p><p class="c42"><span class="c1">rate ratio </span></p><p class="c42"><span class="c1">4.561644 </span></p><p class="c19 c52"><span class="c1"></span></p><p class="c19"><span class="c14">Let&rsquo;s walk through this output line by line. Right after the command, we get a brief confirmation from the function that we&rsquo;re comparing two event rates in this test rather than just evaluating a single rate: &quot;Comparison of Poisson rates.&quot; The next line confirms the data we provided. The next line, that begins with &quot;</span><span class="c14 c17">count1 = 333</span><span class="c5">&quot; confirms the basis of of the comparison and then shows a &quot;pooled&quot; count that is the weighted average of 333 and 73. The pvalue on that same line represents the position of a probability tail for &quot;false positives.&quot; Together with the information on the next line, &quot;alternative hypothesis,&quot; this constitutes what statisticians call a &quot;null hypothesis significance test.&quot; Although this is widely used in academic research, it contains less useful information than confidence intervals and we will ignore it for now. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c14">The next line, &quot;95% confidence interval,&quot; is a label for the most important information, which is on the line that follows. The values of 3.53 and 5.96 represent the upper and lower limits of the 95% </span><span class="c14 c46">confidence interval around the observed rate ratio </span><span class="c5">of 4.56 (reported on the final line). So, for 95% of samples that we might draw from twitter, the ratio of the Gaga/Oprah rates might be as low as 3.53 and as high as 5.96. So we can be pretty sure (95% confidence) that Lady Gaga gets tweets at least 3.5 times as fast as Oprah. Because the confidence interval does not include 1, which would be the same thing as saying that the two rates are identical, we can be pretty certain that the observed rate ratio of 4.56 is not a statistical fluke. </span></p><p class="c29"><span class="c5">For this comparison, we chose two topics that had very distinctive event rates. As the bar chart on the previous page attests, there was a substantial difference between the two samples in the rates of arrival of new tweets. The statistical test confirmed this for us, and although the ability to calculate and visualize the confidence intervals was helpful, we probably could have guessed that such a large difference over a total of 1000 tweets was not a result due to sampling error. </span></p><p class="c7"><span class="c5">With other topics and other comparisons, however, the results will not be as clear cut. After completing the chapter challenge on the next page, we compared the &quot;#obama&quot; hashtag to the &quot;#romney&quot; hashtag. Over samples of 250 tweets each, Obama had 159 events at or under the mean, while Romney had only 128, for a ratio of 1.24 in Obama&rsquo;s favor. The confidence interval told a different story, however: the lower bound of the confidence interval was 0.978, very close to, but slightly below one. This signifies that we can&rsquo;t rule out the possibility that the two rates are, in fact, equal and that the slightly higher rate (1.24 to 1) that we observed for Obama in this one sample might have come about due to sampling error. When a confidence interval overlaps the point where we consider something to be a &quot;null result&quot; (in this case a ratio of 1:1) we have to take seriously the possibility that peculiarities of the sample(s) we drew created the observed difference, and that a new set of samples might show the opposite of what we found this time. </span></p><p class="c7"><span class="c59 c14 c57 c23">Chapter Challenge </span></p><p class="c7"><span class="c14">Write a function that takes two search strings as arguments and that returns the results of a Poisson rate ratio test on the arrival rates of tweets on the two topics. Your function should first run the necessary Twitter searches, then sort the tweets by ascending time of arrival and calculate the two vectors of time differentials. Use the mean of one of these vectors as the basis for comparison and for each vector, count how many events are at or below the mean. Use this information and the numbers of tweets requested to run the </span><span class="c14 c17">poisson.test() </span><span class="c5">rate comparison. </span></p><h3 class="c20" id="h.khllbfyy2zg2"><span class="c38 c57 c23 c18">Sources </span></h3><p class="c7"><span class="c14 c57 c46 c23 c97">Barplots </span></p><ul class="c32 lst-kix_hdtt0qm0bdu2-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph&amp;sa=D&amp;source=editors&amp;ust=1751550345266960&amp;usg=AOvVaw2G3l9uPOgsC9NJC1CbWyCy">http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph =54 </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatGraphCo&amp;sa=D&amp;source=editors&amp;ust=1751550345267379&amp;usg=AOvVaw011zy0kqnJzcFS9R9IbN3g">http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatGraphCo urse/graphscourse.pdf </a></span></li><li class="c29 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id%3Dgplots:barplot2&amp;sa=D&amp;source=editors&amp;ust=1751550345267683&amp;usg=AOvVaw32elR38iafAnZEL3eqN08I">http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=gplots:barplot2</a></span><span class="c11">&nbsp;</span></li></ul><p class="c29"><span class="c24 c23 c18">&nbsp;</span></p><p class="c19"><span class="c14 c57 c46 c23 c97">Poisson Distribution </span></p><ul class="c32 lst-kix_rwyosv65luta-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://books.google.com/books?id%3DZKswvkqhygYC%26printsec%3Dfr&amp;sa=D&amp;source=editors&amp;ust=1751550345268100&amp;usg=AOvVaw2mauZ-6qfnPtkQW66sr2bq">http://books.google.com/books?id=ZKswvkqhygYC&amp;printsec=fr ontcover </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.khanacademy.org/math/probability/v/poisson-process-1&amp;sa=D&amp;source=editors&amp;ust=1751550345268399&amp;usg=AOvVaw2eJavMAh0P-zHJzR2CaDhW">http://www.khanacademy.org/math/probability/v/poisson-process-1 </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.khanacademy.org/math/probability/v/poisson-process-2&amp;sa=D&amp;source=editors&amp;ust=1751550345268634&amp;usg=AOvVaw1CvW0ZVMr7HCyK4-Cxp02C">Khan Academy Poisson Process 2</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://stat.ethz.ch/R-manual/R-patched/library/stats/html/Poisson.html&amp;sa=D&amp;source=editors&amp;ust=1751550345268851&amp;usg=AOvVaw32AnOKBivV_HYxspi-p9Ci">The Poisson Distribution</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://stat.ethz.ch/R-manual/R-patched/library/stats/html/poisson.test.html&amp;sa=D&amp;source=editors&amp;ust=1751550345269054&amp;usg=AOvVaw1bfvu3hqkJ6rjayGVoHpMX">Exact Poisson tests</a></span></li><li class="c152 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stats.stackexchange.com/questions/10926/how-to-calculat&amp;sa=D&amp;source=editors&amp;ust=1751550345269328&amp;usg=AOvVaw0ebIJDJ-uZ8TwyVJisF9PZ">How to calculate confidence interval for count data in R?</a></span></li></ul><h3 class="c20" id="h.dveiphdpb5z9"><span class="c38 c57 c23 c18">R Functions Used in this Chapter </span></h3><ul class="c32 lst-kix_o0jblmcky2dx-0 start"><li class="c7 c13 li-bullet-0"><span class="c14 c17">as.integer() </span><span class="c5">Coerces another data type to integer if possible </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">barplot2() </span><span class="c5">Creates a bar graph </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">c() </span><span class="c5">Concatenates items to make a list </span></li><li class="c0 li-bullet-0"><span class="c14 c17">diff() </span><span class="c5">Calculates time difference on neighboring cases </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">EnsurePackage() </span><span class="c14">Custom function, </span><span class="c14 c17">install() </span><span class="c14">and </span><span class="c14 c17">require() </span><span class="c5">package </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">for() </span><span class="c5">Creates a loop, repeating execution of code </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">hist() </span><span class="c5">Creates a frequency histogram </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">mean() </span><span class="c5">Calculates the arithmetic mean </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">order() </span><span class="c5">Provides a list of indices reflecting a new sort order </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">plot() </span><span class="c5">Begins an X-Y plot </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">points() </span><span class="c14">Adds points to a plot started with </span><span class="c1">plot() </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">poisson.test() </span><span class="c5">Confidence intervals for poisson events or ratios </span></li><li class="c105 c13 li-bullet-0"><span class="c14 c17">qpois() </span><span class="c14">Does the inverse of </span><span class="c14 c17">ppois()</span><span class="c5">: Probability into threshold </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">rpois() </span><span class="c5">Generates random numbers fitting a Poisson distribution </span></li><li class="c13 c152 li-bullet-0"><span class="c14 c17">TweetFrame() </span><span class="c5">Custom procedure yielding a dataset of tweets </span></li><li class="c0 li-bullet-0"><span class="c14 c17">ppois() </span><span class="c5">Returns a cumulative probability for particular threshold </span></li><li class="c13 c93 li-bullet-0"><span class="c14 c17">sum() </span><span class="c5">Adds together a list of numbers </span></li><li class="c13 c105 li-bullet-0"><span class="c14 c17">var() </span><span class="c5">Calculates variance of a list of numbers </span></li></ul><p class="c72"><span class="c53 c68">Question: </span><span class="c53 c36">The Poisson distribution has a characteristic shape that would be described as: </span></p><ol class="c32 lst-kix_rvej8okh7pu0-0 start" start="1"><li class="c42 c13 li-bullet-0"><span class="c24 c53 c23">Negatively (left) skewed</span></li><li class="c42 c13 li-bullet-0"><span class="c24 c53 c23">Positively (right) skewed </span></li><li class="c42 c13 li-bullet-0"><span class="c24 c53 c23">Symmetric (not skewed)</span></li><li class="c42 c13 li-bullet-0"><span class="c53 c64">None of these</span></li></ol><h3 class="c39" id="h.xhkr1e4brues"><span><br></span><span class="c38 c23 c18 c57">R Script Create Vector of Probabilities From Delay Times </span></h3><p class="c42"><span class="c26 c17 c23"># Like ArrivalProbability, but works with unsorted list </span></p><p class="c42"><span class="c26 c17 c23"># of delay times </span></p><p class="c42"><span class="c26 c17 c23">DelayProbability&lt;-function(delays, increment, max)</span></p><p class="c42"><span class="c26 c17 c23">{</span></p><p class="c42"><span class="c26 c17 c23"># Initialize an empty vector</span></p><p class="c42"><span class="c26 c17 c23">plist &lt;- NULL</span></p><p class="c42"><span class="c26 c17 c23"># Probability is defined over the size of this sample</span></p><p class="c42"><span class="c26 c17 c23"># of arrival times</span></p><p class="c42"><span class="c26 c17 c23">delayLen &lt;- length(delays)</span></p><p class="c42"><span class="c26 c17 c23"># May not be necessary, but checks for input mistake</span></p><p class="c42"><span class="c26 c17 c23">if (increment&gt;max) {return(NULL)}</span></p><p class="c42"><span class="c26 c17 c23">for (i in seq(increment, max, by=increment))</span></p><p class="c42"><span class="c26 c17 c23">{</span></p><p class="c42"><span class="c26 c17 c23"># logical test &lt;=i provides list of TRUEs and FALSEs</span></p><p class="c42"><span class="c26 c17 c23"># of length = timeLen, then sum() counts the TRUEs</span></p><p class="c42 c111"><span class="c26 c17 c23">plist&lt;-c(plist,(sum(delays&lt;=i)/delayLen))</span></p><p class="c42"><span class="c26 c17 c23">}</span></p><p class="c42"><span class="c26 c17 c23">return(plist)</span></p><p class="c42"><span class="c17 c23 c26">}</span></p><p class="c42 c52"><span class="c26 c17 c23"></span></p><p class="c42 c52"><span class="c26 c17 c23"></span></p><p class="c81 c62 title" id="h.kuf20lncm26o"><span class="c70">CHAPTER 12</span><span><br></span><span>String Theory<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 419.50px; height: 314.85px;"><img alt="" src="images/image6.png" style="width: 419.50px; height: 314.85px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span><br></span><span class="c14 c46">Prior chapters focused on statistical analysis of tweet arrival times and built on earlier knowledge of samples and distributions. This chapter switches gears to focus on manipulating so-called &quot;unstructured&quot; data, which in most cases means natural language texts. Tweets are again a useful source of data for this because tweets are mainly a short (280 characters or less) character strings.</span><span class="c37 c36"><br><br></span><span class="c14 c64">Yoiks, that last chapter was very challenging! Lots of numbers, lots of statistical concepts, lots of graphs. Let&rsquo;s take a break from all that (temporarily) and focus on a different kind of data for a while. If you think about the Internet, and specifically about the World Wide Web for a while, you will realize: 1) That there are zillions of web pages; and 2) That most of the information on those web pages is &quot;unstructured,&quot; in the sense that it does not consist of nice rows and columns of numeric data with measurements of time or other attributes. Instead, most of the data spread out across the Internet is text, digital photographs, or digital videos. These last two categories are interesting, but we will have to postpone consideration of them while we consider the question of text.</span><span class="c59 c14 c57 c23">&nbsp;</span></p><p class="c29"><span class="c5">Text is, of course, one of the most common forms of human communication, hence the label that researchers use sometimes: natural language. When we say natural language text we mean words created by humans and for humans. With our cool computer technology, we have collectively built lots of ways of dealing with natural language text. At the most basic level, we have a great system for representing individual characters of text inside of computers called &quot;Unicode.&quot; Among other things Unicode provides for a binary representation of characters in most of the world&rsquo;s written languages, over 110,000 characters in all. Unicode supersedes ASCII (the American Standard Code for Information Interchange), which was one of the most popular standards (especially in the U.S.) for representing characters from the dawn of the computer age. </span></p><p class="c7"><span class="c5">With the help of Unicode, most computer operating systems, and most application programs that handle text have a core strategy for representing text as lists of binary codes. Such lists are commonly referred to as &quot;character strings&quot; or in most cases just &quot;strings.&quot; One </span></p><p class="c19"><span class="c5">of the most striking things about strings from a computer programming perspective is that they seem to be changing their length all the time. You can&rsquo;t perform the usual mathematical operations on character strings the way you can with numbers no multiplication or division but it is very common to &quot;split&quot; strings into smaller strings, and to &quot;add&quot; strings together to form longer strings. So while we may start out with, &quot;the quick brown fox,&quot; we may end up with &quot;the quick brown&quot; in one string and &quot;fox&quot; in another, or we may end up with something longer like, &quot;the quick brown fox jumped over the lazy dog.&quot; </span></p><p class="c7"><span class="c5">Fortunately, R, like most other data handling applications, has a wide range of functions for manipulating, keeping track of, searching, and even analyzing string data. In this chapter, we will use our budding skills working with tweet data to learn the essentials of working with unstructured text data. The learning goal here is simply to become comfortable with examining and manipulating text data. We need these basic skills before we can tackle a more interesting problem. </span></p><p class="c7"><span class="c14">Let&rsquo;s begin by loading a new package, called &quot;</span><span class="c14 c17">stringr</span><span class="c14">&quot;. Although R has quite a few string functions in its core, they tend to be a bit disorganized. So Hadley Wickham, a professor of statistics at Rice University, created this &quot;</span><span class="c14 c17">stringr</span><span class="c14">&quot; package to make a set of string manipulation functions a bit easier to use and more comprehensive. You can</span><span class="c14 c17">&nbsp;install() </span><span class="c14">and </span><span class="c14 c17">library()</span><span class="c14">&nbsp;this package using the point and click features of R-Studio (look in the lower right hand pane under the Packages tab), or if you created the </span><span class="c14 c17">EnsurePackage() </span><span class="c5">function from a couple of chapters back, you can use that: </span></p><p class="c2"><span class="c1">&gt; EnsurePackage(&quot;stringr&quot;) </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c14">Now we can grab a new set of tweets with our custom function </span><span class="c14 c17">TweetFrame() </span><span class="c5">from a couple of chapters ago (if you need the code, look in the chapter entitled &quot;Tweet, Tweet&quot;; we&rsquo;ve also pasted the enhanced function, that sorts the tweets into arrival order, into the end of this chapter): </span></p><p class="c2"><span class="c1">tweetDF &lt;- TweetFrame(&quot;#solar&quot;,100) </span></p><p class="c29"><span class="c14">This command should return a data frame containing about 100 tweets, mainly having to do with solar energy. You can choose any topic you like all of the string techniques we examine in this chapter are widely applicable to any text strings. We should get oriented by taking a look at what we retrieved. The </span><span class="c14 c17">head() </span><span class="c5">function can return the first entries in any vector or list: </span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 545.00px; height: 248.00px;"><img alt="" src="images/image45.png" style="width: 545.00px; height: 248.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="head(tweetDF1)"></span></p><p class="c43"><span class="c5">We provide a screenshot from R-Studio here just to preserve the formatting of this output. In the left hand margin, the number 97 represents R&rsquo;s indexing of the original order in which the tweet </span></p><p class="c19"><span class="c14">was received. The tweets were re-sorted into arrival order by our enhanced </span><span class="c14 c17">TweetFrame()</span><span class="c14">&nbsp;function (see the end of the chapter for code). So this is the first element in our dataframe, but internally R has numbered it as 97 out of the 100 tweets we obtained. On the first line of the output, R has placed the label &quot;</span><span class="c14 c17">text</span><span class="c14">&quot; and this is the field name of the column in the dataframe that contains the texts of the tweets. Other dataframe fields that we will not be using in this chapter include: &quot;</span><span class="c14 c17">favorited</span><span class="c14">,&quot; &quot;</span><span class="c14 c17">replyToSN</span><span class="c14">,&quot; and &quot;</span><span class="c14 c17">truncated</span><span class="c14">.&quot; You may also recognize the field name &quot;</span><span class="c14 c17">created</span><span class="c5">&quot; which contains the POSIX format time and date stamp that we used in previous chapters. </span></p><p class="c99"><span class="c14">Generally speaking, R has placed the example data (from tweet 97) that goes with the field name just underneath it, but the text justification can be confusing, and it makes this display very hard to read. For example, there is a really long number that starts with &quot;1908&quot; that is the unique numeric identifier (a kind of serial number) for this tweet. The field name &quot;</span><span class="c14 c17">id</span><span class="c14">&quot; appears just above it, but is right justified (probably because the field is a number). The most important fact for us to note is that if we want to work with the text string that is the tweet itself, we need to use the field name &quot;text.&quot; Let&rsquo;s see if we can get a somewhat better view if we use the </span><span class="c14 c17">head()</span><span class="c14">&nbsp;function just on the text field. This command should provide just the first 2 entries in the &quot;</span><span class="c14 c17">text</span><span class="c5">&quot; column of the dataframe: </span></p><p class="c2"><span class="c1">&gt; head(tweetDF$text,2) </span></p><p class="c10"><span class="c14 c17">[1] &quot;If your energy needs increase after you install a #solar system can you upgrade? Our experts have the answer! </span><span class="c4 c17"><a class="c3" href="https://www.google.com/url?q=http://t.co/ims8gDWW&amp;sa=D&amp;source=editors&amp;ust=1751550345282459&amp;usg=AOvVaw3Tucqk97L7KXoZrsKnWn5o">http://t.co/ims8gDWW</a></span><span class="c14 c17">&quot; <br></span><span class="c38 c17 c23 c18">&nbsp;</span></p><p class="c19"><span class="c14 c17">[2] &quot;#green New solar farms in West Tennessee signal growth: Two new solar energy farms producing electricity ... </span><span class="c4 c17"><a class="c3" href="https://www.google.com/url?q=http://t.co/37PKAF3N&amp;sa=D&amp;source=editors&amp;ust=1751550345282807&amp;usg=AOvVaw2pOyE-cTMVgzi66kUPbB3c">http://t.co/37PKAF3N #solar</a></span><span class="c1">&quot; </span></p><p class="c129"><span class="c14">A couple of things which will probably seem obvious, but are nonetheless important to point out: The [1] and [2] are not part of the tweet, but are the typical line numbers that R uses in its output. The actual tweet text is between the double quotes. You can see the hashtag &quot;#solar&quot; appears in both tweets, which makes sense because this was our search term. There is also a second hashtag in the first tweet &quot;#green&quot; so we will have to be on the lookout for multiple hashtags. There is also a &quot;shortened&quot; URL in each of these tweets. If a Twitter user pastes in the URL of a website to which they want to refer people, the Twitter software automatically shortens the URL to something that begins with &quot;</span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://t.co/&amp;sa=D&amp;source=editors&amp;ust=1751550345283635&amp;usg=AOvVaw1DQUDj24pJ6pIgeYDMRBn_">http://t.co/</a></span><span class="c5">&quot; in order to save space in the tweet. </span></p><p class="c7"><span class="c5">An even better way to look at these data, including the text and the other fields is to use the data browser that is built into R-Studio. If you look in the upper right hand pane of R-Studio, and make sure that the Workspace tab is clicked, you should see a list of available </span></p><p class="c19"><span class="c14">dataframes, under the heading &quot;</span><span class="c14 c17">Data</span><span class="c14">.&quot; One of these should be &quot;</span><span class="c14 c17">tweetDF</span><span class="c14">.&quot; If you click on </span><span class="c14 c17">tweetDF</span><span class="c14">, the data browser will open in the upper left hand pane of R-Studio and you should be able to see the first field or two of the first dozen rows. Here&rsquo;s a screenshot: <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 545.00px; height: 145.00px;"><img alt="" src="images/image25.png" style="width: 545.00px; height: 145.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="tweetDF"></span></p><p class="c7"><span class="c14">This </span><span class="c14">screenshot confirms what we observed in the command line output, but gives us a much more appealing and convenient way of looking through our data. Before we start to manipulate our strings, let&rsquo;s </span><span class="c14 c17">attach() tweetDF </span><span class="c14">so that we don&rsquo;t have to keep using the $ notation to access the text field. And before that, let&rsquo;s check what is already attached with the</span><span class="c14 c17">&nbsp;search()</span><span class="c5">&nbsp;function: </span></p><p class="c10"><span class="c44 c23 c54 c36">&gt; search() </span></p><p class="c82"><span class="c44 c23 c54 c36">[1] &quot;.GlobalEnv&quot; &quot;sortweetDF&quot; &quot;package:gplots&quot; </span></p><p class="c82"><span class="c44 c23 c54 c36">[4] &quot;package:KernSmooth&quot; &quot;package:grid&quot; &quot;package:caTools&quot; </span></p><p class="c92"><span class="c14">We&rsquo;ve truncated this list to save space, but you can see on the first line &quot;</span><span class="c14 c17">sortweetDF</span><span class="c14">&quot; left over from our work in a previous chapter. The other entries are all function packages that we want to keep active. So let&rsquo;s </span><span class="c14 c17">detach()</span><span class="c14">&nbsp;</span><span class="c14 c17">sortweetDF </span><span class="c14">and </span><span class="c1">attach() tweetDF: </span></p><p class="c2"><span class="c1">&gt; detach(sortweetDF) </span></p><p class="c10"><span class="c1">&gt; attach(tweetDF) </span></p><p class="c29"><span class="c14">These commands should yield no additional output. If you get any messages about &quot;The following object(s) are masked from...&quot; you should run</span><span class="c14 c17">&nbsp;search() </span><span class="c14">again and look for other dataframes that should be detached before proceeding. Once you can run attach(&quot;</span><span class="c14 c17">tweetDF</span><span class="c14">&quot;) without any warnings, you can be sure that the fields in this dataframe are ready to use without interference. <br></span><span class="c24 c23 c18">&nbsp;</span></p><p class="c19"><span class="c14">The first and most basic thing to do with strings is to see how long they are. The stringr package gives us the </span><span class="c14 c17">str_length() </span><span class="c5">function to accomplish this task: </span></p><p class="c2"><span class="c1">&gt; str_length(text) </span></p><p class="c9"><span class="c59 c17 c23 c48 c36">[1] 130 136 136 128 98 75 131 139 85 157 107 49 75 139 136 136 </span></p><p class="c22"><span class="c59 c17 c23 c48 c36">[17] 136 72 73 136 157 123 160 142 142 122 122 122 122 134 82 87 </span></p><p class="c82"><span class="c59 c17 c23 c48 c36">[33] 89 118 94 74 103 91 136 136 151 136 139 135 70 122 122 136 </span></p><p class="c22"><span class="c59 c17 c23 c48 c36">[49] 123 111 83 136 137 85 154 114 117 98 125 138 107 92 140 119 </span></p><p class="c82"><span class="c59 c17 c23 c48 c36">[65] 92 125 84 81 107 107 73 73 138 63 137 139 131 136 120 124 </span></p><p class="c22"><span class="c59 c17 c23 c48 c36">[81] 124 114 78 118 138 138 116 112 101 94 153 79 79 125 125 102 </span></p><p class="c22"><span class="c59 c17 c23 c48 c36">[97] 102 139 138 153 </span></p><p class="c9"><span class="c5">These are the string lengths of the texts as reported to the command line. It is interesting to find that there are a few of them that are longer than 280 characters: </span></p><p class="c2"><span class="c1">&gt; tail(text,1) </span></p><p class="c10"><span class="c1">[1] &quot;RT @SolarFred: Hey, #solar &amp; wind people. Tell @SpeakerBoehner and @Reuters that YOU have a green job and proud to be providing energy Independence to US&quot; </span></p><p class="c7"><span class="c14">As you can see, the </span><span class="c14 c17">tail() </span><span class="c14">command works like the head() command except from the bottom up rather than the top down. So we have learned that under certain circumstances Twitter apparently does allow tweets longer than 280 characters. Perhaps the initial phrase &quot;</span><span class="c14 c17">RT @SolarFred</span><span class="c14">&quot; does not count against the total. By the way &quot;</span><span class="c14 c17">RT</span><span class="c5">&quot; stands for &quot;retweet&quot; and it indicates when the receiver of a tweet has passed along the same message to his or her followers. </span></p><p class="c19"><span class="c5">We can glue the string lengths onto the respective rows in the dataframe by creating a new field/column: </span></p><p class="c2"><span class="c1">tweetDF$textlen &lt;str_length(text) </span></p><p class="c29"><span class="c14">After running this line of text, you should use the data browser in R-studio to confirm that the </span><span class="c14 c17">tweetDF </span><span class="c14">now has a new column of data labeled &quot;</span><span class="c14 c17">textlen</span><span class="c14">&quot;. You will find the new column all the way on the rightmost side of the dataframe structure. One peculiarity of the way R treats attached data is that you will not be able to access the new field without the </span><span class="c14 c17">$</span><span class="c14">&nbsp;notation unless you </span><span class="c14 c17">detach() </span><span class="c14">and then again </span><span class="c14 c17">attach() </span><span class="c5">the data frame. One advantage of grafting this new field onto our existing dataframe is that we can use it to probe the dataframe structure: </span></p><p class="c10"><span class="c1">&gt; detach(tweetDF) </span></p><p class="c2"><span class="c1">&gt; attach(tweetDF) </span></p><p class="c10"><span class="c1">&gt; tweetDF[textlen&gt;140, &quot;text&quot;] </span></p><p class="c10"><span class="c14 c17">[1] &quot;RT @andyschonberger: Exciting (and tempting) to see #EVs all over the #GLS12 show. Combine EVs w #solar generation and we have a winner! </span><span class="c4 c17"><a class="c3" href="https://www.google.com/url?q=http://t.co/NVsfq4G3&amp;sa=D&amp;source=editors&amp;ust=1751550345289657&amp;usg=AOvVaw3sSGktbgSMXgSc1fBmfaBW">http://t.co/NVsfq4G3</a></span><span class="c1">&quot; </span></p><p class="c29"><span class="c14">We&rsquo;ve truncated the output to save space, but in the data we are using here, there were nine tweets with lengths greater than 140 (which was the original tweet max). Not all of them had &quot;</span><span class="c14 c17">RT</span><span class="c14">&quot; in them, though, so the mystery remains. An important word about the final command line above, though: We&rsquo;re using the square brackets notation to access the elements of </span><span class="c14 c17">tweetDF</span><span class="c14">. In the first entry, &quot;</span><span class="c14 c17">textlen&gt;280</span><span class="c14">&quot;, we&rsquo;re using a conditional expression to control which rows are reported. Only those rows where our new field &quot;</span><span class="c14 c17">textlen</span><span class="c14">&quot; contains a quantity larger than 280 will be reported to the output. In the second entry within square brackets, &quot;</span><span class="c14 c17">text</span><span class="c14">&quot; controls which columns are reported onto the output. The square bracket notation is extremely powerful and sometimes a little unpredictable and confusing, so it is worth experimenting with. For example, how would you change that last command above to report </span><span class="c14 c46">all </span><span class="c14">of the columns/fields for the matching rows? Or how would you request the &quot;</span><span class="c14 c17">screenName</span><span class="c14">&quot; column instead of the &quot;</span><span class="c14 c17">text</span><span class="c14">&quot; column? What would happen if you substituted the number 1 in place of &quot;</span><span class="c14 c17">text</span><span class="c5">&quot; on that command? </span></p><p class="c7"><span class="c14">The next common task in working with strings is to count the number of words as well as the number of other interesting elements within the text. Counting the words can be accomplished in several ways. One of the simplest ways is to count the separators between the words these are generally spaces. We need to be careful not to over count, if someone has mistakenly typed two spaces between a word, so let&rsquo;s make sure to take out doubles. The </span><span class="c14 c17">str_replace_all()</span><span class="c14">&nbsp;function from </span><span class="c14 c17">stringr </span><span class="c5">can be used to accomplish this: </span></p><p class="c10"><span class="c8">&gt; tweetDF$modtext &lt;- str_replace_all(text,&quot; &quot;,&quot; &quot;) </span></p><p class="c73"><span class="c8">&gt; tweetDF$textlen2 &lt;- str_length(tweetDF$modtext) </span></p><p class="c10"><span class="c8">&gt; detach(tweetDF) </span></p><p class="c10"><span class="c8">&gt; attach(tweetDF) </span></p><p class="c73"><span class="c8">&gt; tweetDF[textlen != textlen2,] </span></p><p class="c7"><span class="c14">The first line above uses the</span><span class="c14 c17">&nbsp;str_replace_all() </span><span class="c14">function to substitute the one string in place of another as many times as the matching string appears in the input. Three arguments appear on the function above: the first is the input string, and that is </span><span class="c14 c17">tweetDF$text </span><span class="c14">(although we&rsquo;ve referred to it just as &quot;</span><span class="c14 c17">text</span><span class="c14">&nbsp;because the dataframe is attached). The second argument is the string to look for and the third argument is the string to substitute in place of the first. Note that here we are asking to substitute one space any time that two in a row are found. Almost all computer languages have a function similar to this, although many of them only supply a function that replaces the </span><span class="c14 c46">first </span><span class="c5">instance of the matching string. </span></p><p class="c7"><span class="c5">In the second command we have calculated a new string length variable based on the length of the strings where the substitutions have occurred. We preserved this in a new variable/field/column so that we can compare it to the original string length in the final command. Note the use of the bracket notation in R to address a certain subset of rows based on where the inequality is true. So here we are looking for a report back of all of the strings whose lengths changed. In the tweet data we are using here, the output indicated that there were seven strings that had their length reduced by the elimination of duplicate spaces. </span></p><p class="c7"><span class="c14">Now we are ready to count the number of words in each tweet using the </span><span class="c14 c17">str_count() </span><span class="c14">function. If you give it some thought, it should be clear that generally there is one more word than there are spaces. For instance, in the sentence, &quot;</span><span class="c14 c17">Go for it,</span><span class="c14">&quot; there are two spaces but three words. So if we want to have an accurate count, we should add one to the total that we obtain from the </span><span class="c14 c17">str_count()</span><span class="c5">&nbsp;function: </span></p><p class="c10"><span class="c1">&gt; tweetDF$wordCount &lt;-(str_count(modtext,&quot; &quot;) + 1) </span></p><p class="c10"><span class="c1">&gt; detach(tweetDF) </span></p><p class="c2"><span class="c1">&gt; attach(tweetDF) </span></p><p class="c10"><span class="c14 c17">&gt; mean(wordCount) <br></span><span class="c38 c17 c23 c18">&nbsp;</span></p><p class="c19"><span class="c1">[1] 14.24 </span></p><p class="c7"><span class="c5">In this last command, we&rsquo;ve asked R to report the mean value of the vector of word counts, and we learn that on average a tweet in our dataset has about 14 words in it. </span></p><p class="c7"><span class="c14">Next, let&rsquo;s do a bit of what computer scientists (and others) call &quot;parsing.&quot; Parsing is the process of dividing a larger unit, like a sentence, into smaller units, like words, based on some kind of rule. In many cases, parsing requires careful use of pattern matching. Most computer languages accomplish pattern matching through the use of a strategy called &quot;regular expressions.&quot; A regular expression is a set of symbols used to match patterns. For example,</span><span class="c14 c17">&nbsp;[a-z] </span><span class="c14">is used to match any lowercase letter and the asterisk is used to represent a sequence of zero or more characters. So the regular expression &quot;</span><span class="c14 c17">[az]*</span><span class="c5">&quot; means, &quot;match a sequence of zero or more lowercase characters. </span></p><p class="c7"><span class="c14">If we wanted to parse the retweet sequence that appears at the beginning of some of the tweets, we might use a regular expression like this: &quot;</span><span class="c14 c17">RT @[a-z,A-Z]*: &quot;</span><span class="c14">. Each character up to the square bracket is a &quot;literal&quot; that has to match exactly. Then the &quot;</span><span class="c14 c17">[a-z,A-Z]*</span><span class="c14">&quot; lets us match any sequence of uppercase and lowercase characters. Finally, the &quot;</span><span class="c14 c17">: </span><span class="c14">&quot; is another literal that matches the end of the sequence. You can experiment with it freely before you commit to using a particular expression, by asking R to echo the results to the command line, using the function </span><span class="c14 c17">str_match() </span><span class="c5">like this: </span></p><p class="c10"><span class="c1">str_match(modtext,&quot;RT @[a-z,A-Z]*: &quot;) </span></p><p class="c7"><span class="c5">Once you are satisfied that this expression matches the retweet phrases properly, you can commit the results to a new column/ field/variable in the dataframe: <br></span></p><p class="c19"><span class="c26 c17 c23">&gt; tweetDF$rt &lt;str_match(modtext,&quot;RT @[a-z,A-Z]*: &quot;) </span></p><p class="c10"><span class="c26 c17 c23">&gt; detach(tweetDF) </span></p><p class="c10"><span class="c26 c17 c23">&gt; attach(tweetDF) </span></p><p class="c135"><span class="c14">Now you can review what you found by echoing the new variable &quot;</span><span class="c14 c17">rt</span><span class="c5">&quot; to the command line or by examining it in R-studio&rsquo;s data browser: </span></p><p class="c2"><span class="c1">&gt; head(rt, 10) </span></p><p class="c10"><span class="c1">[,1] </span></p><p class="c10"><span class="c1">[1,] NA </span></p><p class="c10"><span class="c1">[2,] NA </span></p><p class="c10"><span class="c1">[3,] NA </span></p><p class="c10"><span class="c1">[4,] NA </span></p><p class="c10"><span class="c1">[5,] NA </span></p><p class="c10"><span class="c1">[6,] NA </span></p><p class="c10"><span class="c1">[7,] NA </span></p><p class="c2"><span class="c1">[8,] &quot;RT @SEIA: &quot; </span></p><p class="c10"><span class="c1">[9,] NA </span></p><p class="c10"><span class="c1">[10,] &quot;RT @andyschonberger: &quot; </span></p><p class="c29"><span class="c14">This may be the first time we have seen the value &quot;</span><span class="c14 c17">NA</span><span class="c14">.&quot; In R, NA means that there is no value available, in effect that the location is empty. Statisticians also call this missing data. These </span><span class="c14 c17">NA</span><span class="c14">s appear in cases where there was no match to the regular expression that we provided to the function </span><span class="c14 c17">str_match()</span><span class="c5">. So there is nothing wrong here, this is an expected outcome of the fact that not all tweets were retweets. If you look carefully, though, you will see something else that is interesting. </span></p><p class="c7"><span class="c14">R is trying to tell us something with the bracket notation. At the top of the list there is a notation of</span><span class="c14 c17">&nbsp;[,1] </span><span class="c14">which signifies that R is showing us the first column of something. Then, each of the entries looks like </span><span class="c14 c17">[#,] </span><span class="c14">with a row number in place of </span><span class="c14 c17"># </span><span class="c14">and an empty column designator, suggesting that R is showing us the contents of a row, possibly across multiple columns. This seems a bit mysterious, but a check of the documentation for </span><span class="c14 c17">str_match() </span><span class="c14">reveals that it returns a </span><span class="c14 c46">matrix </span><span class="c14">as its result. This means that </span><span class="c14 c17">tweetDF$rt</span><span class="c14">&nbsp;could potentially contain its own rectangular data object: In effect, the variable </span><span class="c14 c17">rt </span><span class="c5">could itself contain more than one column! </span></p><p class="c7"><span class="c14">In our case, our regular expression is very simple and it contains just one chunk to match, so there is only one column of new data in </span><span class="c14 c17">tweetDF$rt</span><span class="c14">&nbsp;that was generated form using </span><span class="c14 c17">str_match()</span><span class="c14">. Yet the full capability of regular expressions allows for matching a whole sequence of chunks, not just one, and so </span><span class="c14 c17">str_match() </span><span class="c14">has set up the data that it returns to prepare for the eventuality that each row of </span><span class="c14 c17">tweetDF$rt</span><span class="c5">&nbsp;might actually have a whole list of results. </span></p><p class="c7"><span class="c14">If, for some reason, we wanted to simplify the structure of</span><span class="c14 c17">&nbsp;tweetDF$rt </span><span class="c5">so that each element was simply a single string, we could use this command: </span></p><p class="c10"><span class="c1">tweetDF$rt &lt;- tweetDF$rt[ ,1] </span></p><p class="c7"><span class="c14">This assigns to each element of </span><span class="c14 c17">tweetDF$rt</span><span class="c5">&nbsp;the contents of the first column of the matrix. If you run that command and reexamine </span></p><p class="c19"><span class="c14 c17">tweetDF$rt </span><span class="c14">with </span><span class="c14 c17">head() </span><span class="c5">you will find the simplified structure: no more column designator. </span></p><p class="c7"><span class="c14">For us to be able to make some use of the retweet string we just isolated, we probably should extract just the &quot;screenname&quot; of the individual whose tweet got retweeted. A screenname in Twitter is like a username, it provides a unique identifier for each person who wants to post tweets. An individual who is frequently retweeted by others may be more influential because their postings reach a wider audience, so it could be useful for us to have a listing of all of the screennames without the extraneous stuff. This is easy to do with </span><span class="c14 c17">str_replace()</span><span class="c14">. Note that we used </span><span class="c14 c17">str_replace_all()</span><span class="c5">&nbsp;earlier in the chapter, but we don&rsquo;t need it here, because we know that we are going to replace just one instance of each string: </span></p><p class="c2"><span class="c1">tweetDF$rt&lt;-str_replace(rt, &quot;RT @&quot;,&quot;&quot;) </span></p><p class="c10"><span class="c1">tweetDF$rt&lt;-str_replace(rt,&quot;: &quot;,&quot;&quot;) </span></p><p class="c10"><span class="c1">&gt; tail(rt, 1) </span></p><p class="c10"><span class="c1">[,1] </span></p><p class="c2"><span class="c1">[100,] &quot;SolarFred&quot; </span></p><p class="c10"><span class="c1">tweetDF$rt &lt;- tweetDF$rt[ ,1] </span></p><p class="c29"><span class="c14">In the first command, we substitute the empty string in place of the four character prefix &quot;</span><span class="c14 c17">RT @</span><span class="c14">&quot;, while in the second command we substitute the empty string in place of the two character suffix &quot;</span><span class="c14 c17">: </span><span class="c14">&quot;. In each case we assign the resulting string back to </span><span class="c14 c17">tweetDF$rt</span><span class="c5">. You may be wondering why sometimes we create a new column or field when we calculate some new data while other times we do not. The golden rule with data columns is never to mess with the original data that was supplied. When you are working on a &quot;derived&quot; column, i.e., one that is calculated from other data, it may require several intermediate steps to get the data looking the way you want. In this case, rt is a derived column that we extracted from the text field of the tweet and our goal was to reduce it to the bare screenname of the individual whose post was retweeted. So these commands, which successfully overwrite rt with closer and closer versions of what we wanted, were fair game for modification. </span></p><p class="c7"><span class="c14">You may also have noticed the very last command. It seems that one of our steps, probably the use of</span><span class="c14 c17">&nbsp;str_match()</span><span class="c5">&nbsp;must have &quot;matrix-ized&quot; our data again, so we use the column trick that appeared earlier in this chapter to flatten the matrix back to a single column of string data. </span></p><p class="c7"><span class="c5">This would be a good point to visualize what we have obtained. Here we introduce two new functions, one which should seem familiar and one that is quite new: </span></p><p class="c10"><span class="c1">table(as.factor(rt)) </span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 543.00px; height: 193.00px;"><img alt="" src="images/image65.png" style="width: 543.00px; height: 193.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="table(as.factor(rt))"></span></p><p class="c7"><span class="c14">The </span><span class="c14 c17">as.factor() </span><span class="c14">function is a type/mode coercion and just a new one in a family we have seen before. In previous chapters we used </span><span class="c14 c17">as.integer() </span><span class="c14">and </span><span class="c14 c17">as.character()</span><span class="c5">&nbsp;to perform other conversions. In R a factor is a collection of descriptive labels and corresponding unique identifying numbers. The identifying numbers are not usually visible in outputs. Factors are often used for dividing up a dataset into categories. In a survey, for instance, if you had a variable containing the gender of a participant, the variable would frequently be in the form of a factor with (at least) two distinct categories (or what statisticians call levels), male and female. Inside R, each of these categories would be represented as a number, but the corresponding label would usually be the only thing you would see as output. As an experiment, try running this command: </span></p><p class="c2"><span class="c1">&gt; str(as.factor(rt)) </span></p><p class="c29"><span class="c5">This will reveal the &quot;structure&quot; of the data object after coercion. </span></p><p class="c7"><span class="c14">Returning to the earlier </span><span class="c14 c17">table(as.factor(rt))</span><span class="c14">&nbsp;command, the</span><span class="c14 c17">&nbsp;table() </span><span class="c5">function takes as input one or more factors and returns a so called contingency table. This is easy to understand for use with just one factor: The function returns a unique list of factor &quot;levels&quot; (unique: meaning no duplicates) along with a count of how many rows/instances there were of each level in the dataset as a whole. </span></p><p class="c7"><span class="c14">The screenshot shows the command and the output. There are about 15 unique screennames of Twitter users who were retweeted. The highest number of times that a screenname appeared was three, in the case of SEIA. The </span><span class="c14 c17">table() </span><span class="c14">function is used more commonly to create two-way (two dimensional) contingency </span></p><p class="c19"><span class="c5">tables. We could demonstrate that here if we had two factors, so let&rsquo;s create another factor. </span></p><p class="c7"><span class="c5">Remember earlier in the chapter we noticed some tweets had text that was longer than the 140 characters originally allowed by Twitter. We can make a new variable, we&rsquo;ll call it longtext, that will be TRUE if the original tweet was longer than 140 characters and FALSE if it was not: </span></p><p class="c2"><span class="c1">&gt; tweetDF$longtext &lt;- (textlen&gt;140) </span></p><p class="c10"><span class="c1">&gt; detach(tweetDF) </span></p><p class="c10"><span class="c1">&gt; attach(tweetDF) </span></p><p class="c29"><span class="c14">The first command above has an inequality expression on the right hand side. This is tested for each row and the result, either TRUE or FALSE, is assigned to the new variable longtext. Computer scientists sometimes call this a &quot;flag&quot; variable because it flags whether or not a certain attribute is present in the data. Now we can run the</span><span class="c14 c17">&nbsp;table() </span><span class="c5">function on the two factors: </span></p><p class="c29"><span class="c1">&gt; table(as.factor(rt),as.factor(longtext)) </span></p><p class="c42 c111 c112"><span class="c1">FALSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRUE </span></p><p class="c42"><span class="c1">EarthTechling &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 </span></p><p class="c42"><span class="c1">FeedTheGrid &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">FirstSolar &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">GreenergyNews &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">RayGil &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 </span></p><p class="c42"><span class="c1">SEIA &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">SolarFred &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 </span></p><p class="c42"><span class="c1">SolarIndustry &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">SolarNovus &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">andyschonberger 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 </span></p><p class="c42"><span class="c1">deepgreendesign 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 </span></p><p class="c42"><span class="c1">gerdvdlogt &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">seia &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">solarfred &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c42"><span class="c1">thesolsolution &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c29"><span class="c14">For a two-way contingency table, the first argument you supply to </span><span class="c14 c17">table() </span><span class="c5">is used to build up the rows and the second argument is used to create the columns. The command and output above give us a nice compact display of which retweets are longer than 140 characters (the TRUE column) and which are not (the FALSE column). It is easy to see at a glance that there are many in each category. So, while doing a retweet may contribute to having an extra long tweet, there are also many retweets that are 140 characters or less. It seems a little cumbersome to look at the long list of retweet screennames, so we will create another flag variable that indicates whether a tweet text contains a retweet. This will just provide a more compact way of reviewing which tweets have retweets and which do not: </span></p><p class="c10"><span class="c8">&gt; tweetDF$hasrt &lt;!(is.na(rt)) </span></p><p class="c73"><span class="c17 c18 c36">&gt; detach(tweetDF) <br></span><span class="c38 c17 c23 c18">&nbsp;</span></p><p class="c19"><span class="c8">&gt; attach(tweetDF) </span></p><p class="c10"><span class="c17 c18 c36">&gt; View(tweetDF)</span><span class="c44 c23 c18 c36">&nbsp;</span></p><p class="c7"><span class="c14">The first command above uses a function we have not encountered before: </span><span class="c14 c17">is.na()</span><span class="c14">. A whole family of functions that start with &quot;</span><span class="c14 c17">is</span><span class="c14">&quot; exists in R (as well as in other programming languages) and these functions provide a convenient way of testing the status or contents of a data object or of a particular element of a data object. The</span><span class="c14 c17">&nbsp;is.na() </span><span class="c14">function tests whether an element of the input variable has the value NA, which we know from earlier in the chapter is R&rsquo;s way of showing a missing value (when a particular data element is empty). So the expression,</span><span class="c14 c17">&nbsp;is.na(rt)</span><span class="c14">&nbsp;will return TRUE if a particular cell of </span><span class="c14 c17">tweetDF$rt </span><span class="c14">contains the empty value NA, and false if it contains some real data. If you look at the name of our new variable, however, which we have called &quot;hasrt&quot; you may see that we want to reverse the sense of the TRUE and FALSE that </span><span class="c14 c17">is.na() </span><span class="c14">returns. To do that job we use the &quot;</span><span class="c14 c17">!</span><span class="c14">&quot; character, which computers scientists may either call &quot;bang&quot; or more accurately, &quot;not.&quot; Using &quot;not&quot; is more accurate because the &quot;</span><span class="c14 c17">!</span><span class="c14">&quot; character provides the Boolean NOT function, which changes a TRUE to a FALSE and vice versa. One last little thing is that the </span><span class="c14 c17">View() </span><span class="c5">command causes R-Studio to freshen the display of the dataframe in its upper left hand pane. Let&rsquo;s look again at retweets and long tweet texts: </span></p><p class="c2"><span class="c1">&gt; table(hasrt,longtext) </span></p><p class="c10"><span class="c1">longtext </span></p><p class="c10"><span class="c1">Hasrt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FALSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRUE </span></p><p class="c10"><span class="c1">FALSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;76&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 </span></p><p class="c10"><span class="c1">TRUE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7 </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">There are more than twice as many extra long texts (7) when a tweet contains a retweet than when it does not. </span></p><p class="c7"><span class="c5">Let&rsquo;s now follow the same general procedure for extracting the URLs from the tweet texts. As before the goal is to create a new string variable/column on the original dataframe that will contain the URLs for all of those tweets that have them. Additionally, we will create a flag variable that signifies whether or not each tweet contains a URL. Here, as before, we follow a key principle: Don&rsquo;t mess with your original data. We will need to develop a new regular expression in order to locate and extract the URL string from inside of the tweet text. Actually, if you examine your tweet data in the R-Studio data browser, you may note that some of the tweets have more than one URL in them. So we will have to choose our function call carefully and be equally careful looking at the results to make sure that we have obtained what we need. </span></p><p class="c7"><span class="c14">At the time when this was written, Twitter had imposed an excellent degree of consistency on URLs, such that they all seem to start with the string &quot;</span><span class="c4 c17"><a class="c3" href="https://www.google.com/url?q=http://t.co/&amp;sa=D&amp;source=editors&amp;ust=1751550345314659&amp;usg=AOvVaw0xTOWXTjw6xwSH_PBFkSzy">http://t.co/</a></span><span class="c14">&quot;. Additionally, it seems that the compacted URLs all contain exactly 8 characters after that literal, composed of upper and lower case letters and digits. We can use </span><span class="c14 c17">str_match_all() </span><span class="c5">to extract these URLs using the following code: </span></p><p class="c73"><span class="c17 c18 c36">str_match_all(text,&quot;</span><span class="c17 c18">http://t.co/[a-z,A-Z,0-9]{8}</span><span class="c8">&quot;) </span></p><p class="c7"><span class="c14">We feed the </span><span class="c14 c17">tweetDF$text</span><span class="c14">&nbsp;field as input into this function call (we don&rsquo;t need to provide the </span><span class="c14 c17">tweetDF$</span><span class="c5">&nbsp;part because this dataframe is attached). The regular expression begins with the 12 literal characters ending with a forward slash. Then we have a regular expression pattern to match. The material within the square brackets matches any upper or lowercase letter and any digit. The numeral 8 between the curly braces at the end say to match the previous pattern exactly eight times. This yields output that looks like this: </span></p><p class="c2"><span class="c38 c17 c40 c23">[[6]] </span></p><p class="c10 c111"><span class="c38 c17 c40 c23">[,1] </span></p><p class="c10"><span class="c38 c17 c40 c23">[1,] &quot;http://t.co/w74X9jci&quot; </span></p><p class="c100"><span class="c38 c17 c40 c23">[[7]] </span></p><p class="c2 c111"><span class="c38 c17 c23 c40">[,1] </span></p><p class="c10"><span class="c38 c17 c40 c23">[1,] &quot;http://t.co/DZBUoz5L&quot; </span></p><p class="c10"><span class="c38 c17 c40 c23">[2,] &quot;http://t.co/gmtEdcQI&quot; </span></p><p class="c29"><span class="c14">This is just an excerpt of the output, but there are a couple of important things to note. First, note that the first element is preceded by the notation </span><span class="c14 c17">[[6]]</span><span class="c14">. In the past when R has listed out multiple items on the output, we have seen them with index numbers like</span><span class="c14 c17">&nbsp;[1] </span><span class="c14">and</span><span class="c14 c17">&nbsp;[2]</span><span class="c14">. In this case, however, that could be confusing because each element in the output could have multiple rows (as item </span><span class="c14 c17">[[7]] </span><span class="c5">above clearly shows). So R is using double bracket notation to indicate the ordinal number of each chunk of data in the list, where a given chunk may itself contain multiple elements. </span></p><p class="c7"><span class="c14">Confusing? Let&rsquo;s go at it from a different angle. Look at the output under the</span><span class="c14 c17">&nbsp;[[7]] </span><span class="c14">above. As we noted a few paragraphs ago, some of those tweets have multiple URLs in them. The </span><span class="c14 c17">str_match_all() </span><span class="c14">function handles this by creating, </span><span class="c14 c46">for every single row in the tweet data</span><span class="c5">, a data object that itself contains exactly one column but one or possibly more than one row one row for each URL that appears in the </span></p><p class="c19"><span class="c5">tweet. So, just as we saw earlier in the chapter, we are getting back from a string function a complex matrix-like data object that requires careful handling if we are to make proper use of it. </span></p><p class="c7"><span class="c14">The only other bit of complexity is this: What if a tweet contained no URLs at all? Your output from running the </span><span class="c14 c17">str_match_all() </span><span class="c5">function probably contains a few elements that look like this: </span></p><p class="c2"><span class="c1">[[30]] </span></p><p class="c10"><span class="c1">character(0) </span></p><p class="c100"><span class="c1">[[31]] </span></p><p class="c10"><span class="c1">character(0) </span></p><p class="c29"><span class="c14">So elements </span><span class="c14 c17">[[30]] </span><span class="c14">and</span><span class="c14 c17">&nbsp;[[31]] </span><span class="c14">of the data returned from </span><span class="c14 c17">str_match_all() </span><span class="c5">each contain a zero length string. No rows, no columns, just character(0), the so-called null character, which in many computer programming languages is used to &quot;terminate&quot; a string. </span></p><p class="c7"><span class="c14">Let&rsquo;s go ahead and store the output from </span><span class="c14 c17">str_match_all() </span><span class="c14">into a new vector on </span><span class="c14 c17">tweetDF </span><span class="c5">and then see what we can do to tally up the URLs we have found: </span></p><p class="c73"><span class="c8">&gt; tweetDF$urlist&lt;-str_match_all(text,+ </span></p><p class="c33"><span class="c8">&quot;http://t.co/[a-z,A-Z,0-9]{8}&quot;) </span></p><p class="c10"><span class="c8">&gt; detach(tweetDF) </span></p><p class="c73"><span class="c8">&gt; attach(tweetDF) </span></p><p class="c10"><span class="c8">&gt; head(tweetDF$urlist,2) </span></p><p class="c10"><span class="c17 c18 c36">[[1]] <br></span><span class="c38 c17 c23 c18">&nbsp;</span></p><p class="c19"><span class="c8">[,1] </span></p><p class="c10"><span class="c8">[1,] &quot;http://t.co/ims8gDWW&quot; </span></p><p class="c131"><span class="c8">[[2]] </span></p><p class="c10"><span class="c8">[,1] </span></p><p class="c73"><span class="c8">[1,] &quot;http://t.co/37PKAF3N&quot; </span></p><p class="c7"><span class="c14">Now we are ready to wrestle with the problem of how to tally up the results of our URL parsing. Unlike the situation with retweets, where there either was or was not a single retweet indication in the text, we have the possibility of zero, one or more URLs within the text of each tweet. Our new object &quot;urlist&quot; is a multi-dimensional object that contains a single null character, one row/column of character data, or one column with more than one row of character data. The key to summarizing this is the </span><span class="c14 c17">length() </span><span class="c5">function, which will happily count up the number of elements in an object that you supply to it: </span></p><p class="c10"><span class="c1">&gt; length(urlist[[1]]) </span></p><p class="c2"><span class="c1">[1] 1 </span></p><p class="c10"><span class="c1">&gt; length(urlist[[5]]) </span></p><p class="c10"><span class="c1">[1] 0 </span></p><p class="c10"><span class="c1">&gt; length(urlist[[7]]) </span></p><p class="c10"><span class="c1">[1] 2 </span></p><p class="c29"><span class="c5">Here you see that double bracket notation again, used as an index into each &quot;chunk&quot; of data, where the chunk itself may have some </span></p><p class="c19"><span class="c14">internal complexity. In the case of element</span><span class="c14 c17">&nbsp;[[1]] </span><span class="c14">above, there is one row, and therefore one URL. For element</span><span class="c14 c17">&nbsp;[[5]] </span><span class="c14">above, we see a zero, which means that </span><span class="c14 c17">length()</span><span class="c14">&nbsp;is telling us that this element has no rows in it at all. Finally, for element</span><span class="c14 c17">&nbsp;[[7]] </span><span class="c14">we see </span><span class="c14 c17">2</span><span class="c5">, meaning that this element contains two rows, and therefore two URLs. </span></p><p class="c7"><span class="c5">In previous work with R, we&rsquo;ve gotten used to leaving the inside of the square brackets empty when we want to work with a whole list of items, but that won&rsquo;t work with the double brackets: </span></p><p class="c73"><span class="c8">&gt; length(urlist[[]]) </span></p><p class="c10"><span class="c8">Error in urlist[[]] : invalid subscript type &#39;symbol&#39; </span></p><p class="c7"><span class="c14">The double brackets notation is designed to reference just a single element or component in a list, so empty double brackets does not work as a shorthand for every element in a list. So what we must do if we want to apply the </span><span class="c14 c17">length() </span><span class="c14">function to each element in </span><span class="c14 c17">urlist()</span><span class="c14">&nbsp;is to loop. We could accomplish this with a for loop, as we did in the last chapter, using an index quantity such as &quot;</span><span class="c14 c17">i</span><span class="c14">&quot; and substituting </span><span class="c14 c17">i</span><span class="c14">&nbsp;into each expression like this: </span><span class="c14 c17">urlist[[i]]</span><span class="c14">. But let&rsquo;s take this opportunity to learn a new function in R, one that is generally more efficient for looping. The</span><span class="c14 c17">&nbsp;rapply() </span><span class="c14">function is part of the &quot;</span><span class="c14 c17">apply</span><span class="c14">&quot; family of functions, and it stands for &quot;recursive apply.&quot; Recursive in this case means that the function will dive down into the complex, nested structure of urlist and repetitively run a function for us, in this case the </span><span class="c14 c17">length() </span><span class="c5">function: </span></p><p class="c2"><span class="c1">&gt; tweetDF$numurls&lt;-rapply(urlist,length) </span></p><p class="c22"><span class="c1">&gt; detach(tweetDF) </span></p><p class="c22"><span class="c1">&gt; attach(tweetDF) </span></p><p class="c22"><span class="c14 c17">&gt; head(numurls,10) <br></span><span class="c38 c17 c23 c18">&nbsp;</span></p><p class="c19"><span class="c1">[1] 1 1 1 1 0 1 2 1 1 1 </span></p><p class="c9"><span class="c14">Excellent! We now have a new field on </span><span class="c14 c17">tweetDF </span><span class="c14">that counts up the number of URLs. As a last step in examining our tweet data, let&rsquo;s look at a contingency table that looks at the number of URLs together with the flag indicating an extra long tweet. Earlier in the chapter, we mentioned that the</span><span class="c14 c17">&nbsp;table() </span><span class="c14">function takes factors as its input. In the command below we have supplied the numurls field to the</span><span class="c14 c17">&nbsp;table() </span><span class="c14">function without coercing it to a factor. Fortunately, the </span><span class="c14 c17">table() </span><span class="c14">function has some built in intelligence that will coerce a numeric variable into a factor. In this case because </span><span class="c14 c17">numurls </span><span class="c14">only takes on the values of 0, 1, or 2, it makes good sense to allow </span><span class="c14 c17">table()</span><span class="c5">&nbsp;to perform this coercion: </span></p><p class="c2"><span class="c1">&gt; table(numurls,longtext) </span></p><p class="c10"><span class="c1">longtext </span></p><p class="c10"><span class="c1">Numurls&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FALSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRUE </span></p><p class="c10"><span class="c1">0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 </span></p><p class="c10"><span class="c1">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;72 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6 </span></p><p class="c10"><span class="c1">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c29"><span class="c5">This table might be even more informative if we looked at it as proportions, so here is a trick to view proportions instead of counts: </span></p><p class="c2"><span class="c1">&gt; prop.table(table(numurls,longtext)) </span></p><p class="c10"><span class="c1">longtext </span></p><p class="c45"><span class="c1">Numurls&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FALSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRUE </span></p><p class="c45"><span class="c1">0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.16 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.03 </span></p><p class="c45"><span class="c1">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.72 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.06 </span></p><p class="c45"><span class="c1">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.03 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.00 </span></p><p class="c7"><span class="c14">That looks familiar! Now, of course, we remember that we had exactly 100 tweets, so each of the counts could be considered a percentage with no further calculation. Still, </span><span class="c14 c17">prop.table() </span><span class="c5">is a useful function to have when you would rather view your contingency tables as percentages rather than counts. We can see from these results that six percent of the tweets have one URL, but only three percent have no URLS. </span></p><p class="c7"><span class="c5">So, before we close out this chapter, let&rsquo;s look at a three way contingency table by putting together our two flag variables and the number of URLs: </span></p><p class="c45"><span class="c1">&gt; table(numurls,hasrt,longtext) </span></p><p class="c45"><span class="c1">, , longtext = FALSE </span></p><p class="c45"><span class="c1">hasrt </span></p><p class="c45"><span class="c1">Numurls&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FALSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRUE </span></p><p class="c45"><span class="c1">0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 </span></p><p class="c45"><span class="c1">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;58 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14 </span></p><p class="c45"><span class="c1">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c45"><span class="c1">, , longtext = TRUE </span></p><p class="c45"><span class="c1">hasrt </span></p><p class="c45"><span class="c1">Numurls&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FALSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRUE </span></p><p class="c45"><span class="c1">0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 </span></p><p class="c45"><span class="c1">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4 </span></p><p class="c45"><span class="c1">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 </span></p><p class="c9"><span class="c14">Not sure this entirely solves the mystery, but if we look at the second two-way table above, where </span><span class="c14 c17">longtext = TRUE</span><span class="c5">, it seems that extra long tweets either have a retweet (3 cases), or a single URL (2 cases) or both (4 cases). </span></p><p class="c29"><span class="c5">When we said we would give statistics a little rest in this chapter, we lied just a tiny bit. Check out these results: </span></p><p class="c10"><span class="c8">&gt; mean(textlen[hasrt&amp;longtext]) </span></p><p class="c10"><span class="c8">[1] 155 </span></p><p class="c73"><span class="c8">&gt; mean(textlen[!hasrt&amp;longtext]) </span></p><p class="c10"><span class="c8">[1] 142 </span></p><p class="c7"><span class="c5">In both commands we have requested the mean of the variable textlen, which contains the length of the original tweet (the one without the space stripped out). In each command we have also used the bracket notation to choose a particular subset of the cases. Inside the brackets we have a logical expression. The only cases that will be included in the calculation of the mean are those where the expression inside the brackets evaluates to TRUE. In the first command we ask for the mean tweet length for those tweets that have a retweet AND are extra long (the ampersand is the Boolean AND operator). In the second command we use the logical NOT (the &quot;!&quot; character) to look at only those cases that have extra long text but do not have a retweet. The results are instructive. The really long tweets, with a mean length of 155 characters, are those that have retweets. It seems that Twitter does not penalize an individual who retweets by counting the number of characters in the </span></p><p class="c19"><span class="c14">&quot;</span><span class="c14 c17">RT @SCREENNAME:</span><span class="c14">&quot; string. If you have tried the web interface for Twitter you will see why this makes sense: Retweeting is accomplished with a click, and the original tweet which after all may already be 140 characters appears underneath the screenname of the originator of the tweet. The &quot;</span><span class="c14 c17">RT @</span><span class="c5">&quot; string does not even appear in the text of the tweet at that point. </span></p><p class="c7"><span class="c5">Looking back over this chapter, we took a close look at some of the string manipulation functions provided by the package &quot;stringr&quot;. These included some of the most commonly used actions such as finding the length of a string, finding matching text within a string, and doing search and replace operations on a string. We also became aware of some additional complexity in nested data structures. Although statisticians like to work with nice, well-ordered rectangular datasets, computer scientists often deal with much more complex data structures although these are built up out of parts that we are familiar with such as lists, vectors, and matrices. </span></p><p class="c7"><span class="c5">Twitter is an excellent source of string data, and although we have not yet done much in analyzing the contents of tweets or their meanings, we have looked at some of the basic features and regularities of the text portion of a tweet. In the next chapter we will become familiar with a few additional text tools and then be in a position to manipulate and analyze text data. </span></p><p class="c7"><span class="c59 c14 c57 c23">Chapter Challenges </span></p><p class="c7"><span class="c14">Create a function that takes as input a dataframe of tweets and returns as output a list of all of the retweet screennames. As an extra challenge, see if you can reduce that list of screennames to a unique set (i.e., no duplicates) while also generating a count of the number of times that each retweet screenname appeared. <br></span></p><p class="c19"><span class="c14">Once you have written that function, it should be a simple matter to copy and modify it to create a new function that extracts a unique list of </span><span class="c14 c46">hashtags </span><span class="c14">from a dataframe of tweets. Recall that hashtags begin with the &quot;</span><span class="c14 c17">#</span><span class="c5">&quot; character and may contain any combination of upper and lowercase characters as well as digits. There is no length limit on hashtags, so you will have to assume that a hashtag ends when there is a space or a punctuation mark such as a comma, semicolon, or period. </span></p><h3 class="c94" id="h.wvfpa8pb0ohf"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_srsp18wge5o6-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/web/packages/stringr/index.html&amp;sa=D&amp;source=editors&amp;ust=1751550345333628&amp;usg=AOvVaw3OpguGk4kD4OkYs-a_-5MR">http://cran.r-project.org/web/packages/stringr/index.html </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/ASCII&amp;sa=D&amp;source=editors&amp;ust=1751550345333914&amp;usg=AOvVaw0zgBvJx_TaiGl_r_Scrc-i">http://en.wikipedia.org/wiki/ASCII</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Regular_expression&amp;sa=D&amp;source=editors&amp;ust=1751550345334162&amp;usg=AOvVaw1P5jqrtcJF_9vBIY7nf8uO">http://en.wikipedia.org/wiki/Regular_expression </a></span></li><li class="c29 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Unicode&amp;sa=D&amp;source=editors&amp;ust=1751550345334343&amp;usg=AOvVaw0_VdMy7y9zqcY2--fw2jQG">http://en.wikipedia.org/wiki/Unicode </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://had.co.nz/&amp;sa=D&amp;source=editors&amp;ust=1751550345334503&amp;usg=AOvVaw0v_EoLT_ZafBkb0NGr_qrE">http://had.co.nz/</a></span><span class="c5">&nbsp;(Hadley Wickham) </span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://mashable.com/2010/08/14/twitter-140-bug/&amp;sa=D&amp;source=editors&amp;ust=1751550345334740&amp;usg=AOvVaw2AC3E3e6zoDB9wslUa5v2Z">http://mashable.com/2010/08/14/twitter-140-bug/ </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stat.ethz.ch/R-manual/R-devel/library/base/html/search.html&amp;sa=D&amp;source=editors&amp;ust=1751550345335061&amp;usg=AOvVaw0jJTo9wDGly6LwfsuLcLgI">http://stat.ethz.ch/R-manual/R-devel/library/base/html/search.html </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stat.ethz.ch/R-manual/R-devel/library/base/html/table.html&amp;sa=D&amp;source=editors&amp;ust=1751550345335284&amp;usg=AOvVaw1Ri5Tblo3fSs8J_80LF-r5">http://stat.ethz.ch/R-manual/R-devel/library/base/html/table.html </a></span></li></ul><p class="c7 c52"><span class="c11"></span></p><h3 class="c39" id="h.fwd0w2cfg9zv"><span class="c38 c57 c23 c18">R Code for TweetFrame() Function </span></h3><p class="c42"><span class="c17 c23 c36 c47"># TweetFrame() Return a dataframe based on a search of Twitter </span></p><p class="c42"><span class="c47 c17 c23 c36">TweetFrame&lt;-function(searchTerm, maxTweets) </span></p><p class="c42"><span class="c47 c17 c23 c36">{ </span></p><p class="c42"><span class="c47 c17 c23 c36">tweetList &lt;searchTwitter(searchTerm, n=maxTweets) </span></p><p class="c42"><span class="c47 c17 c23 c36"># as.data.frame() coerces each list element into a row </span></p><p class="c42"><span class="c47 c17 c23 c36"># lapply() applies this to all of the elements in twtList </span></p><p class="c42"><span class="c47 c17 c23 c36"># rbind() takes all of the rows and puts them together </span></p><p class="c42"><span class="c47 c17 c23 c36"># do.call() gives rbind() all rows as individual elements </span></p><p class="c42"><span class="c47 c17 c23 c36">tweetDF&lt;do.call(&quot;rbind&quot;, lapply(tweetList,as.data.frame)) </span></p><p class="c42"><span class="c47 c17 c23 c36"># This last step sorts the tweets in arrival order </span></p><p class="c42"><span class="c47 c17 c23 c36">return(tweetDF[order(as.integer(tweetDF$created)), ]) </span></p><p class="c42"><span class="c47 c17 c23 c36">} </span></p><p class="c2"><span class="c18">&nbsp;</span></p><p class="c81 c62 title" id="h.dxdju57ou42s"><span class="c70">CHAPTER 13 </span><span><br>Word Perfect<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 390.67px;"><img alt="" src="images/image20.png" style="width: 624.00px; height: 390.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Word Cloud"></span><span class="c14 c57 c46 c23 c97">In the previous chapter we mastered some of the most basic and important functions for examining and manipulating text. Now we are in a position to analyze the actual words that appear in text documents. Some of the most basic functions of the Internet, such as keyword search, are accomplished by analyzing the &quot;content&quot; i.e., the words in a body of text. </span></p><p class="c19"><span class="c14">The picture at the start of this chapter is a so-called &quot;word cloud&quot; that was generated by examining all of the words returned from a Twitter search of the term &quot;data science.&quot; (This one was made using a web application at </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.jasondavies.com&amp;sa=D&amp;source=editors&amp;ust=1751550345337533&amp;usg=AOvVaw3NZR0CCbC7NjMgmFe3K51a">http://www.jasondavies.com</a></span><span class="c14">.</span><span class="c5">) These colorful word clouds are fun to look at, but they also do contain some useful information. The geometric arrangement of words on the figure is partly random and partly designed and organized to please the eye. Same with the colors. The font size of each word, however, conveys some measure of its importance in the &quot;corpus&quot; of words that was presented to the word cloud graphics program. Corpus, from the Latin word meaning &quot;body,&quot; is a word that text analysts use to refer to a body of text material, often consisting of one or more documents. When thinking about a corpus of textual data, a set of documents could really be anything: web pages, word processing documents on your computer, a set of Tweets, or government reports. In most cases, text analysts think of a collection of documents, each of which contains some natural language text, as a corpus if they plan to analyze all the documents together. </span></p><p class="c7"><span class="c5">The word cloud on the previous page shows that &quot;Data&quot; and &quot;Science&quot; are certainly important terms that came from the search of Twitter, but there are dozens and dozens of less important, but perhaps equally interesting, words that the search results contained. We see words like algorithms, molecules, structures, and research, all of which could make sense in the context of data science. We also see other terms, like #christian, Facilitating, and Coordinator, that don&rsquo;t seem to have the same obvious connection to our original search term &quot;data science.&quot; This small example shows one of the fundamental challenges of natural language processing and the closely related area of search: ensuring that the analysis of text produces results that are relevant to the task that the user has in mind. </span></p><p class="c7"><span class="c5">In this chapter we will use some new R packages to extend our abilities to work with text and to build our own word cloud from data retrieved from Twitter. If you have not worked on the chapter &quot;String Theory&quot; that precedes this chapter, you should probably do so before continuing, as we build on the skills developed there. </span></p><p class="c7"><span class="c14">Depending upon where you left off after the previous chapter, you will need to retrieve and pre-process a set of tweets, using some of the code you already developed, as well as some new code. At the end of the previous chapter, we have provided sample code for the </span><span class="c14 c17">TweetFrame() </span><span class="c5">function, that takes a search term and a maximum tweet limit and returns a time-sorted dataframe containing tweets. Although there are a number of comments in that code, there are really only three lines of functional code thanks to the power of the twitteR package to retrieve data from Twitter for us. For the activities below, we are still working with the dataframe that we retrieved in the previous chapter using this command: </span></p><p class="c10"><span class="c8">tweetDF &lt;-TweetFrame(&quot;#solar&quot;,100) </span></p><p class="c10"><span class="c14">This yields a dataframe, tweetDF, that contains 100 tweets with the hashtag #solar, presumably mostly about solar energy and related &quot;green&quot; topics. Before beginning our work with the two new R packages, we can improve the quality of our display by taking out a lot of the junk that won&rsquo;t make sense to show in the word cloud. To accomplish this, we have authored another function that strips out extra spaces, gets rid of all URL strings, takes out the retweet header if one exists in the tweet, removes hashtags, and eliminates references to other people&rsquo;s Twitter handles. For all of these transformations, we have used string replacement functions from the stringr package that was introduced in the previous chapter. As an example of one of these transformations, consider this command, which appears as the second to last line of the </span><span class="c14 c17">CleanTweet() </span><span class="c5">function: </span></p><p class="c10"><span class="c8">tweets &lt;- str_replace_all(tweets,&quot;@[a-z,A-Z]*&quot;,&quot;&quot;) </span></p><p class="c7"><span class="c14">You should feel pretty comfortable reading this line of code, but if not, here&rsquo;s a little more practice. The left hand side is easy: we use the assignment arrow to assign the results of the right hand side expression to a data object called &quot;tweets.&quot; Note that when this statement is used inside the function as shown at the end of the chapter, &quot;tweets&quot; is a temporary data object that is used just within </span><span class="c14 c17">CleanTweets() </span><span class="c5">after which it disappears automatically. </span></p><p class="c7"><span class="c14">The right hand side of the expression uses the </span><span class="c14 c17">str_replace_all() </span><span class="c14">function from the stringr package. We use the &quot;all&quot; function rather than </span><span class="c14 c17">str_replace()</span><span class="c14">&nbsp;because we are expecting multiple matches within each individual tweet. There are three arguments to the </span><span class="c14 c17">str_replace_all() </span><span class="c5">function. The first is the input, which is a vector of character strings (we are using the temporary data object &quot;tweets&quot; as the source of the text data as well as its destination), the second is the regular expression to match, and the third is the string to use to replace the matches, in this case the empty string as signified by two double quotes with nothing between them. The regular expression in this case is the at sign, &quot;@&quot;, followed by zero or more upper and lowercase letters. The asterisk, &quot;*&quot;, after the stuff in the square brackets is what indicates the zero or more. That regular expression will match any screenname referral that appears within a tweet. </span></p><p class="c7"><span class="c5">If you look at a few tweets you will find that people refer to each other quite frequently by their screennames within a tweet, so @SolarFred might occur from time to time within the text of a tweet. </span></p><p class="c19"><span class="c5">Here&rsquo;s something you could investigate on your own: Can screennames contain digits as well as letters? If so, how would you have to change the regular expression in order to also match the digits zero through nine as part of the screen name? On a related note, why did we choose to strip these screen names out of our tweets? What would the word cloud look like if you left these screennames in the text data? </span></p><p class="c7"><span class="c5">Whether you typed in the function at the end of this chapter or you plan to enter each of the cleaning commands individually, let&rsquo;s begin by obtaining a separate vector of texts that is outside the original dataframe: </span></p><p class="c10"><span class="c1">&gt; cleanText &lt;- tweetDF$text </span></p><p class="c10"><span class="c1">&gt; head(cleanText, 10) </span></p><p class="c7"><span class="c14">There&rsquo;s no critical reason for doing this except that it will simplify the rest of the presentation. You could easily copy the tweetDF$text data into another column in the same dataframe if you wanted to. We&rsquo;ll keep it separate for this exercise so that we don&rsquo;t have to worry about messing around with the rest of the dataframe. The </span><span class="c14 c17">head() </span><span class="c5">command above will give you a preview of what you are starting with. Now let&rsquo;s run our custom cleaning function: </span></p><p class="c2"><span class="c1">&gt; cleanText&lt;-CleanTweets(cleanText) </span></p><p class="c10"><span class="c1">&gt; head(cleanText, 10) </span></p><p class="c29"><span class="c14">Note that we used our &quot;</span><span class="c14 c17">cleanText</span><span class="c14">&quot; data object in the first command above as both the source and the destination. This is an old computer science trick for cutting down on the number of temporary variables that need to be used. In this case it will do exactly what we want, first evaluating the right hand side of the expression by running our </span><span class="c14 c17">CleanTweets()</span><span class="c14">&nbsp;function with the </span><span class="c14 c17">cleanText</span><span class="c14">&nbsp;object as input and then taking the result that is returned by </span><span class="c14 c17">CleanTweets() </span><span class="c14">and assigning it back into </span><span class="c14 c17">cleanText</span><span class="c5">, thus overwriting the data that was in there originally. Remember that we have license to do whatever we want to cleanText because it is a copy of our original data, and we have left the original data intact (i.e., the text column inside the tweetDF dataframe). </span></p><p class="c7"><span class="c14">The </span><span class="c14 c17">head() </span><span class="c14">command should now show a short list of tweets with much of the extraneous junk filtered out. If you have followed these steps, cleanText is now a vector of character strings (in this example exactly 100 strings) ready for use in the rest of our work below. We will now use the &quot;</span><span class="c14 c17">tm</span><span class="c14">&quot; package to process our texts. The &quot;</span><span class="c14 c17">tm</span><span class="c14">&quot; in this case refers to &quot;text mining,&quot; and is a popular choice among the many text analysis packages available in R. By the way, text mining refers to the practice of extracting useful analytic information from corpora of text (corpora is the plural of corpus). Although some people use text mining and natural language processing interchangeably, there are probably a couple subtle differences worth considering. First, the &quot;mining&quot; part of text mining refers to an area of practice that looks for unexpected patterns in large data sets, or what some people refer to as knowledge discovery in databases. In contrast, natural language processing reflects a more general interest in understanding how machines can be programmed (or learn on their own) how to digest and make sense of human language. In a similar vein, text mining often focuses on statistical approaches to analyzing text data, using strategies such as counting word frequencies in a corpus. In natural language processing, one is more likely to hear consideration given to linguistics, and therefore to the processes of breaking text into its component grammatical pieces such as nouns and verbs. In the case of the &quot;</span><span class="c14 c17">tm</span><span class="c5">&quot; addon package for R, we are definitely in the statistical camp, where the main process is to break down a corpus into sequences of words and then to tally-up the different words and sequences we have found. </span></p><p class="c7"><span class="c14">To begin, make sure that the </span><span class="c14 c17">tm</span><span class="c14">&nbsp;package is installed and &quot;libraryed&quot; in your copy of R and R-Studio. You can use the graphic interface in R-Studio for this purpose or the </span><span class="c14 c17">EnsurePackage() </span><span class="c5">function that we wrote in a previous chapter. Once the tm package is ready to use, you should be able to run these commands: </span></p><p class="c73"><span class="c8">&gt; tweetCorpus&lt;-Corpus(VectorSource(cleanText)) </span></p><p class="c10"><span class="c8">&gt; tweetCorpus </span></p><p class="c10"><span class="c44 c23 c18 c36">A corpus with 100 text documents </span></p><p class="c73"><span class="c8">&gt; tweetCorpus&lt;-tm_map(tweetCorpus, tolower) </span></p><p class="c10"><span class="c8">&gt; tweetCorpus&lt;-tm_map(tweetCorpus, removePunctuation) </span></p><p class="c10"><span class="c8">&gt; tweetCorpus&lt;-tm_map(tweetCorpus,removeWords,+ </span></p><p class="c136"><span class="c8">stopwords(&#39;english&#39;)) </span></p><p class="c7"><span class="c14">In the first step above , we &quot;coerce&quot; our </span><span class="c14 c17">cleanText </span><span class="c5">vector into a custom &quot;Class&quot; provided by the tm package and called a &quot;Corpus,&quot; storing the result in a new data object called &quot;tweetCorpus.&quot; This is the first time we have directly encountered a &quot;Class.&quot; The term &quot;class&quot; comes from an area of computer science called &quot;object oriented programming.&quot; Although R is different in many ways from object-oriented languages such as Java or C++, it does contain many of the most fundamental features that define an object-oriented language. For our purposes here, there are just a few things to know about a class. First, a class is nothing more or less than a definition for the structure of a data object. Second, classes use basic data types, such as numbers, to build up more complex data structures. For example, if we made up a new &quot;Dashboard&quot; class, it could contain one number for &quot;Miles Per Hour,&quot; another number for &quot;RPM,&quot; and perhaps a third one indicating the remaining &quot;Fuel Level.&quot; That brings up another point about Classes: users of R can build their own. In this case, the author of the tm package, Ingo Feinerer, created a new class, called Corpus, as the central data structure for text mining functions. (Feinerer is a computer science professor who works at the Vienna University of Technology in the Database and Artificial Intelligence Group.) Last, and most important for this discussion, a Class not only contains definitions about the structure of data, it also contains references to functions that can work on that Class. In other words, a Class is a data object that carries with it instructions on how to do operations on it, from simple things like add and subtract all the way up to complicated operations such as graphing. </span></p><p class="c7"><span class="c14">In the case of the tm package, the Corpus Class defines the most fundamental object that text miners care about, a corpus containing a collection of documents. Once we have our texts stored in a Corpus, the many functions that the </span><span class="c14 c17">tm </span><span class="c14">package provides to us are available. The last three commands in the group above show the use of the </span><span class="c14 c17">tm_map()</span><span class="c14">&nbsp;function, which is one of the powerful capabilities provided by </span><span class="c14 c17">tm</span><span class="c14">. In each case where we call the </span><span class="c14 c17">tm_map() </span><span class="c5">function, we are providing tweetCorpus as the input data, and then we are providing a command that undertakes a transformation on the corpus. We have done three transformations here, first making all of the letters lowercase, then removing the punctuation, and finally taking out the so-called &quot;stop&quot; words. </span></p><p class="c19"><span class="c5"><br>The stop words deserve a little explanation. Researchers who developed the early search engines for electronic databases found that certain words interfered with how well their search algorithms worked. Words such as &quot;the,&quot; &quot;a,&quot; and &quot;at&quot; appeared so commonly in so many different parts of the text that they were useless for differentiating between documents. The unique and unusual nouns, verbs, and adjectives that appeared in a document did a much better job of setting a document apart from other documents in a corpus, such that researchers decided that they should filter out all of the short, commonly used words. The term &quot;stop words&quot; seems to have originated in the 1960s to signify words that a computer processing system would throw out or &quot;stop using&quot; because they had little meaning in a data processing task. To simplify the removal of stop words, the tm package contains lists of such words for different languages. In the last command on the previous page we requested the removal of all of the common stop words. </span></p><p class="c7"><span class="c5">At this point we have processed our corpus into a nice uniform &quot;bag of words&quot; that contains no capital letters, punctuation, or stop words. We are now ready to conduct a kind of statistical analysis of the corpus by creating what is known as a &quot;term-document matrix.&quot; The following command from the tm package creates the matrix: </span></p><p class="c10"><span class="c1">&gt; tweetTDM&lt;-TermDocumentMatrix(tweetCorpus) </span></p><p class="c28"><span class="c1">&gt; tweetTDM </span></p><p class="c22"><span class="c1">A term-document matrix (375 terms, 100 documents) </span></p><p class="c22"><span class="c1">Non-/sparse entries: 610/36890 </span></p><p class="c22"><span class="c1">Sparsity : 98% </span></p><p class="c22"><span class="c1">Maximal term length: 21 </span></p><p class="c22"><span class="c1">Weighting : term frequency (tf) </span></p><p class="c9"><span class="c5">A term-document matrix, also sometimes called a document-term matrix, is a rectangular data structure with terms as the rows and documents as the columns (in other uses you may also make the terms as columns and documents as rows). A term may be a single word, for example, &quot;biology,&quot; or it could also be a compound word, such as &quot;data analysis.&quot; The process of determining whether words go together in a compound word can be accomplished statistically by seeing which words commonly go together, or it can be done with a dictionary. The tm package supports the dictionary approach, but we have not used a dictionary in this example. So if a term like &quot;data&quot; appears once in the first document, twice in the second document, and not at all in the third document, then the column for the term data will contain 1, 2, 0. </span></p><p class="c7"><span class="c14">The statistics reported when we ask for </span><span class="c14 c17">tweetTDM</span><span class="c14">&nbsp;on the command line give us an overview of the results. The </span><span class="c14 c17">TermDocumentMatrix() </span><span class="c5">function extracted 375 different terms from the 100 tweets. The resulting matrix mainly consists of zeros: Out of 37,500 cells in the matrix, only 610 contain non-zero entries, while 36,890 contain zeros. A zero in a cell means that that particular term did not appear in that particular document. The maximal term length was 21 words, which an inspection of the input tweets indicates is also the maximum word length of the input tweets. Finally, the last line, starting with &quot;Weighting&quot; indicates what kind of statistic was stored in the term-document matrix. In this case we used the default, and simplest, option which simply records the count of the number of times a term appears across all of the documents in the </span></p><p class="c19"><span class="c5">corpus. You can peek at what the term-document matrix contains by using the inspect function: </span></p><p class="c2"><span class="c1">inspect(tweetTDM) </span></p><p class="c29"><span class="c5">Be prepared for a large amount of output. Remember the term &quot;sparse&quot; in the summary of the matrix? Sparse refers to the overwhelming number of cells that contain zero indicating that the particular term does not appear in a given document. Most term document matrices are quite sparse. This one is 98% sparse because 36890/37500 = 0.98. In most cases we will need to cull or filter the term-document matrix for purposes of presenting or visualizing it. The tm package provides several methods for filtering out sparsely used terms, but in this example we are going to leave the heavy lifting to the word cloud package. </span></p><p class="c7"><span class="c14">As a first step we need to install and </span><span class="c14 c17">library() </span><span class="c14">the &quot;wordcloud&quot; package. As with other packages, either use the package interface in R-Studio or the </span><span class="c14 c17">EnsurePackage() </span><span class="c5">function that we wrote a few chapters ago. The wordcloud package was written by freelance statistician Ian Fellows, who also developed the &quot;Deducer&quot; user interface for R. Deducer provides a graphical interface that allows users who are more familiar with SPSS or SAS menu systems to be able to use R without resorting to the command line. </span></p><p class="c7"><span class="c14">Once the </span><span class="c14 c17">wordcloud</span><span class="c5">&nbsp;package is loaded, we need to do a little preparation to get our data ready to submit to the word cloud generator function. That function expects two vectors as input arguments, the first a list of the terms, and the second a list of the frequencies of occurrence of the terms. The list of terms and frequencies must be sorted with the most frequent terms appearing first. To accomplish this we first have to coerce our tweet data back into a plain data matrix so that we can sort it by frequency. The first command below accomplishes this: </span></p><p class="c2"><span class="c1">&gt; tdMatrix &lt;- as.matrix(tweetTDM) </span></p><p class="c10"><span class="c1">&gt; sortedMatrix &lt;- sort(rowSums(tdMatrix),+ </span></p><p class="c33"><span class="c1">decreasing=TRUE) </span></p><p class="c10"><span class="c1">&gt; cloudFrame&lt;-data.frame( + </span></p><p class="c33"><span class="c1">word=names(sortedMatrix),freq=sortedMatrix) </span></p><p class="c2"><span class="c1">&gt; wordcloud(cloudFrame$word,cloudFrame$freq) </span></p><p class="c29"><span class="c5">In the next command above, we are accomplishing two things in one command: We are calculating the sums across each row, which gives us the total frequency of a term across all of the different tweets/documents. We are also sorting the resulting values with the highest frequencies first. The result is a named list: Each item of the list has a frequency and the name of each item is the term to which that frequency applies. </span></p><p class="c7"><span class="c14">In the second to last command above, we are extracting the names from the named list in the previous command and binding them together into a dataframe with the frequencies. This dataframe, &quot;</span><span class="c14 c17">cloudFrame</span><span class="c14">&quot;, contains exactly the same information as the named list. &quot;</span><span class="c14 c17">sortedMatrix</span><span class="c14">,&quot; but </span><span class="c14 c17">cloudFrame </span><span class="c14">has the names in a separate column of data. This makes it easier to do the final command above, which is the call to the </span><span class="c14 c17">wordcloud()</span><span class="c14">&nbsp;function. The </span><span class="c14 c17">wordcloud() </span><span class="c14">function has lots of optional parameters for making the word cloud more colorful, controlling its shape, and controlling how frequent an item must be before it appears in the cloud, but we have used the default settings for all of these parameters for the sake of simplicity. We pass to the </span><span class="c14 c17">wordcloud() </span><span class="c14">function the term list and frequency list that we bound into the dataframe and </span><span class="c14 c17">wordcloud()</span><span class="c5">&nbsp;produces the nice graphic that you see below. </span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image60.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="word cloud"></span></p><p class="c19"><span class="c5">If you recall the Twitter search that we used to retrieve those tweets (#solar) it makes perfect sense that &quot;solar&quot; is the most frequent term (even though we filtered out all of the hashtags. The next most popular term is &quot;energy&quot; and after that there are a variety of related words such as &quot;independence,&quot; &quot;green,&quot; &quot;wind,&quot; and &quot;metering.&quot; </span></p><p class="c73"><span class="c59 c14 c57 c23">Chapter Challenge </span></p><p class="c7"><span class="c14">Develop a function that builds upon previous functions we have developed, such as </span><span class="c14 c17">TweetFrame()</span><span class="c14">&nbsp;and </span><span class="c14 c17">CleanTweets()</span><span class="c14">, to take a search term, conduct a Twitter search, clean up the resulting texts, formulate a term-document matrix, and submit resulting term frequencies to the </span><span class="c14 c17">wordcloud() </span><span class="c5">function. Basically this would be a &quot;turnkey&quot; package that would take a Twitter search term and produce a word cloud from it, much like the Jason Davies site described at the beginning of this chapter. </span></p><h3 class="c94" id="h.jokn7b5l847d"><span class="c38 c57 c23 c18">Sources Used in This Chapter </span></h3><ul class="c32 lst-kix_1ikdj3ttsm27-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550345360649&amp;usg=AOvVaw2wNtDZ9uohM7D2nNenN-Rf">http://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.dbai.tuwien.ac.at/staff/feinerer/&amp;sa=D&amp;source=editors&amp;ust=1751550345360872&amp;usg=AOvVaw0tBfBBabUBajX9Hg-MWVxq">http://www.dbai.tuwien.ac.at/staff/feinerer/ </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Document-term_matrix&amp;sa=D&amp;source=editors&amp;ust=1751550345361053&amp;usg=AOvVaw09s8mhu_TeGSscU5sOXe8D">http://en.wikipedia.org/wiki/Document-term_matrix </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Stop_words&amp;sa=D&amp;source=editors&amp;ust=1751550345361207&amp;usg=AOvVaw1YZg8Wo3BeAHhl4A1VEM5X">http://en.wikipedia.org/wiki/Stop_words</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Text_mining&amp;sa=D&amp;source=editors&amp;ust=1751550345361363&amp;usg=AOvVaw0jTZUr8yfXqr8u4F1R3-Kp">http://en.wikipedia.org/wiki/Text_mining</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stat.ethz.ch/R-manual/R-devel/library/base/html/colSu&amp;sa=D&amp;source=editors&amp;ust=1751550345361641&amp;usg=AOvVaw3YzDfvfQ1cFBqs4X9RSjYD">http://stat.ethz.ch/R-manual/R-devel/library/base/html/colSu ms.html </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.jasondavies.com/wordcloud/&amp;sa=D&amp;source=editors&amp;ust=1751550345361847&amp;usg=AOvVaw13odhy2EnB392rUucutFyE">http://www.jasondavies.com/wordcloud/ </a></span></li></ul><h3 class="c20" id="h.a3wlmcbnfcot"><span class="c38 c57 c23 c18">R Code for CleanTweets() Function </span></h3><p class="c42"><span class="c8"># CleanTweets() Takes the junk out of a vector of </span></p><p class="c42"><span class="c8"># tweet texts </span></p><p class="c42"><span class="c8">CleanTweets&lt;-function(tweets) </span></p><p class="c42"><span class="c8">{ </span></p><p class="c42"><span class="c8"># Remove redundant spaces </span></p><p class="c42"><span class="c8">tweets &lt;str_replace_all(tweets,&quot; &quot;,&quot; &quot;) </span></p><p class="c42"><span class="c8"># Get rid of URLs </span></p><p class="c42"><span class="c8">tweets &lt;- str_replace_all(tweets, + </span></p><p class="c42"><span class="c8">&quot;http://t.co/[a-z,A-Z,0-9]{8}&quot;,&quot;&quot;) </span></p><p class="c42"><span class="c8"># Take out retweet header, there is only one </span></p><p class="c42"><span class="c8">tweets &lt;- str_replace(tweets,&quot;RT @[a-z,A-Z]*: &quot;,&quot;&quot;) </span></p><p class="c42"><span class="c8"># Get rid of hashtags </span></p><p class="c42"><span class="c8">tweets &lt;- str_replace_all(tweets,&quot;#[a-z,A-Z]*&quot;,&quot;&quot;) </span></p><p class="c42"><span class="c8"># Get rid of references to other screennames </span></p><p class="c42"><span class="c8">return(tweets) </span></p><p class="c42"><span class="c8">} </span></p><p class="c42"><span class="c8">tweets &lt;- str_replace_all(tweets,&quot;@[a-z,A-Z]*&quot;,&quot;&quot;) </span></p><p class="c19 c52"><span class="c44 c23 c18 c36"></span></p><p class="c19 c52"><span class="c44 c23 c76"></span></p><p class="c81 c62 title" id="h.dn2dcw3kafn8"><span class="c70">CHAPTER 14</span><span class="c38 c57 c23 c84">&nbsp; <br>Storage Wars</span></p><p class="c81 c62 title" id="h.69oc215asqj7"><span class="c37 c36">Before now we have only used small amounts of data that we typed in ourselves, or somewhat larger amounts that we extracted from Twitter. The world is full of other sources of data, however, and we need to examine how to get them into R, or at least how to make them accessible for manipulation in R. In this chapter, we examine various ways that data are stored, and how to access them. </span></p><p class="c19"><span class="c5">Older technologists who have watched the evolution of technology over recent decades remember a time when storage was expensive and it had to be hoarded like gold. Over the last few years, however, the accelerating trend of Moore&rsquo;s Law has made data storage almost &quot;too cheap to meter&quot; (as they used to predict about nuclear power). Although this opens many opportunities, it also means that people keep data around for a long time, since it doesn&rsquo;t make sense to delete anything, and they may keep data around in many different formats. As a result, the world is full of different data formats, some of which are proprietary designed and owned by a single company such as SAS and some of which are open, such as the lowly but infinitely useful &quot;comma separated variable,&quot; or CSV format. </span></p><p class="c7"><span class="c5">In fact, one of the basic dividing lines in data formats is whether data are human readable or not. Formats that are not human readable, often called binary formats, are very efficient in terms of how much data they can pack in per kilobyte, but are also squirrelly in the sense that it is hard to see what is going on inside of the format. As you might expect, human readable formats are inefficient from a storage standpoint, but easy to diagnose when something goes wrong. For high volume applications, such as credit card processing, the data that is exchanged between systems is almost universally in binary formats. When a data set is archived for later reuse, for example in the case of government data sets available to the public, they are usually available in multiple formats, at least one of which is a human readable format. </span></p><p class="c7"><span class="c5">Another dividing line, as mentioned above is between proprietary and open formats. One of the most common ways of storing and sharing small datasets is as Microsoft Excel spreadsheets. Although </span></p><p class="c19"><span class="c5">this is a proprietary format, owned by Microsoft, it has also become a kind of informal and ubiquitous standard. Dozens of different software applications can read Excel formats (there are several different formats that match different versions of Excel). In contrast, the OpenDocument format is an open format, managed by a standards consortium, that anyone can use without worrying what the owner might do. OpenDocument format is based on XML, which stands for Extensible markup language. XML is a whole topic in and of itself, but briefly it is a data exchange format designed specifically to work on the Internet and is both human and machine readable. XML is managed by the W3C consortium, which is responsible for developing and maintaining the many standards and protocols that support the web. </span></p><p class="c7"><span class="c5">As an open source program with many contributors, R offers a wide variety of methods of connecting with external data sources. This is both a blessing and a curse. There is a solution to almost any data access problem you can imagine with R, but there is also a dizzying array of options available such that it is not always obvious what to choose. We&rsquo;ll tackle this problem in two different ways. In the first half of this chapter we will look at methods for importing existing datasets. These may exist on a local computer or on the Internet but the characteristic they share in common is that they are contained (usually) within one single file. The main trick here is to choose the right command to import that data into R. In the second half of the chapter, we will consider a different strategy, namely linking to a &quot;source&quot; of data that is not a file. Many data sources, particularly databases, exist not as a single discrete file, but rather as a system. The system provides methods or calls to &quot;query&quot; data from the system, but from the perspective of the user (and of R) the data never really take the form of a file. </span></p><p class="c7"><span class="c14">The first and easiest strategy for getting data into R is to use the data import dialog in R-Studio. In the upper right hand pane of RStudio, the &quot;Workspace&quot; tab gives views of currently available data objects, but also has a set of buttons at the top for managing the work space. One of the choices there is the &quot;Import Dataset&quot; button: This enables a drop down menu where one choice is to import, &quot;From Text File...&quot; If you click this option and choose an appropriate file you will get a screen like this: </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 506.50px; height: 427.77px;"><img alt="" src="images/image22.png" style="width: 506.50px; height: 427.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="import data"></span><span class="c5"><br>The most important stuff is on the left side. Heading controls whether or not the first line of the text file is treated as containing variable names. The separator drop down gives a choice of different characters that separate the fields/columns in the data. RStudio tries to guess the appropriate choice here based on a scan of </span></p><p class="c19"><span class="c5">the data. In this case it guessed right by choosing &quot;tab-delimited.&quot; As mentioned above, tab-delimited and comma-delimited are the two most common formats used for interchange between data programs.The next drop down is for &quot;Decimal&quot; and this option accounts for the fact the a dot is used in the U.S. while a comma may be used for the decimal point in Europe and elsewhere. Finally, the &quot;Quote&quot; drop down controls which character is used to contain quoted string/text data. The most common method is double quotes. </span></p><p class="c43"><span class="c5">Of course, we skipped ahead a bit here because we assumed that an appropriate file of data was available. It might be useful to see some examples of human readable data: <br></span></p><p class="c115"><span class="c1">Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Age&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gender&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c115"><span class="c1">&quot;Fred&quot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;22&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;M&quot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c115"><span class="c14 c17">&quot;Ginger&quot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;21&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;F&quot;</span><span class="c47 c17 c23 c36">&nbsp;</span></p><p class="c29"><span class="c5">Of course you can&rsquo;t see the tab characters on the screen, but there is one tab character in between each pair of values. In each case, for both command tab-delimited, one line equals one row. The end of a line is marked, invisibly, with a so-called &quot;newline&quot; character. On occasion you may run into differences between different operating systems on how this end of line designation is encoded. </span></p><p class="c7"><span class="c5">The above is a very simple example of a comma-delimited file where the first row contains a &quot;header,&quot; i.e. the information about the names of variables. The second and subsequent rows contain actual data. Each field is separated by a comma, and the text strings are enclosed in double quotes. The same file tab-delimited might look like this: </span></p><p class="c19"><span class="c14"><br></span><span class="c1">Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Age&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gender&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c115"><span class="c1">&quot;Fred&quot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;22&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;M&quot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c115"><span class="c14 c17">&quot;Ginger&quot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;21&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;F&quot; </span></p><p class="c7"><span class="c5">Files containing comma or tab delimited data are ubiquitous across the Internet, but sometimes we would like to gain direct access to binary files in other formats. There are a variety of packages that one might use to access binary data. A comprehensive access list appears here: </span></p><p class="c7"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/doc/manuals/R-data.html&amp;sa=D&amp;source=editors&amp;ust=1751550345370841&amp;usg=AOvVaw0DLmFWqLHpZXfqC0HINDN9">http://cran.r-project.org/doc/manuals/R-data.html </a></span></p><p class="c7"><span class="c5">This page shows a range of methods for obtaining data from a wide variety of programs and formats. Because Excel is such a widely used program for small, informal data sets, we will use it as an example here to illustrate both the power and the pitfalls of accessing binary data with R. Down near the bottom of the page mentioned just above there are several paragraphs of discussion of how to access Excel files with R. In fact, the first sentence mentions that one of the most commonly asked data questions about R is how to access Excel data. </span></p><p class="c7"><span class="c5">Interestingly, this is one area where Mac and Linux users are at a disadvantage relative to Windows users. This is perhaps because Excel is a Microsoft product, originally written to be native to Windows, and as a result it is easier to create tools that work with Windows. One example noted here is the package called RODBC. The acronym ODBC stands for Open Database Connection, and this is a Windows facility for exchanging data among Windows programs. Although there is a proprietary ODBC driver available for the Mac, most Mac users will want to try a different method for getting access to Excel data. </span></p><p class="c19"><span class="c5">Another Windows-only package for R is called xlsReadWrite. This package provides convenient one-command calls for importing data directly from Excel spreadsheets or exporting it directly to spreadsheets. There are also more detailed commands that allow manipulating individual cells. </span></p><p class="c7"><span class="c5">Two additional packages, xlsx and XLConnect, supposedly will work with the Mac, but at the time of this writing both of these packages had version incompatibilities that made it impossible to install the packages directly into R. Note that the vast majority of packages provide the raw &quot;source code&quot; and so it is theoretically possible, but generally highly time consuming, to &quot;compile&quot; your own copies of these packages to create your own installation. </span></p><p class="c29"><span class="c5">Fortunately, a general purpose data manipulation package called gdata provides essential facilities for importing spreadsheet files. In the example that follows, we will use a function from gdata to read Excel data directly from a website. The gdata package is a kind of &quot;Swiss Army Knife&quot; package containing many different functions for accessing and manipulating data. For example, you may recall that R uses the value &quot;NA&quot; to represent missing data. Frequently, however, it is the case that data sets contain other values, such as 999, to represent missing data. The gdata package has several functions that find and transform these values to be consistent with R&rsquo;s strategy for handling missing data. </span></p><p class="c7"><span class="c14">Begin by using </span><span class="c14 c17">install.package()</span><span class="c14">&nbsp;and</span><span class="c14 c17">&nbsp;library() </span><span class="c5">functions to prepare the gdata package for use: </span></p><p class="c2"><span class="c1">&gt; install.packages(&quot;gdata&quot;) </span></p><p class="c10"><span class="c14 c17"># ... lots of output here <br></span></p><p class="c19"><span class="c1">&gt; library(&quot;gdata&quot;) </span></p><p class="c10"><span class="c1">gdata: read.xls support for &#39;XLS&#39; (Excel 97-2004) files </span></p><p class="c2"><span class="c1">gdata: ENABLED. </span></p><p class="c10"><span class="c1">gdata: read.xls support for &#39;XLSX&#39; (Excel 2007+) files ENABLED. </span></p><p class="c29"><span class="c14">Of course, you could also use the </span><span class="c14 c17">EnsurePackage()</span><span class="c14">&nbsp;function that we developed in an earlier chapter, but it was important here to see the output from the </span><span class="c14 c17">library()</span><span class="c5">&nbsp;function. Note that the gdata package reported some diagnostics about the different versions of Excel data that it supports. Note that this is one of the major drawbacks of binary data formats, particularly proprietary ones: you have to make sure that you have the right software to access the different versions of data that you might encounter. In this case it looks like we are covered for the early versions of Excel (97-2004) as well as later versions of Excel (2007+). We must always be on the lookout, however, for data that is stored in even newer versions of Excel that may not be supported by gdata or other packages. </span></p><p class="c7"><span class="c5">Now that gdata is installed, we can use the read.xls() function that it provides. The documentation for the gdata package and the read.xls() function is located here: </span></p><p class="c7"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/web/packages/gdata/gdata.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550345376078&amp;usg=AOvVaw2Wuaqixd96wjBbIpLXX96H">http://cran.r-project.org/web/packages/gdata/gdata.pdf </a></span></p><p class="c7"><span class="c5">A review of the documentation reveals that the only required argument to this function is the location of the XLS file, and that this location can be a pathname, a web location with http, or an Internet location with ftp (file transfer protocol, a way of sending and receiving files without using a web browser). If you hearken back to </span></p><p class="c19"><span class="c5">a very early chapter in this book, you may remember that we accessed some census data that had population counts for all the different U.S. states. For this example, we are going to read the Excel file containing that data directly into a dataframe using the read.xls() function: </span></p><p class="c10"><span class="c8">&gt; testFrame&lt;-read.xls( + &quot;http://www.census.gov/popest/data/state/totals/2011/ tables/NST-EST2011-01.xls&quot;) </span></p><p class="c10"><span class="c17 c18 c36">trying URL &#39;http://www.census.gov/popest/data/state/totals/2011/ tables/NST-EST2011-01.xls</span><span class="c59 c17 c23 c61 c18">&#39; </span></p><p class="c10"><span class="c8">Content type &#39;application/vnd.ms-excel&#39; length 31232 bytes (30 Kb) </span></p><p class="c10"><span class="c8">opened URL </span></p><p class="c10"><span class="c8">================================================== </span></p><p class="c73"><span class="c8">downloaded 30 Kb </span></p><p class="c7"><span class="c14">The command in the first three lines above provides the URL of the Excel file to the </span><span class="c14 c17">read.xls() </span><span class="c5">function. The subsequent lines of output show the function attempting to open the URL, succeeding, and downloading 30 kilobytes of data. </span></p><p class="c7"><span class="c5">Next, let&rsquo;s take a look at what we got back. In R-Studio we can click on the name of the dataframe in the upper right hand data pane or we can use this command: </span></p><p class="c2"><span class="c14 c17">&gt; View(testFrame) </span><span class="c14"><br></span></p><p class="c19"><span class="c14">Either method will show the contents of the dataframe in the upper left hand window of R-Studio. Alternatively, we could use the </span><span class="c14 c17">str()</span><span class="c5">&nbsp;function to create a summary of the structure of testFrame: </span></p><p class="c2"><span class="c1">&gt; str(testFrame) </span></p><p class="c10"><span class="c1">&#39;data.frame&#39;: 65 obs. of 10 variables: </span></p><p class="c143"><span class="c14 c17">$ </span><span class="c96 c17 c36">table.with.row.headers.in.column.A.and.column.hea </span><span class="c1">ders.in.rows.3.through.4...leading.dots.indicate. sub.parts.: Factor w/ 65 levels &quot;&quot;,&quot;.Alabama&quot;,..: 62 53 1 64 55 54 60 65 2 3 ... </span></p><p class="c10"><span class="c1">$ X : Factor w/ 60 levels &quot;&quot;,&quot;1,052,567&quot;,..: 1 59 60 27 38 47 10 49 32 50 ... </span></p><p class="c10"><span class="c1">$ X.1 : Factor w/ 59 levels &quot;&quot;,&quot;1,052,567&quot;,..: 1 1 59 27 38 47 10 49 32 50 ... </span></p><p class="c10"><span class="c1">$ X.2 : Factor w/ 60 levels &quot;&quot;,&quot;1,052,528&quot;,..: 1 60 21 28 39 48 10 51 33 50 ... </span></p><p class="c10"><span class="c1">$ X.3 : Factor w/ 59 levels &quot;&quot;,&quot;1,051,302&quot;,..: 1 1 21 28 38 48 10 50 33 51 ... </span></p><p class="c10"><span class="c1">$ X.4 : logi NA NA NA NA NA NA ... </span></p><p class="c10"><span class="c1">$ X.5 : logi NA NA NA NA NA NA ... </span></p><p class="c19"><span class="c1">$ X.6 : logi NA NA NA NA NA NA ... </span></p><p class="c10"><span class="c1">$ X.7 : logi NA NA NA NA NA NA ... </span></p><p class="c10"><span class="c1">$ X.8 : logi NA NA NA NA NA NA ... </span></p><p class="c29"><span class="c14">The last few lines are reminiscent of that late 60s song entitled, &quot;&quot;Na Na Hey Hey Kiss Him Goodbye.&quot; Setting aside all the NA NA NA NAs, however, the overall structure is 65 observations of 10 variables, signifying that the spreadsheet contained 65 rows and 10 columns of data. The variable names that follow are pretty bizarre. The first variable name is: </span><span class="c1">&quot;table.with.row.headers.in.column.A.and.column.headers.in.rows. 3.through.4...leading.dots.indicate.sub.parts.&quot; </span></p><p class="c7"><span class="c14">What a mess! It is clear that </span><span class="c14 c17">read.xls() </span><span class="c5">treated the upper leftmost cell as a variable label, but was flummoxed by the fact that this was really just a note to human users of the spreadsheet (the variable labels, such as they are, came on lower rows of the spreadsheet). Subsequent variable names include X, X.1, and X.2: clearly the read.xls() function did not have an easy time getting the variable names out of this file. </span></p><p class="c7"><span class="c14">The other worrisome finding from </span><span class="c14 c17">str()</span><span class="c14">&nbsp;is that all of our data are &quot;factors.&quot; This indicates that R did not see the incoming data as numbers, but rather as character strings that it interpreted as factor data. Again, this is a side effect of the fact that some of the first cells that</span><span class="c14 c17">&nbsp;read.xls()</span><span class="c5">&nbsp;encountered were text rather than numeric. The numbers came much later in the sheet. This also underscores the idea that it is much better to export a data set in a more regular, structured format such as CSV rather than in the original spreadsheet format. Clearly we have some work to do if we are to make use of these data as numeric population values. </span></p><p class="c7"><span class="c5">First, we will use an easy trick to get rid of stuff we don&rsquo;t need. The Census bureau put in three header rows that we can eliminate like this: </span></p><p class="c2"><span class="c1">&gt; testFrame&lt;-testFrame[-1:-3,] </span></p><p class="c29"><span class="c5">The minus sign used inside the square brackets refers to the index of rows that should be eliminated from the dataframe. So the notation -1:-3 gets rid of the first three rows. We also leave the column designator empty so that we can keep all columns for now. So the interpretation of all of the notation within the square brackets is that rows 1 through 3 should be dropped, all other rows should be included, and all columns should be included. We assign the result back to the same data object thereby replacing the original with our new, smaller, cleaner version. </span></p><p class="c7"><span class="c5">Next, we know that of the ten variables we got from read.xls(), only the first five are useful to us (the last five seem to be blank). So this command keeps the first five columns of the dataframe: </span></p><p class="c2"><span class="c1">&gt; testFrame&lt;-testFrame[,1:5] </span></p><p class="c29"><span class="c14">In the same vein, the </span><span class="c14 c17">tail() </span><span class="c5">function shows us that the last few rows just contained some census bureau notes: </span></p><p class="c2"><span class="c1">&gt; tail(testFrame,5) </span></p><p class="c29"><span class="c5">So we can safely eliminate those like this: </span></p><p class="c19"><span class="c1">&gt; testFrame&lt;-testFrame[-58:-62,] </span></p><p class="c7"><span class="c5">If you&rsquo;re alert you will notice that we could have combined some of these commands, but for the sake of clarity we have done each operation individually. The result (which you can check in the upper right hand pane of R-Studio) is a dataframe with 57 rows and five observations. Now we are ready to perform a couple of data transformations. Before we start these, let&rsquo;s give our first column a more reasonable name: </span></p><p class="c2"><span class="c1">&gt; testFrame$region &lt;- testFrame[,1] </span></p><p class="c29"><span class="c5">We&rsquo;ve used a little hack here to avoid typing out the ridiculously long name of that first variable/column. We&rsquo;ve used the column notation in the square brackets on the right hand side of the expression to refer to the first column (the one with the ridiculous name) and simply copied the data into a new column entitled &quot;region.&quot; Let&rsquo;s also remove the offending column with the stupid name so that it does not cause us problems later on: </span></p><p class="c10"><span class="c1">&gt; testFrame&lt;-testFrame[,-1] </span></p><p class="c7"><span class="c14">Next, we can change formats and data types as needed. We can remove the dots from in front of the state names very easily with </span><span class="c14 c17">str_replace()</span><span class="c5">: </span></p><p class="c2"><span class="c1">&gt; testFrame$region &lt;str_replace( + </span></p><p class="c33"><span class="c1">testFrame$region,&quot;\\.&quot;,&quot;&quot;) </span></p><p class="c29"><span class="c14">Don&rsquo;t forget that </span><span class="c14 c17">str_replace() </span><span class="c14">is part of the stringr package, and you will have to use </span><span class="c14 c17">install.packages() </span><span class="c14">and </span><span class="c14 c17">library() </span><span class="c5">to load it if it is not already in place. The two backslashes in the string expression above are called &quot;escape characters&quot; and they force the dot that follows to be treated as a literal dot rather than as a wildcard character. The dot on its own is a wildcard that matches one instance of any character. </span></p><p class="c7"><span class="c14">Next, we can use </span><span class="c14 c17">str_replace_all() </span><span class="c14">and </span><span class="c14 c17">as.numeric()</span><span class="c5">&nbsp;to convert the data contained in the population columns to usable numbers. Remember that those columns are now represented as R &quot;factors&quot; and what we are doing is taking apart the factor labels (which are basically character strings that look like this: &quot;308,745,538&quot;) and making them into numbers. This is sufficiently repetitive that we could probably benefit by created our own function call to do it: </span></p><p class="c73"><span class="c8"># Numberize() Gets rid of commas and other junk and </span></p><p class="c10"><span class="c8"># converts to numbers </span></p><p class="c10"><span class="c8"># Assumes that the inputVector is a list of data that </span></p><p class="c73"><span class="c8"># can be treated as character strings </span></p><p class="c10"><span class="c8">Numberize &lt;- function(inputVector) <br>{ </span></p><p class="c73"><span class="c8"># Get rid of commas </span></p><p class="c10"><span class="c8">inputVector&lt;-str_replace_all(inputVector,&quot;,&quot;,&quot;&quot;) </span></p><p class="c10"><span class="c8"># Get rid of spaces </span></p><p class="c73"><span class="c8">inputVector&lt;-str_replace_all(inputVector,&quot; &quot;,&quot;&quot;) return(as.numeric(inputVector)) </span></p><p class="c73"><span class="c44 c23 c18 c36">}</span></p><p class="c73 c52"><span class="c5"></span></p><p class="c19"><span class="c5">This function is flexible in that it will deal with both unwanted commas and spaces, and will convert strings into numbers whether they are integers or not (i.e., possibly with digits after the decimal point). So we can now run this a few times to create new vectors on the dataframe that contain the numeric values we wanted: </span></p><p class="c10"><span class="c8">testFrame$april10census &lt;-Numberize(testFrame$X) </span></p><p class="c73"><span class="c8">testFrame$april10base &lt;-Numberize(testFrame$X.1) </span></p><p class="c10"><span class="c8">testFrame$july10pop &lt;-Numberize(testFrame$X.2) </span></p><p class="c10"><span class="c8">testFrame$july11pop &lt;-Numberize(testFrame$X.3) </span></p><p class="c7"><span class="c14">By the way, the choice of variable names for the new columns in the dataframe was based on an examination of the original data set that was imported by </span><span class="c14 c17">read.xls()</span><span class="c14">. You can (and should) confirm that the new columns on the dataframe are numeric. You can use </span><span class="c14 c17">str()</span><span class="c5">&nbsp;to accomplish this. </span></p><p class="c7"><span class="c5">We&rsquo;ve spent half a chapter so far just looking at one method of importing data from an external file (either on the web or local storage). A lot of our time was spent conditioning the data we got in order to make it usable for later analysis. Herein lies a very important lesson (or perhaps two). An important, and sometimes time consuming aspect of what data scientists do is to make sure that data are &quot;fit for the purpose&quot; to which they are going to be put. We had the convenience of importing a nice data set directly from the web with one simple command, and yet getting those data actually ready to analyze took several additional steps. </span></p><p class="c7"><span class="c5">A related lesson is that it is important and valuable to try to automate as many of these steps as possible. So when we saw that numbers had gotten stored as factor labels, we moved immediately to create a general function that would convert these to numbers. Not only does this save a lot of future typing, it prevents mistakes from creeping into our processes. </span></p><p class="c7"><span class="c5">Now we are ready to consider the other strategy for getting access to data: querying it from external databases. Depending upon your familiarity with computer programming and databases, you may notice that the abstraction is quite a bit different here. Earlier in the chapter we had a file (a rather messy one) that contained a complete copy of the data that we wanted, and we read that file into R and stored it in our local computer&rsquo;s memory (and possibly later on the hard disk for safekeeping). This is a good and reasonable strategy for small to medium sized datasets, let&rsquo;s say just for the sake of argument anything up to 100 megabytes. </span></p><p class="c29"><span class="c5">But what if the data you want to work with is really large too large to represent in your computer&rsquo;s memory all at once and too large to store on your own hard drive. This situation could occur even with smaller datasets if the data owner did not want people making complete copies of their data, but rather wanted everyone who was using it to work from one &quot;official&quot; version of the data. Similarly, if data do need to be shared among multiple users, it is much better to have them in a database that was designed for this purpose: For the most part R is a poor choice for maintaining data that must be used simultaneously by more than one user. For these reasons, it becomes necessary to do one or both of the following things: </span></p><ol class="c32 lst-kix_31p751u2s1-0 start" start="1"><li class="c29 c13 li-bullet-0"><span class="c5">Allow R to send messages to the large, remote database asking </span></li></ol><p class="c99 c110"><span class="c5">for summaries, subsets, or samples of the data. </span></p><ol class="c32 lst-kix_31p751u2s1-0" start="2"><li class="c13 c128 li-bullet-0"><span class="c5">Allow R to send computation requests to a distributed data processing system asking for the results of calculations performed on the large remote database. </span></li></ol><p class="c7"><span class="c5">Like most contemporary programming languages, R provides several methods for performing these two tasks. The strategy is the same across most of these methods: a package for R provides a &quot;client&quot; that can connect up to the database server. The R client supports sending commands mostly in SQL, structured query language to the database server. The database server returns a result to the R client, which places it in an R data object (typically a data frame) for use in further processing or visualization. </span></p><p class="c7"><span class="c5">The R community has developed a range of client software to enable R to connect up with other databases. Here are the major databases for which R has client software: </span></p><ul class="c32 lst-kix_7j2lgrfbtajv-0 start"><li class="c7 c13 li-bullet-0"><span class="c5">RMySQL Connects to MySQL, perhaps the most popular open source database in the world. MySQL is the M in &quot;LAMP&quot; which is the acronym for Linux, Apache, MySQL, and PHP. Together, these four elements provide a complete solution for data driven web applications. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">ROracle Connects with the widely used Oracle commercial database package. Oracle is probably the most widely used commercial database package. Ironically, Oracle acquired Sun Microsystems a few years ago and Sun developers predominate in development and control of the open source MySQL system, </span></li><li class="c29 c13 li-bullet-0"><span class="c5">RPostgreSQL Connects with the well-developed, full featured PostgreSQL (sometimes just called Postgres) database system. PostgreSQL is a much more venerable system than MySQL and has a much larger developer community. Unlike MySQL, which is effectively now controlled by Oracle, PostgreSQL has a developer community that is independent of any company and a licensing scheme that allows anybody to modify and reuse the code. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">RSQlite Connects with SQlite, another open source, independently developed database system. As the name suggests, SQlite has a very light &quot;code footprint&quot; meaning that it is fast and compact. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">RMongo Connects with the MongoDB system, which is the only system here that does not use SQL. Instead, MongoDB uses JavaScript to access data. As such it is well suited for web development applications. </span></li><li class="c7 c13 li-bullet-0"><span class="c5">RODBC Connects with ODBC compliant databases, which include Microsoft&rsquo;s SQLserver, Microsoft Access, and Microsoft Excel, among others. Note that these applications are native to Windows and Windows server, and as such the support for Linux and Mac OS is limited. </span></li></ul><p class="c7"><span class="c5">For demonstration purposes, we will use RMySQL. This requires installing a copy of MySQL on your computer. Use your web browser to go to this page: </span></p><p class="c7"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://dev.mysql.com/downloads/&amp;sa=D&amp;source=editors&amp;ust=1751550345393694&amp;usg=AOvVaw3u4lQybmVVLoOiTDNiF8cN">http://dev.mysql.com/downloads/ </a></span></p><p class="c7"><span class="c5">Then look for the &quot;MySQL Community Server.&quot; The term community in this context refers to the free, open source developer community version of MySQL. Note that there are also commercial versions of SQL developed and marketed by various companies including Oracle. </span></p><p class="c7"><span class="c5">Download the version of MySQL Community Server that is most appropriate for your computer&rsquo;s operating system and install it. Note that unlike user applications, such as a word processor, there is no real user interface to server software like the MySQL Community Server. Instead, this software runs in the &quot;background&quot; providing services that other programs can use. This is the essence of the client-server idea. In many cases the server is on some remote computer to which we do not have physical access. In this case, we will run the server on our local computer so that we can complete the demonstration. </span></p><p class="c7"><span class="c5">On the Mac installation used in preparation of this chapter, after installing the MySQL server software, it was also important to install the &quot;MySQL Preference Pane,&quot; in order to provide a simple graphical interface for turning the server on and off. Because we are just doing a demonstration here, and we want to avoid future security problems, it is probably sensible to turn MySQL server off when we are done with the demonstration. In Windows, you can use MySQL Workbench to control the server settings on your local computer. </span></p><p class="c7"><span class="c14">Returning to R, use </span><span class="c14 c17">install.packages()</span><span class="c14">&nbsp;and</span><span class="c14 c17">&nbsp;library()</span><span class="c5">&nbsp;to prepare the RMySQL package for use. If everything is working the way it should, you should be able to run the following command from the command line: </span></p><p class="c9"><span class="c26 c17 c23">&gt; con &lt;- dbConnect(dbDriver(&quot;MySQL&quot;), dbname = &quot;test&quot;) </span></p><p class="c135"><span class="c14">The </span><span class="c14 c17">dbConnect() </span><span class="c14">function establishes a linkage or &quot;connection&quot; between R and the database we want to use. This underscores the point that we are connecting to an &quot;external&quot; resource and we must therefore manage the connection. If there were security controls involved, this is where we would provide the necessary information to establish that we were authorized users of the database. In this case, because we are on a &quot;local server&quot; of MySQL, we don&rsquo;t need to provide these. The </span><span class="c14 c17">dbDriver() </span><span class="c14">function provided as an argument to dbConnect specifies that we want to use a MySQL client. The database name specified as dbname=&quot;test&quot; is just a placeholder at this point. We can use the </span><span class="c14 c17">dbListTables() </span><span class="c5">function to see what tables are accessible to us (for our purposes, a table is just like a dataframe, but it is stored inside the database system): </span></p><p class="c2"><span class="c1">&gt; dbListTables(con) </span></p><p class="c10"><span class="c1">character(0) </span></p><p class="c29"><span class="c14">The response &quot;</span><span class="c14 c17">character(0)</span><span class="c5">&quot; means that there is an empty list, so no tables are available to us. This is not surprising, because we just installed MySQL and have not used it for anything yet. Unless you have another database available to import into MySQL, we can just use the census data we obtained earlier in the chapter to create a table in MySQL: </span></p><p class="c10"><span class="c1">&gt; dbWriteTable(con, &quot;census&quot;, testFrame, + </span></p><p class="c33"><span class="c1">overwrite = TRUE) </span></p><p class="c10"><span class="c1">[1] TRUE </span></p><p class="c7"><span class="c14">Take note of the arguments supplied to the </span><span class="c14 c17">dbWriteTable() </span><span class="c14">function. The first argument provides the database connection that we established with the</span><span class="c14 c17">&nbsp;dbConnect() </span><span class="c14">function. The &quot;census&quot; argument gives our new table in MySQL a name. We use testFrame as the source of data as noted above a dataframe and a relational database table are very similar in structure. Finally, we provide the argument </span><span class="c14 c17">overwrite=TRUE</span><span class="c5">, which was not really needed in this case because we know that there were no existing tables but could be important in other operations where we need to make sure to replace any old table that may have been left around from previous work. The function returns the logical value TRUE to signal that it was able to finish the request that we made. This is important in programming new functions because we can use the signal of success or failure to guide subsequent steps and provide error or success messages. </span></p><p class="c7"><span class="c14">Now if we run </span><span class="c14 c17">dbListTables() </span><span class="c5">we should see our new table: </span></p><p class="c2"><span class="c1">&gt; dbListTables(con) </span></p><p class="c10"><span class="c1">[1] &quot;census&quot; </span></p><p class="c29"><span class="c5">Now we can run an SQL query on our table: </span></p><p class="c73"><span class="c8">&gt; dbGetQuery(con, &quot;SELECT region, july11pop FROM census WHERE july11pop&lt;1000000&quot;) </span></p><p class="c10"><span class="c8">region july11pop </span></p><p class="c73"><span class="c8">1 Alaska 722718 </span></p><p class="c10"><span class="c8">2 Delaware 907135 </span></p><p class="c10"><span class="c8">3 District of Columbia 617996 </span></p><p class="c73"><span class="c8">4 Montana 998199 </span></p><p class="c10"><span class="c8">5 North Dakota 683932 </span></p><p class="c10"><span class="c8">6 South Dakota 824082 </span></p><p class="c73"><span class="c8">7 Vermont 626431 </span></p><p class="c10"><span class="c8">8 Wyoming 568158 </span></p><p class="c7"><span class="c14">Note that the </span><span class="c14 c17">dbGetQuery() </span><span class="c5">call shown above breaks onto two lines, but the string starting with SELECT has to be typed all on one line. The capitalized words in that string are the SQL commands. It is beyond the scope of this chapter to give an SQL tutorial, but, briefly, SELECT chooses a subset of the table and the fields named after select are the ones that will appear in the result. The FROM command choose the table(s) where the data should come from. The WHERE command specified a condition, in this case that we only wanted rows where the July 2011 population was less than one million. SQL is a powerful and flexible language and this just scratches the surface. </span></p><p class="c7"><span class="c14">In this case we did not assign the results of </span><span class="c14 c17">dbGetQuery()</span><span class="c5">&nbsp;to another data object, so the results were just echoed to the R console. But it would be easy to assign the results to a dataframe and then use that dataframe for subsequent calculations or visualizations. </span></p><p class="c7"><span class="c14">To emphasize a point made above, the normal motivation for accessing data through MySQL or another database system is that a large database exists on a remote server. Rather than having our own complete copy of those data, we can use </span><span class="c14 c17">dbConnect(), dbGetQuery()</span><span class="c5">&nbsp;and other database functions to access the remote data through SQL. We can also use SQL to specify subsets of the data, to preprocess the data with sorts and other operations, and to create summaries of the data. SQL is also particularly well suited to &quot;joining&quot; data from multiple tables to make new combinations. In the present example, we only used one table, it was a very small table, and we had created it ourselves in R from an Excel source, so none of these were very good motivations for storing our data in MySQL, but this was only a demonstration. </span></p><p class="c7"><span class="c5">The next step beyond remote databases is toward distributed computing across a &quot;cluster&quot; of computers. This combines the remote </span></p><p class="c19"><span class="c5">access to data that we just demonstrated with additional computational capabilities. At this writing, one of the most popular systems for large scale distributed storage and computing is &quot;Hadoop&quot; (named after the toy elephant of the young son of the developer). </span></p><p class="c7"><span class="c5">Hadoop is not a single thing, but is rather a combination of pieces of software called a library. Hadoop is developed and maintained by the same people who maintain the Apache open source web server. There are about a dozen different parts of the Hadoop framework, but the Hadoop Distributed Files System (HDFS) and Hadoop MapReduce framework are two of the most important frameworks. </span></p><p class="c7"><span class="c5">HDFS is easy to explain. Imagine your computer and several other computers at your home or workplace. If we could get them all to work together, we could call them a &quot;cluster&quot; and we could theoretically get more use out of them by taking advantage of all of the storage and computing power they have as a group. Running HDFS, we can treat this cluster of computers as one big hard drive. If we have a really large file too big to fit on any one of the computers HDFS can divide up the file and store its different parts in different storage areas without us having to worry about the details. With a proper configuration of computer hardware, such as an IT department could supply, HDFS can provide an enormous amount of &quot;throughput&quot; (i.e., a very fast capability for reading and writing data) as well as redundancy and failure tolerance. </span></p><p class="c29"><span class="c14 c17">MapReduce </span><span class="c5">is a bit more complicated, but it follows the same logic of trying to divide up work across multiple computers. The term MapReduce is used because there are two big processes involved: map and reduce. For the map operation, a big job is broken up into lots of separate parts. For example, if we wanted to create a search index for all of the files on a company&rsquo;s intranet servers, we could break up the whole indexing task into a bunch of separate jobs. Each job might take care of indexing the files on one server. </span></p><p class="c7"><span class="c5">In the end, though, we don&rsquo;t want dozens or hundreds of different search indices. We want one big one that covers all of the files our company owns. This is where the reduce operation comes in. As all of the individual indexing jobs finish up, a reduce operation combines them into one big job. This combining process works on the basis of a so-called &quot;key.&quot; In the search indexing example, some of the small jobs might have found files that contained the word &quot;fish.&quot; As each small job finishes, it mentioned whether or not fish appeared in a document and perhaps how many times fish appeared. The reduce operation uses fish as a key to match up the results from all of the different jobs, thus creating an aggregated summary listing all of the documents that contained fish. Later, if anyone searched on the word fish, this list could be used to direct them to documents that contained the word. </span></p><p class="c7"><span class="c5">In short, &quot;map&quot; takes a process that the user specifies and an indication of which data it applies to, and divides the processing into as many separate chunks as possible. As the results of each chunk become available, &quot;reduce&quot; combines them and eventually creates and returns one aggregated result. </span></p><p class="c7"><span class="c5">Recently, an organization called RevolutionAnalytics has developed an R interface or &quot;wrapper&quot; for Hadoop that they call RHadoop. This package is still a work in progress in the sense that it does not appear in the standard CRAN package archive, not because there is anything wrong with it, but rather because RevolutionAnalytics wants to continue to develop it without having to provide stable versions for the R community. There is a nice tutorial here: </span></p><p class="c7"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial&amp;sa=D&amp;source=editors&amp;ust=1751550345404466&amp;usg=AOvVaw3AAjPh4JnC8GJcpwN2--fI">https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial</a></span></p><p class="c7"><span class="c14">We will break open the first example presented in the tutorial just to provide further illustration of the use of MapReduce. As with our MySQL example, this is a rather trivial activity that would not normally require the use of a large cluster of computers, but it does show how </span><span class="c14 c17">MapReduce </span><span class="c5">can be put to use. </span></p><p class="c7"><span class="c14">The tutorial example first demonstrates how a repetitive operation is accomplished in R without the use of MapReduce. In prior chapters we have used several variations of the </span><span class="c14 c17">apply()</span><span class="c14">&nbsp;function. The</span><span class="c14 c17">&nbsp;lapply() </span><span class="c14">or list-apply is one of the simplest. You provide an input vector of values and a function to apply to each element, and the </span><span class="c14 c17">lapply() </span><span class="c5">function does the heavy lifting. The example in the RHadoop tutorial squares each integer from one to 10. This first command fills a vector with the input data: </span></p><p class="c2"><span class="c1">&gt; small.ints &lt;1:10 </span></p><p class="c10"><span class="c1">&gt; small.ints </span></p><p class="c10"><span class="c1">[1] 1 2 3 4 5 6 7 8 9 10 </span></p><p class="c29"><span class="c5">Next we can apply the &quot;squaring function&quot; (basically just using the ^ operator) to each element of the list: </span></p><p class="c2"><span class="c1">&gt; out &lt;lapply(small.ints, function(x) x^2) </span></p><p class="c10"><span class="c14 c17">&gt; out <br></span></p><p class="c19"><span class="c1">[[1]] </span></p><p class="c10"><span class="c1">[1] 1 </span></p><p class="c74"><span class="c1">[[2]] </span></p><p class="c10"><span class="c1">[1] 4 </span></p><p class="c9"><span class="c8">... (shows all of the values up to [[10]] [1] 100) </span></p><p class="c86"><span class="c14">In the first command above, we have used</span><span class="c14 c17">&nbsp;lapply() </span><span class="c5">to perform a function on the input vector small.ints. We have defined the function as taking the value x and returning the value x^2. The result is a list of ten vectors (each with just one element) containing the squares of the input values. Because this is such a small problem, R was able to accomplish it in a tiny fraction of a second. </span></p><p class="c7"><span class="c5">After installing both Hadoop and RHadoop which, again, is not an official package, and therefore has to be installed manually we can perform this same operation with two commands: </span></p><p class="c2"><span class="c1">&gt; small.ints &lt;- to.dfs(1:10) </span></p><p class="c10"><span class="c1">&gt; out &lt;- mapreduce(input = small.ints, + </span></p><p class="c33"><span class="c1">map = function(k,v) keyval(v, v^2)) </span></p><p class="c29"><span class="c5">In the first command, we again create a list of integers from one to ten. But rather than simply storing them in a vector, we are using the &quot;distributed file system&quot; or dfs class that is provided by RHadoop. Note that in most cases we would not need to create this ourselves because our large dataset would already exist on the HDFS (Hadoop Distributed FIle System). We would have connected to </span></p><p class="c19"><span class="c14">HDFS and selected the necessary data much as we did earlier in this chapter with </span><span class="c14 c17">dbConnect()</span><span class="c5">. </span></p><p class="c7"><span class="c14">In the second command, we are doing essentially the same thing as we did with </span><span class="c14 c17">lapply()</span><span class="c5">. We provide the input data structure (which, again is a dfs class data object, a kind of pointer to the data stored by Hadoop in the cluster). We also provide a &quot;map function&quot; which is the process that we want to apply to each element in our data set. Notice that the function takes two arguments, k and v. The k refers to the &quot;key&quot; that we mentioned earlier in the chapter. We actually don&rsquo;t need the key in this example because we are not supplying a reduce function. There is in fact, no aggregation or combining activity that needs to occur because our input list (the integers) and the output list (the squares of those integers) are lists of the same size. If we had needed to aggregate the results of the map function, say by creating a mean or a sum, we would have had to provide a &quot;reduce function&quot; that would do the job. </span></p><p class="c7"><span class="c14">The </span><span class="c14 c17">keyval()</span><span class="c14">&nbsp;function, for which there is no documentation at this writing, is characterized as a &quot;helper&quot; function in the tutorial. In this case it is clear that the first argument to keyval, &quot;v&quot; is the integer to which the process must be applied, and the second argument, &quot;v^2&quot; is the squaring function that is applied to each argument. The data returned by </span><span class="c14 c17">mapreduce() </span><span class="c14">is functionally equivalent to that returned by </span><span class="c14 c17">lapply()</span><span class="c5">, i.e., a list of the squares of the integers from 1 to 10. </span></p><p class="c90"><span class="c14">Obviously there is no point in harnessing the power of a cluster of computers to calculate something that could be done with a pencil and a paper in a few seconds. If, however, the operation was more complex and the list of input data had millions of elements, the use of lapply() would be impractical as it would take your computer quite a long time to finish the job. On the other hand, the second strategy of using </span><span class="c14 c17">mapreduce() </span><span class="c5">could run the job in a fraction of a second, given a sufficient supply of computers and storage. </span></p><p class="c7"><span class="c5">On a related note, Amazon, the giant online retailer, provides virtual computer clusters that can be used for exactly this kind of work. Amazon&rsquo;s product is called the Elastic Compute Cloud or EC2, and it is possible to create a small cluster of Linux computers for only a few cents per hour. </span></p><p class="c7"><span class="c5">To summarize this chapter, although there are many analytical problems that require only a small amount of data, the wide availability of larger data sets has added new challenges to data science. As a single user program running on a local computer, R is well suited for work by a single analyst on a data set that is small enough to fit into the computer&rsquo;s memory. We can retrieve these small datasets from individual files stored in human readable 9e.g., CSV) or binary (e.g., XLS) formats. </span></p><p class="c7"><span class="c5">To be able to tackle the larger data sets, however, we need to be able to connect R with either remote databases or remote computational resources or both. A variety of packages is available to connect R to mature database technologies such as MySQL. In fact, we demonstrated the use of MySQL by installing it on a local machine and then using the RMySQL package to create a table and query it. The more cutting edge technology of Hadoop is just becoming available for R users. This technology, which provides the potential for both massive storage and parallel computational power, promises to make very large datasets available for processing and analysis in R. <br></span></p><p class="c19"><span class="c59 c14 c57 c23">Chapter Challenge </span></p><p class="c7"><span class="c5">Hadoop is a software framework designed for use with Apache, which is first and foremost a Linux server application. Yet there are development versions of Hadoop available for Windows and Mac as well. These are what are called single node instances, that is they use a single computer to simulate the existence of a large cluster of computers. See if you can install the appropriate version of Hadoop for your computer&rsquo;s operating system. </span></p><p class="c7"><span class="c5">As a bonus activity, if you are successful in installing Hadoop, then get a copy of the RHadoop package from RevolutionAnalytics and install that. If you are successful with both, you should be able to run the MapReduce code presented in this chapter. </span></p><h3 class="c124" id="h.os8lqgdj7y1t"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_io5u6hn27c2s-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/doc/manuals/R-data.html&amp;sa=D&amp;source=editors&amp;ust=1751550345412532&amp;usg=AOvVaw046BvPEOPSPrwFb8mYe2eD">http://cran.r-project.org/doc/manuals/R-data.html </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/doc/manuals/R-data.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550345412737&amp;usg=AOvVaw3UeU_6rkHzytzPI_VNKJFR">http://cran.r-project.org/doc/manuals/R-data.pdf </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/web/packages/gdata/gdata.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550345412972&amp;usg=AOvVaw2tXex2RHARTOVjNDE-aa2Y">http://cran.r-project.org/web/packages/gdata/gdata.pdf </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://dev.mysql.com/downloads/&amp;sa=D&amp;source=editors&amp;ust=1751550345413171&amp;usg=AOvVaw2vo59cgqk74bd8dWwVPQCw">http://dev.mysql.com/downloads/ </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Comparison_of_relational_databas&amp;sa=D&amp;source=editors&amp;ust=1751550345413383&amp;usg=AOvVaw0uyqD8CPopUhdWaBwXoUC7">http://en.wikipedia.org/wiki/Comparison_of_relational_databas e_management_systems </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Mapreduce&amp;sa=D&amp;source=editors&amp;ust=1751550345413523&amp;usg=AOvVaw29fQsRWumMJ1csROhSmsoO">http://en.wikipedia.org/wiki/Mapreduce </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial&amp;sa=D&amp;source=editors&amp;ust=1751550345413708&amp;usg=AOvVaw24wJwu6K4v5MlKN_MUgC_c">https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial </a></span></li></ul><h3 class="c94 c104" id="h.6rkqhnqffbxm"><span class="c38 c57 c18 c147"></span></h3><h3 class="c39" id="h.ey3tsuagxywo"><span class="c38 c57 c23 c18"><br>R Functions Used in this Chapter </span></h3><ul class="c32 lst-kix_ourtwwltf86-0 start"><li class="c0 li-bullet-0"><span class="c14 c17">as.numeric() </span><span class="c5">Convert another data type to a number </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">dbConnect() </span><span class="c5">Connect to an SQL database </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">dbGetQuery() </span><span class="c5">Run an SQL query and return the results </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">dbListTables() </span><span class="c5">Show the tables available in a connection </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">dbWriteTable() </span><span class="c5">Send a data table to an SQL systems </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">install.packages() </span><span class="c5">Get the code for an R package </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">lapply() </span><span class="c5">Apply a function to elements of a list </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">library() </span><span class="c5">Make an R package available for use </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">Numberize() </span><span class="c5">A custom function created in this chapter </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">read.xls() </span><span class="c5">Import data from a binary R file; part of the gdata package </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">return() </span><span class="c5">Used in functions to designate the data returned by the function </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">str_replace() </span><span class="c5">Replace a character string with another </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">str_replace_all() </span><span class="c5">Replace multiple instances of a character string with another </span></li></ul><p class="c7 c52"><span class="c5"></span></p><hr style="page-break-before:always;display:none;"><p class="c81 c62 c52 title" id="h.no79sk19q4j4"><span class="c38 c57 c23 c84"></span></p><p class="c81 c62 title" id="h.aooh6ulrxk2k"><span class="c70">CHAPTER 15</span><span class="c38 c57 c23 c84">&nbsp;<br>Map MashUp </span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image7.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="topo map book and potato masher"></span><span class="c44 c23 c37 c36">Much of what we have accomplished so far has focused on the standard rectangular dataset: one neat table with rows and columns well defined. Yet much of the power of data science comes from bringing together different sources of data in complementary ways. In this chapter we combine different sources of data to make a unique product that transcends any one source. </span></p><p class="c19 c52"><span class="c44 c23 c37 c36"></span></p><p class="c19"><span class="c5">MThe term mashup originated in the music business decades ago related to the practice of overlaying one music recording on top of another one. The term has entered general usage to mean anything that brings together disparate influences or elements. In the application development area, mashup often refers to bringing together various sources of data to create a new product with unique value. There&rsquo;s even a non-profit group called the Open Mashup Alliance that develops standards and methods for creating new mashups. </span></p><p class="c7"><span class="c14">One example of a mashup is </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.housingmaps.com&amp;sa=D&amp;source=editors&amp;ust=1751550345416879&amp;usg=AOvVaw1F2oXuhETEBaljoGEPjcSg">HousingMaps</a></span><span class="c5">, a web application that grabs apartment rental listings from the classified advertising service Craigslist and plots them on an interactive map that shows the location of each listing. If you have ever used Craigslist you know that it provides a very basic text-based interface and that the capability to find listings on a map would be welcome. </span></p><p class="c7"><span class="c5">In this chapter we tackle a similar problem. Using some address data from government records, we call the Google geocoding API over the web to find the latitude and longitude of each address. Then we plot these latitudes and longitudes on a map of the U.S. This activities reuses skills we learned in the previous chapter for reading in data files, adds some new skills related to calling web APIs, and introduces us to a new type of data, namely the shapefiles that provide the basis for electronic maps. </span></p><p class="c29"><span class="c5">Let&rsquo;s look at the new stuff first. The Internet is full of shapefiles that contain mapping data. Shapefiles are a partially proprietary, partially open format supported by a California software company called ESRI. Shapefile is actually an umbrella term that covers several different file types. Because the R community has provided some packages to help deal with shapefiles, we don&rsquo;t need too much information about the details. The most important thing to know is that shapefiles contain points, polygons, and &quot;polylines.&quot; Everyone knows what a point and a polygon are, but a polyline is a term used by computer scientist to refer to a multi-segment line. In many graphics applications it is much easier to approximate a curved line with many tiny connected straight lines than it is to draw a truly curved line. If you think of a road or a river on a map, you will have a good idea of a polyline. </span></p><p class="c7"><span class="c14">The U.S. Census bureau publishes shapefiles at various levels of detail for every part of the country. Search for the term &quot;shapefile&quot; at &quot;site:census.gov&quot; and you will find several pages with listings of different shapefiles. For this exercise, we are using a relatively low detail map of the whole U.S. from </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html&amp;sa=D&amp;source=editors&amp;ust=1751550345419713&amp;usg=AOvVaw09bhGxyKqPU_gFW7Kcr8KS">Cartographic Boundary Files - Shapefile</a></span><span class="c14">. </span><span class="c5">&nbsp;We downloaded a &quot;zip&quot; file. Unzip this (usually just by double-clicking on it) and you will find several files inside it. The file ending in &quot;shp&quot; is the main shapefile. Another file that will be useful to us ends in &quot;dbf&quot; which contains labels and other information. </span></p><p class="c7"><span class="c14">To get started, we will need two new R packages called </span><span class="c14 c17">PBSmapping </span><span class="c14">and </span><span class="c14 c17">maptools</span><span class="c14">. </span><span class="c14 c17">PBSmapping </span><span class="c14">refers not to public broadcasting, but rather to the Pacific Biology Station, whose researchers and technologists kindly bundled up a wide range of the R processing tools that they use to manage map data. The </span><span class="c14 c17">maptools </span><span class="c14">package was developed by Nicholas J. Lewin-Koh (University of Singapore) and others to provide additional tools and some &quot;wrappers&quot; to make </span><span class="c14 c17">PBSmapping </span><span class="c5">functions easier to use. In this chapter we only scratch the surface of the available tools: there could be a whole book just dedicated to R mapping functions alone. </span></p><p class="c7"><span class="c5">Before we read in the data we grabbed from the Census Bureau, let&rsquo;s set the working directory in R-Studio so that we don&rsquo;t have to type it out on the command line. Click on the Tools menu and then choose &quot;Set Working Directory.&quot; Use the dialog box to designate the folder where you have unzipped the shape data. After that, these commands will load the shape data into R and show us what we have: <br></span></p><p class="c19"><span class="c1">&gt; usShape &lt;- importShapefile( + </span></p><p class="c19"><span class="c1">&quot;gz_2010_us_040_00_500k&quot;,readDBF=TRUE) </span></p><p class="c19"><span class="c1">&gt; summary(usShape) </span></p><p class="c19"><span class="c1">PolySet </span></p><p class="c19"><span class="c1">Records : 90696 </span></p><p class="c19"><span class="c1">Contours : 574 </span></p><p class="c19"><span class="c1">Holes : 0 </span></p><p class="c19"><span class="c1">Events : NA </span></p><p class="c19"><span class="c1">On boundary : NA </span></p><p class="c19"><span class="c1">Ranges </span></p><p class="c19"><span class="c1">X : [-179.14734, 179.77847] </span></p><p class="c19"><span class="c1">Y : [17.884813, 71.3525606439998] </span></p><p class="c19"><span class="c1">Attributes </span></p><p class="c19"><span class="c1">Projection : LL </span></p><p class="c19"><span class="c1">Zone : NULL <br></span></p><p class="c19"><span class="c1">Extra columns : </span></p><p class="c19"><span class="c1">&gt; plotPolys(usShape) </span></p><p class="c7"><span class="c5">This last command gives us a simple plot of the 90,696 shapes that our shapefile contains. Here is the plot: </span></p><p class="c19"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image10.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c19"><span class="c14">This is funny looking! The ranges output from the </span><span class="c14 c17">summary() </span><span class="c5">command gives us a hint as to why. The longitude of the elements in our map range from -179 to nearly 180: this covers pretty much the whole of the planet. The reason is that the map contains shapes for Hawaii and Alaska. Both states have far western longitudes, but the Aleutian peninsula in Alaska extends so far that it crosses over the longitude line where -180 and 180 meet in the Pacific Ocean. As a result, the continental U.S. is super squashed. We can specify a more limited area of the map to consider by using the xlim and ylim parameters. The following command: </span></p><p class="c2"><span class="c1">&gt; plotPolys(usShape,+ </span></p><p class="c33"><span class="c1">xlim=c(-130,-60),ylim=c(20,50)) </span></p><p class="c29"><span class="c5">...gives a plot that shows the continental U.S. more in its typical proportions. </span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image51.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=" the continental U.S."></span><span class="c5"><br>So now we have some map data stored away and ready to use. The PBSmapping package gives us the capability of adding points to an existing map. For now, let&rsquo;s demonstrate this with a made up point somewhere in Texas: </span></p><p class="c19"><span class="c24 c23 c18">&nbsp;</span></p><p class="c19"><span class="c1">&gt; X &lt;-100 </span></p><p class="c10"><span class="c1">&gt; Y &lt;- 30 </span></p><p class="c10"><span class="c1">&gt; EID &lt;- 1 </span></p><p class="c2"><span class="c1">&gt; pointData &lt;- data.frame(EID,X,Y) </span></p><p class="c10"><span class="c1">&gt; eventData &lt;-as.EventData( + </span></p><p class="c33"><span class="c1">pointData,projection=NA) </span></p><p class="c10"><span class="c1">&gt; addPoints(eventData,col=&quot;red&quot;,cex=.5) </span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 550.00px; height: 450.00px;"><img alt="" src="images/image37.png" style="width: 550.00px; height: 450.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="continental U.S. with red dot in Texas"></span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">You have to look carefully, but in southern Texas there is now a little red dot. We began by manually creating a single point specified by X (longitude), Y (latitude), and EID (an identification number) and sticking it into a dataframe. Then we converted the data in that dataframe into an EventData object. This is a custom class of objects specified by the PBSmapping package. The final command above adds the EventData to the plot. </span></p><p class="c7"><span class="c5">The idea of EventData is a little confusing, but if you remember that this package was developed by biologists at the Pacific Biology Station to map sightings of fish and other natural phenomena it makes more sense. In their lingo, an event was some observation of interest that occurred at a particular day, time, and location. The &quot;event id&quot; or EID &lt;1 that we stuck in the data frame was just saying that this was the first point in our list that we wanted to plot. For us it is not an event so much as a location of something we wanted to see on the map. </span></p><p class="c7"><span class="c14">Also note that the &quot;</span><span class="c14 c17">projection=NA</span><span class="c14">&quot; parameter in the as.</span><span class="c14 c17">EventData() </span><span class="c5">coercion is just letting the mapping software know that we don&rsquo;t want our point to be transformed according to a mapping projection. If you remember from your Geography class, a projection is a mapping technique to make a curved object like the Earth seem sensible on a flat map. In this example, we&rsquo;ve already flattened out the U.S., so there is no need to transform the points. </span></p><p class="c7"><span class="c14">Next, we need a source of points to add to our map. This could be anything that we&rsquo;re interested in: the locations of restaurants, crime scenes, colleges, etc. In Google a search for filetype:xls or filetype;csv with appropriate additional search terms can provide interesting data sources. You may also have mailing lists of customers or clients. The most important thing is that we will need street address, city, and state in order to geocode the addresses. For this example, we searched for &quot;housing street address list filetype:csv&quot;, and this turned up a data set of small businesses that have contracts with the U.S. Department of Health and Human services. Let&rsquo;s read this in using </span><span class="c14 c17">read.csv()</span><span class="c5">: </span></p><p class="c2"><span class="c1">&gt; dhhsAddrs &lt;read.csv(&quot;DHHS_Contracts.csv&quot;) </span></p><p class="c10"><span class="c1">&gt; str(dhhsAddrs) </span></p><p class="c10"><span class="c1">&#39;data.frame&#39;: 599 obs. of 10 variables: </span></p><p class="c10"><span class="c1">$ Contract.Number : Factor w/ 285 levels &quot;26301D0054&quot;,&quot;500000049&quot;,..: 125 125 125 279 164 247 19 242 275 70 ... </span></p><p class="c2"><span class="c1">$ Contractor.Name : Factor w/ 245 levels &quot;2020 COMPANY LIMITED LIABILITY COMPANY&quot;,..: 1 1 1 2 2 3 4 6 5 7 ... </span></p><p class="c143"><span class="c1">$ Contractor.Address : Factor w/ 244 levels &quot;1 CHASE SQUARE 10TH FLR, ROCHESTER, NY &quot;,..: 116 116 116 117 117 136 230 194 64 164 ... </span></p><p class="c143"><span class="c1">$ Description.Of.Requirement: Factor w/ 468 levels &quot;9TH SOW DEFINTIZE THE LETTER CONTRACT&quot;,..: 55 55 55 292 172 354 308 157 221 340 ... </span></p><p class="c10"><span class="c1">$ Dollars.Obligated : Factor w/ 586 levels &quot; $1,000,000.00 &quot;,..: 342 561 335 314 294 2 250 275 421 21 ... </span></p><p class="c2"><span class="c1">$ NAICS.Code : Factor w/ 55 levels &quot;323119&quot;,&quot;334310&quot;,..: 26 26 26 25 10 38 33 29 27 35 ... </span></p><p class="c10"><span class="c14 c17">$ Ultimate.Completion.Date : Factor w/ 206 levels &quot;1-Aug-2011&quot;,&quot;1-Feb-2013&quot;,..: 149 149 149 10 175 161 124 37 150 91 ... <br></span></p><p class="c19"><span class="c1">$ Contract.Specialist : Factor w/ 95 levels &quot;ALAN FREDERICKS&quot;,..: 14 14 60 54 16 90 55 25 58 57 ... </span></p><p class="c122"><span class="c14 c17">$ Contract.Specialist.Email : Factor w/ 102 levels &quot;410-786-8622&quot;,..: 16 16 64 59 40 98 60 29 62 62 ... </span><span class="c59 c96 c17 c36">$ X : logi NA NA NA NA NA NA ... </span></p><p class="c29"><span class="c14">It sounds like a</span><span class="c5">&nbsp;crazy 60s song again! Anyhow, we read in a comma separated data set with 599 rows and 10 variables. The most important field we have there is Contractor.Address. This contains the street addresses that we need to geocode. We note, however, that the data type for these is Factor rather than character string. So we need to convert that: </span></p><p class="c10"><span class="c1">&gt; dhhsAddrs$strAddr &lt;+ </span></p><p class="c33"><span class="c1">as.character(dhhsAddrs$Contractor.Address) </span></p><p class="c2"><span class="c1">&gt; mode(dhhsAddrs$strAddr) </span></p><p class="c10"><span class="c1">[1] &quot;character&quot; </span></p><p class="c10"><span class="c1">&gt; tail(dhhsAddrs$strAddr,4) </span></p><p class="c10"><span class="c1">[1] &quot;1717 W BROADWAY, MADISON, WI &quot; </span></p><p class="c10"><span class="c1">[2] &quot;1717 W BROADWAY, MADISON, WI &quot; </span></p><p class="c10"><span class="c1">[3] &quot;1717 W BROADWAY, MADISON, WI &quot; </span></p><p class="c10"><span class="c1">[4] &quot;789 HOWARD AVE, NEW HAVEN, CT, &quot; </span></p><p class="c29"><span class="c14">This looks pretty good: Our new column, called </span><span class="c14 c17">dhhsAddrs</span><span class="c5">, is character string data, converted from the factor labels of the original Contractor.Address column. Now we need to geocode these. </span></p><p class="c19"><span class="c5">We will use the Google geocoding application programming interface (API) which is pretty easy to use, does not require an account or application ID, and allows about 2500 address conversions per day. The API can be accessed over the web, using what is called an HTTP GET request. Note that the Terms of Service for the Google geocoding API are very specific about how the interface can be used most notably on the point that the geocodes must be used on Google maps. Make sure you read the Terms of Service before you create any software applications that use the geocoding service. See the link in the bibliography at the end of the chapter. The bibliography has a link to an article with dozens of other geocoding APIs if you disagree with Google&rsquo;s Terms of Service. </span></p><p class="c29"><span class="c5">These acronyms probably look familiar. HTTP is the Hyper Text Transfer Protocol, and it is the standard method for requesting and receiving web page data. A GET request consists of information that is included in the URL string to specify some details about the information we are hoping to get back from the request. Here is an example GET request to the Google geocoding API: </span></p><p class="c7"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://maps.googleapis.com/maps/api/geocode/json?address%3D1&amp;sa=D&amp;source=editors&amp;ust=1751550345431756&amp;usg=AOvVaw3MmYm2waLCOhZrzfoRHbKy">http://maps.googleapis.com/maps/api/geocode/json?address=1 600+Pennsylvania+Avenue,+Washington,+DC&amp;sensor=false </a></span></p><p class="c7"><span class="c14">The first part of this should look familiar: The </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://maps.googleapis.com&amp;sa=D&amp;source=editors&amp;ust=1751550345432053&amp;usg=AOvVaw2ErO4Q0bkGb5VdndeqbKmH">http://maps.googleapis.com</a></span><span class="c5">&nbsp;part of the URL specifies the domain name just like a regular web page. The next part of the URL, &quot;/maps/api/geocode&quot; tells Google which API we want to use. Then the &quot;json&quot; indicates that we would like to receive our result in &quot;Java Script Object Notation&quot; (JSON) which is a structured, but human readable way of sending back some data. The address appears next, and we are apparently looking for the White House at 1600 Pennsylvania Avenue in Washington, DC. Finally, sensor=false is a required parameter indicating that we are not sending our request from a mobile phone. You can type that whole URL into the address field of any web browser and you should get a sensible result back. The JSON notation is not beautiful, but you will see that it makes sense and provides the names of individual data items along with their values. Here&rsquo;s a small excerpt that shows the key parts of the data object that we are trying to get our hands on: </span></p><p class="c2"><span class="c1">{ </span></p><p class="c10"><span class="c1">&quot;results&quot; : [ </span></p><p class="c10"><span class="c1">{ </span></p><p class="c10"><span class="c1">&quot;address_components&quot; : [ </span></p><p class="c10"><span class="c1">&quot;geometry&quot; : { </span></p><p class="c10"><span class="c1">&quot;location&quot; : { </span></p><p class="c10"><span class="c1">&quot;lat&quot; : 38.89788009999999, </span></p><p class="c10"><span class="c1">&quot;lng&quot; : -77.03664780 </span></p><p class="c10"><span class="c1">}, </span></p><p class="c10"><span class="c1">&quot;status&quot; : &quot;OK&quot; </span></p><p class="c10"><span class="c14 c17">}</span></p><p class="c10 c52"><span class="c5"></span></p><p class="c19"><span class="c14">There&rsquo;s tons more data in the JSON object that Google returned, and fortunately there is an R package, called JSONIO, that will extract the data we need from the structure without having to parse it ourselves. &nbsp;In order to get R to send the HTTP GET requests to google, we will also need to use the RCurl package. This will give us a single command to send the request and receive the result back, essentially doing all of the quirky steps that a web browser takes care of automatically for us. To get started,</span><span class="c14 c17">&nbsp;install.packages()</span><span class="c14">&nbsp;and </span><span class="c14 c17">library()</span><span class="c5">&nbsp;the two packages we will need RCurl and JSONIO. </span></p><p class="c7"><span class="c5">Next, we will create a new helper function to take the address field and turn it into the URL that we need: <br></span></p><p class="c42"><span class="c8"># Format an URL for the Google Geocode API </span></p><p class="c42"><span class="c8">MakeGeoURL &lt;- function(address) </span></p><p class="c42"><span class="c8">{ </span></p><p class="c42"><span class="c8">root &lt;- &quot;http://maps.google.com/maps/api/geocode/&quot; </span></p><p class="c42"><span class="c8">url &lt;- paste(root, &quot;json?address=&quot;, + </span></p><p class="c42"><span class="c8">address, &quot;&amp;sensor=false&quot;, sep = &quot;&quot;) </span></p><p class="c42"><span class="c8">return(URLencode(url)) </span></p><p class="c42"><span class="c8">}</span></p><p class="c10"><span class="c14">There are three simple steps here. The first line initializes the beginning part of the URL into a string called root. Then we use paste() to glue together the separate parts of the strong (note the sep=&quot;&quot; so we don&rsquo;t get spaces between the parts). This creates a string that looks almost like the one in the White House example two pages ago. The final step converts the string to a legal URL using a utility function called </span><span class="c14 c17">URLencode() </span><span class="c5">that RCurl provides. Let&rsquo;s try it: </span></p><p class="c2"><span class="c1">&gt; MakeGeoURL( + &quot;1600 Pennsylvania Avenue, Washington, DC&quot;) </span></p><p class="c10"><span class="c1">[1] &quot;http://maps.google.com/maps/api/geocode/json?add ress=1600%20Pennsylvania%20Avenue,%20Washington,% 20DC&amp;sensor=false&quot; </span></p><p class="c29"><span class="c5">Looks good! Just slightly different than the original example (%20 instead of the plus character) but hopefully that won&rsquo;t make a difference. Remember that you can type this function at the command line or you can create it in the script editing window in the upper left hand pane of R-Studio. The latter is the better way to go and if you click the &quot;Source on Save&quot; checkmark, R-Studio will make sure to update R&rsquo;s stored version of your function every time you save the script file. </span></p><p class="c7"><span class="c14">Now we are ready to use our new function,</span><span class="c14 c17">&nbsp;MakeGeoURL()</span><span class="c5">, in another function that will actually request the data from the Google API: </span></p><p class="c2"><span class="c1">Addr2latlng &lt;- function(address) </span></p><p class="c10"><span class="c1">{ </span></p><p class="c10"><span class="c1">url &lt;- MakeGeoURL(address) </span></p><p class="c10"><span class="c1">apiResult &lt;- getURL(url) </span></p><p class="c10"><span class="c1">geoStruct &lt;- fromJSON(apiResult, + </span></p><p class="c33"><span class="c1">simplify = FALSE) </span></p><p class="c10"><span class="c1">lat &lt;- NA </span></p><p class="c19"><span class="c1">lng &lt;- NA </span></p><p class="c10"><span class="c1">try(lat &lt;- + </span></p><p class="c33"><span class="c1">geoStruct$results[[1]]$geometry$location$lat) </span></p><p class="c2"><span class="c1">try(lng &lt;- + </span></p><p class="c33"><span class="c1">geoStruct$results[[1]]$geometry$location$lng) </span></p><p class="c10"><span class="c1">return(c(lat, lng)) </span></p><p class="c10"><span class="c1">}</span></p><p class="c10"><span class="c14">We have defined this function to receive an address string as its only argument. The first thing it does is to pass the URL string to </span><span class="c14 c17">MakeGeoURL()</span><span class="c14">&nbsp;to develop the formatted URL. Then the function passes the URL to </span><span class="c14 c17">getURL()</span><span class="c14">, which actually does the work of sending the request out onto the Internet. The </span><span class="c14 c17">getURL()</span><span class="c14">&nbsp;function is part of the </span><span class="c14 c17">RCurl </span><span class="c5">package. This step is just like typing a URL into the address box of your browser. </span></p><p class="c29"><span class="c14">We capture the result in an object called &quot;apiResult&quot;. If we were to stop and look inside this, we would find the JSON structure that appeared a couple of pages ago. We can pass this structure to the function </span><span class="c14 c17">fromJSON() </span><span class="c14">we put the result in an object called </span><span class="c14 c17">geoStruct</span><span class="c14">. This is a regular R data frame such that we can access any individual element using regular $ notation and the array index </span><span class="c14 c17">[[1]]</span><span class="c5">. In other instances, a JSON object may contain a whole list of data structures, but in this case there is just one. If you compare the variable names &quot;geometry&quot;, &quot;location&quot;, &quot;lat&quot; and &quot;lng&quot; to the JSON example a few pages ago you will find that they match perfectly. The fromJSON() function in the JSONIO package has done all the heavy lifting of breaking the JSON structure into its component pieces. </span></p><p class="c29"><span class="c14">Note that this is the first time we have encountered the </span><span class="c14 c17">try() </span><span class="c14">function. When programmers expect the possibility of an error, they will frequently use methods that are tolerant of errors or that catch errors before they disrupt the code. If our call to </span><span class="c14 c17">getURL()</span><span class="c14">&nbsp;returns something unusual that we aren&rsquo;t expecting, then the JSON structure may not contain the fields that we want. By surrounding our command to assign the lat and lng variables with a </span><span class="c14 c17">try()</span><span class="c5">&nbsp;function, we can avoid stopping the flow of the code if there is an error. Because we initialized lat and lng to NA above, this function will return a two item list with both items being NA if an error occurs in accessing the JSON structure. There are more elegant ways to accomplish this same goal. For example, the Google API puts an error code in the JSON structure and we could choose to interpret that instead. We will leave that to the chapter challenge! </span></p><p class="c7"><span class="c14">In the last step, our new </span><span class="c14 c17">Addr2latlng()</span><span class="c5">&nbsp;function returns a two item list containing the latitude and longitude. We can test it out right now: </span></p><p class="c2"><span class="c1">&gt; testData &lt;- Addr2latlng( + </span></p><p class="c33"><span class="c1">&quot;1600 Pennsylvania Avenue, Washington, DC&quot;) </span></p><p class="c10"><span class="c1">&gt; str(testData) </span></p><p class="c10"><span class="c1">num [1:2] 38.9 -77 </span></p><p class="c90"><span class="c14">Perfect! we called our new function </span><span class="c14 c17">Addr2latlng()</span><span class="c5">&nbsp;with the address of the Whitehouse and got back a list with two numeric items containing the latitude and longitude associated with that address. With just a few lines of R code we have harnessed the power of Google&rsquo;s extensive geocoding capability to convert a brief text street address into mapping coordinates. </span></p><p class="c90"><span class="c5">At this point there isn&rsquo;t too much left to do. We have to create a looping mechanism so that we can process the whole list of addresses in our DHHS data set. We have some small design choices here. It would be possible to attach new fields to the existing dataframe. Instead, the following code keeps everything pretty simple by receiving a list of addresses and returning a new data frame with X, Y, and EID ready to feed into our mapping software: </span></p><p class="c73"><span class="c8"># Process a whole list of addresses </span></p><p class="c10"><span class="c8">ProcessAddrList &lt;- function(addrList) </span></p><p class="c10"><span class="c8">{ </span></p><p class="c73"><span class="c8">resultDF &lt;- data.frame(atext=character(), + </span></p><p class="c33"><span class="c8">X=numeric(),Y=numeric(),EID=numeric()) </span></p><p class="c10"><span class="c8">i &lt;- 1 </span></p><p class="c73"><span class="c8">for (addr in addrList) </span></p><p class="c10"><span class="c8">{ </span></p><p class="c10"><span class="c8">latlng = Addr2latlng(addr) </span></p><p class="c73"><span class="c8">resultDF &lt;- rbind(resultDF,+ </span></p><p class="c33"><span class="c8">data.frame(atext=addr, + X=latlng[[2]],Y=latlng[[1]], EID=i)) </span></p><p class="c19"><span class="c8">i &lt;- i + 1 </span></p><p class="c19"><span class="c8">} </span></p><p class="c10"><span class="c8">return(resultDF) </span></p><p class="c73"><span class="c8">}</span></p><p class="c73"><span class="c96 c36">This new function takes one argument, the list of addresses, which </span><span class="c14">should be character strings. In the first step we make an empty dataframe for use in the loop. In the second step we initialize a scalar index variable called </span><span class="c14 c17">i</span><span class="c5">&nbsp;to the number 1. We will increment this in the loop and use it as our EID. </span></p><p class="c7"><span class="c14">Then we have the for loop. We are using a neat construct here called &quot;</span><span class="c14 c17">in</span><span class="c14">&quot;. The expression &quot;</span><span class="c14 c17">addr</span><span class="c14">&nbsp;in </span><span class="c14 c17">addrList</span><span class="c14">&quot; creates a new variable called addr. Every time that R goes through the loop it assigns to </span><span class="c14 c17">addr </span><span class="c14">the next item in </span><span class="c14 c17">addrList</span><span class="c14">. When </span><span class="c14 c17">addrList </span><span class="c5">runs out of items, the for loop ends. Very handy! </span></p><p class="c7"><span class="c14">Inside the for loop the first thing we do is to call the function that we developed earlier: </span><span class="c14 c17">Addr2latlng()</span><span class="c14">. This performs one conversion of an address to a geocode (latitude and longitude) as described earlier. We pass </span><span class="c14 c17">addr </span><span class="c5">to it as add contains the text address for this time around the loop. We put the result in a new variable called latlng. Remember that this is a two item list. </span></p><p class="c7"><span class="c14">The next statement, starting with &quot;</span><span class="c14 c17">resultDF &lt;- rbind</span><span class="c14">&quot; is the heart of this function. Recall that </span><span class="c14 c17">rbind()</span><span class="c14">&nbsp;sticks a new row on the end of a dataframe. So in the arguments to </span><span class="c14 c17">rbind() </span><span class="c14">we supply our earlier version of </span><span class="c14 c17">resultDF </span><span class="c5">(which starts empty and grows from there) plus a new row of data. The new row of data includes the text address (not strictly necessary but handy for diagnostic purposes), the &quot;X&quot; that our mapping software expects (this is the longitude), the &quot;Y&quot; that the mapping software expects, and the event ID, EID, that the mapping software expects. </span></p><p class="c7"><span class="c14">At the end of the for loop, we increment the index </span><span class="c14 c17">i</span><span class="c14">&nbsp;so that we will have the next number next time around for EID. Once the for loop is done we simply return the dataframe object, </span><span class="c14 c17">resultDF</span><span class="c5">. Piece of cake! </span></p><p class="c7"><span class="c5">Now let&rsquo;s try it and plot our points: <br></span></p><p class="c19"><span class="c8">&gt; dhhsPoints &lt;- ProcessAddrList(dhhsAddrs$strAddr) </span></p><p class="c10"><span class="c8">&gt; dhhsPoints &lt;- dhhsPoints[!is.na(dhhsPoints$X),] </span></p><p class="c73"><span class="c8">&gt; eventData &lt;- as.EventData(dhhsPoints,projection=NA) </span></p><p class="c10"><span class="c8">&gt; addPoints(eventData,col=&quot;red&quot;,cex=.5) </span></p><p class="c7"><span class="c14">The second command above is the only one of the four that may seem unfamiliar. The </span><span class="c14 c17">as.EventData() </span><span class="c14">coercion is picky and will not process the dataframe if there are any fields that are NA. To get rid of those rows that do not have complete latitude and longitude data, we use </span><span class="c14 c17">is.na() </span><span class="c14">to test whether the X value on a given row is NA. We use the </span><span class="c14 c17">!</span><span class="c14">&nbsp;(not) operator to reverse the sense of this test. So the only rows that will survive this step are those where X is not NA. The plot below shows the results. </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 484.00px; height: 342.32px;"><img alt="" src="images/image57.png" style="width: 550.00px; height: 450.36px; margin-left: -41.00px; margin-top: -55.36px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="US map with geolocation"></span></p><p class="c7"><span class="c5">If you like conspiracy theories, there is some support in this graph: The vast majority of the companies that have contracts with DHHS are in the Washington, DC area, with a little trickle of additional companies heading up the eastern seaboard as far as Boston and southern New Hampshire. Elsewhere in the country there are a few companies here and there, particularly near the large cities of the east and south east. </span></p><p class="c7"><span class="c14">Using some of the parameters on </span><span class="c14 c17">plotPolys()</span><span class="c5">&nbsp;you could adjust the zoom level and look at different parts of the country in more detail. If you remember that the original DHHS data also had the monetary size of the contract in it, it would also be interesting to change the size or color of the dots depending on the amount of money in </span></p><p class="c19"><span class="c5">the Dollars.Obligated field. This would require running addPoints() in a loop and setting the col= or cex= parameters for each new point. </span></p><p class="c166"><span class="c5">If you get that far and are still itching for another challenge, try improving the map. One idea for this was mentioned above: you could change the size or color of the dots based on the size of the contract received. An even better (and much harder) idea would be to sum the total dollar amount being given within each state and then color each state according to how much DHHS money it receives. This would require delving into the shapefile data quite substantially so that you could understand how to find the outline of a state and fill it in with a color. </span></p><h3 class="c94" id="h.a5x8og3kh9nq"><span class="c38 c57 c23 c18">R Functions Used in This Chapter </span></h3><ul class="c32 lst-kix_estyhardezh6-0 start"><li class="c7 c13 li-bullet-0"><span class="c14 c17">Addr2latlng() </span><span class="c5">A custom function built for this chapter </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">addPoints() </span><span class="c5">Place more points on an existing plot </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">as.character() </span><span class="c5">Coerces data into a character string </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">as.EventData()</span><span class="c14">&nbsp;Coerce a regular dataframe into an </span><span class="c14 c17">EventData </span><span class="c14">object for use with </span><span class="c14 c17">PBSmapping </span><span class="c5">routines. </span></li></ul><h3 class="c94" id="h.w5hcn5ovosxt"><span class="c38 c57 c23 c18">Chapter Challenge(s) </span></h3><p class="c7"><span class="c14">Improve the </span><span class="c14 c17">Addr2latlng() </span><span class="c5">function so that it checks for errors returned from the Google API. This will require going to the Google API site, looking more closely at the JSON structure, and reviewing the error codes that Google mentions in the documentation. You may also learn enough that you can repair some of the addresses that failed. </span></p><h3 class="c94" id="h.y0vit5t7zzh4"><span>R Functions Used in This Chapter </span></h3><p class="c19 c52"><span class="c24 c23 c18"></span></p><ul class="c32 lst-kix_9go5hcewotk9-0 start"><li class="c0 li-bullet-0"><span class="c14 c17">data.frame() </span><span class="c5">Takes individual variables and ties them together </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">for() </span><span class="c5">Runs a loop, iterating a certain number of times depending upon the expression provided in parentheses </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">fromJSON() </span><span class="c5">Takes JSON data as input and provides a regular R dataframe as output </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">function() </span><span class="c5">Creates a new function </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">importShapeFile() </span><span class="c5">Gets shapefile data from a set of ESRI compatible polygon data files </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">MakeGeoURL() </span><span class="c5">Custom helper function built for this chapter </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">paste() </span><span class="c5">Glues strings together </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">plotPolys() </span><span class="c5">Plots a map from shape data </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">rbind() </span><span class="c5">Binds new rows on a dataframe </span></li><li class="c29 c13 li-bullet-0"><span class="c14 c17">return() </span><span class="c5">Specifies the object that should be returned from a function </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">URLencode() </span><span class="c5">Formats a character string so it can be used in an HTTP request </span></li></ul><h3 class="c39" id="h.6sspvn7v9swq"><span class="c38 c57 c23 c18"><br>Sources </span></h3><ul class="c32 lst-kix_wjprrrk3451p-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://blog.programmableweb.com/2012/06/21/7-free-geocodin&amp;sa=D&amp;source=editors&amp;ust=1751550345450617&amp;usg=AOvVaw0ANvOUG6umY6-H_NapLgYf">http://blog.programmableweb.com/2012/06/21/7-free-geocodin g-apis-google-bing-yahoo-and-mapquest/ </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://developers.google.com/maps/terms&amp;sa=D&amp;source=editors&amp;ust=1751550345450816&amp;usg=AOvVaw34UfF1Zeowasd-XQcdpBrp">https://developers.google.com/maps/terms </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Open_Mashup_Alliance&amp;sa=D&amp;source=editors&amp;ust=1751550345450985&amp;usg=AOvVaw1WwdVoqvu2g9owEDBRg0pR">http://en.wikipedia.org/wiki/Open_Mashup_Alliance</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Shapefile&amp;sa=D&amp;source=editors&amp;ust=1751550345451132&amp;usg=AOvVaw3yKwtmn0DdKF8MnqgIJnn7">http://en.wikipedia.org/wiki/Shapefile </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.housingmaps.com/&amp;sa=D&amp;source=editors&amp;ust=1751550345451273&amp;usg=AOvVaw0l8XxmngcK2lGoM0kadJ0d">http://www.housingmaps.com/ </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.census.gov/geo/www/cob/cbf_state.html&amp;sa=D&amp;source=editors&amp;ust=1751550345451434&amp;usg=AOvVaw3ssI8-aTx37ueXaR9zT_U0">http://www.census.gov/geo/www/cob/cbf_state.html </a></span></li></ul><p class="c165"><span class="c24 c23 c18">&nbsp;</span></p><p class="c81 c62 title" id="h.89epocsv5gfw"><span class="c70">CHAPTER 16 </span><span>&nbsp;<br>Line Up, Please </span><span class="c59 c57 c40 c23 c171">heepdog demonstration, Lone Pine Sanctuary, Brisbane, QLD. Photo credit: Jeff Stanton </span></p><p class="c128"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image44.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Sheepdog demonstration, Lone Pine Sanctuary, Brisbane, QLD. Photo credit: Jeff Stanton"></span><span class="c36 c37"><br></span><span class="c48 c36 c68">Sheepdog demonstration, Lone Pine Sanctuary, Brisbane, QLD. Photo credit: Jeff Stanton</span><span class="c37 c36"><br></span><span class="c44 c23 c37 c36">Data users are often interested in questions about relationships and prediction. For example, those interested in athletics might want to know how the size of the fanbase of a team is connected with attendance on game day. In this chapter, our Australian colleague, Robert de Graaf, introduces the techniques of linear regression, a very important data science tool. </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c59 c14 c57 c23">Using R to Find Relationships between Sets of Data via Multiple Regression, by Robert W. de Graaf </span></p><p class="c7"><span class="c5">Finding relationships between sets of data is one of the key aims of data science. The question of &lsquo;does x influence y&rsquo; is of prime concern for data analysts &ndash; are house prices influenced by incomes, is the growth rate of crops improved by fertilizer, do taller sprinters run faster? </span></p><p class="c7"><span class="c5">The workhorse method used by statisticians to interpret data is linear modeling, which is a term covering a wide variety of methods, from the relatively simple to very sophisticated. You can get an idea of how many different methods there are by looking at the Regression Analysis page in Wikipedia and checking out the number of entries listed under &lsquo;Models&rsquo; on the right hand sidebar (and, by the way, the list is not exhaustive). </span></p><p class="c7"><span class="c14">The basis of all these methods is the idea that it is possible to fit a line to a set of data points which represents the effect an &quot;independent&quot; variable is having on a &quot;dependent&quot; variable. It is easy to visualize how this works with one variable changing in step with another variable. Figure one shows a line fitted to a series of points, using the so called &quot;least squares&quot; method (a relatively simple mathematical method of finding a best fitting line). Note that although the line fits the points fairly well, with an even split (as even as it can be for five points!) of points on either side of the line, none of the points are precisely on the line &ndash; the data do not fit the line precisely. As we discuss the concepts in regression analysis further, we will see that understanding these discrepancies is just as important as understanding the line itself. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 594.67px;"><img alt="" src="images/image66.png" style="width: 624.00px; height: 594.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Figure: A line fitted to some points "></span></p><p class="c19 c111 c110"><span class="c5">Figure: A line fitted to some points </span></p><p class="c170"><span class="c5">The graph in the figure above shows how the relationship between an input variable &ndash; on the horizontal x axis &ndash; relates to the output values on the y axis. </span></p><p class="c7"><span class="c5">The original ideas behind linear regression were developed by some of the usual suspects behind many of the ideas we&rsquo;ve seen already, such as Laplace, Gauss, Galton, and Pearson. The biggest individual contribution was probably by Gauss, who used the procedure to predict movements of the other planets in the solar system when they were hidden from view, and hence correctly predict when and where they would appear in view again. </span></p><p class="c7"><span class="c5">The mathematical idea that allows us to fit lines of best fit to a set of data points like this is that we can find a position for the line that will minimize the distance the line is from all the points. While the mathematics behind these techniques can be handled by someone with college freshman mathematics the reality is that with even only a few data points, the process of fitting with manual calculations becomes very tedious, very quickly. For this reason, we will not discuss the specifics of how these calculations are done, but move quickly to how it can be done for us, using R. </span></p><p class="c7"><span class="c59 c14 c57 c23">Football or Rugby? </span></p><p class="c7"><span class="c5">We can use an example to show how to use linear regression on real data. The example concerns attendances at Australian Rules Football matches. For all of you in the U.S., Australian Rules Football is closer to what you think of as rugby and not so much like American football. The data in this example concerns matches/ games at the largest stadium for the sport, the Melbourne Cricket Ground (note: Australian Rules football is a winter sport, cricket is a summer sport). The Melbourne Cricket Ground or MCG is also considered the most prestigious ground to play a football match, due to its long association with Australian Rules Football. </span></p><p class="c29"><span class="c5">Australian Rules Football is the most popular sport to have been developed in Australia. The object is to kick the ball through the larger goal posts, scoring six points. If the ball is touched before it crosses the line under the large posts, or instead passes through the </span></p><p class="c19"><span class="c5">smaller posts on either side of the large goal posts, a single point is scored. The rules ensure that possession of the ball is always changing. There is full body tackling and possession is turned over frequently. This leads to continuous and exciting on-field action. </span></p><p class="c7"><span class="c5">The main stronghold of Australian Rules Football is in the state of Victoria (south eastern Australia), where the original league, the VFL, became the AFL after its league of mostly Melbourne suburban teams added teams to represent areas outside Victoria, such as West Coast (Perth, the capital of the state of Western Australia) and Adelaide (capital of the state of South Australia). Note that Melbourne is the capital city of Victoria. Much of the popularity of the VFL was based on the rivalries between neighboring suburbs, and teams with long histories, like the Collingwood Football Club based in one of Melbourne&rsquo;s most working class suburbs &ndash; have large and loyal fan bases. </span></p><p class="c7"><span class="c5">While it isn&rsquo;t necessary to know anything about how the sport is played to understand the example, it is useful to know that the Australian Football League, the premiere organization playing Australian Rules Football, was formerly the Victorian Football League, and although teams from outside the Australian state of Victoria have joined, more than half the teams in the league are Victorian, even though Victoria is only one of six Australian states. </span></p><p class="c7"><span class="c59 c14 c57 c23">Getting the Data </span></p><p class="c7"><span class="c14">The data are available from OzDasl, a website which provides public domain data sets for analysis, and the MCG attendance data has its own page at </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.statsci.org/data/oz/afl.html&amp;sa=D&amp;source=editors&amp;ust=1751550345458038&amp;usg=AOvVaw0_W7NOXv960PBQVmY22Q9F">http://www.statsci.org/data/oz/afl.html</a></span><span class="c5">. </span></p><p class="c7"><span class="c5">The variable of interest is MCG attendance, and named &lsquo;MCG&rsquo; in the dataset. Most statisticians would refer to this variable as the dependent variable, because it is the variable that we are most interested in predicting: It is the &quot;outcome&quot; of the situation we are trying to understand. Potential explanatory, or independent, variables include club membership, weather on match day, date of match, etc. There is a detailed description of each of the variables available on the website. You can use the data set to test your own theories of what makes football fans decide whether or not to go to a game, but to learn some of the skills we will test a couple of those factors together. </span></p><p class="c7"><span class="c5">Before we can start, we need R to be able to find the data. Make sure that you download the data to the spot on your computer that R considers the &quot;working&quot; directory. Use this command to find out what the current working directory is: </span></p><p class="c2"><span class="c5">&gt; getwd() </span></p><p class="c29"><span class="c5">After downloading the data set from OzDasl into your R working directory, read the data set into R as follows: </span></p><p class="c2"><span class="c5">&gt; attend&lt;-read.delim(&quot;afl.txt&quot;, header=TRUE) </span></p><p class="c10"><span class="c5">&gt; attach(attend) </span></p><p class="c90"><span class="c5">We include the optional &lsquo;header = TRUE&rsquo; to designate the first row as the column names, and the &lsquo;attach&rsquo; commands turns each of the named columns into a single column vector. </span></p><p class="c7"><span class="c5">Once we&rsquo;ve read the data into R, we can examine some plots of the data. With many techniques in data science, it can be quite valuable to visualize the data before undertaking a more detailed analysis. One of the variables we might consider is the combined mem</span></p><p class="c19"><span class="c5">bership of the two teams playing, a proxy for the popularity of the teams playing. </span></p><p class="c2"><span class="c5">&gt; plot(MCG ~ Members, xlab = &quot;Combined membership of teams playing&quot;, ylab = &quot;MCG Match Day Attendance&quot; ) </span></p><p class="c102"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 521.00px; height: 523.00px;"><img alt="" src="images/image63.png" style="width: 521.00px; height: 523.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Figure: Scatterplot of membership versus attendance "></span></p><p class="c158"><span class="c59 c14 c57 c23">Figure: Scatterplot of membership versus attendance </span></p><p class="c158"><span class="c5">We see evidence of a trend in the points on the left hand side of the graph, and a small group of points representing games with very high combined membership but that don&rsquo;t seem to fit the trend applying to the rest of data. If it wasn&rsquo;t for the four &quot;outliers&quot; on the right hand side of the plot, we would be left with a plot showing a very strong relationship. </span></p><p class="c7"><span class="c5">As a next step we can use R to create a linear model for MCG attendance using the combined membership as the single explanatory variable using the following R code: </span></p><p class="c2"><span class="c5">&gt; model1 &lt;lm(MCG ~ Members-1) </span></p><p class="c10"><span class="c5">&gt; summary(model1) </span></p><p class="c29"><span class="c5">There are two steps here because in the first step the lm() command creates a nice big data structure full of output, and we want to hang onto that in the variable called &quot;model1.&quot; In the second command we request an overview of the contents of model1. </span></p><p class="c7"><span class="c5">It is important to note that in this model, and in the others that follow, we have added a &lsquo;-1&rsquo; term to the specification, which forces the line of best fit to pass through zero on the y axis at zero on the x axis (more technically speaking, the y-intercept is forced to be at the origin). In the present model that is essentially saying that if the two teams are so unpopular they don&rsquo;t have any members, no one will go to see their matches, and vice versa. This technique is appropriate in this particular example, because both of the measures have sensible zero points, and we can logically reason that zero on X implies zero on Y. In most other models, particularly for survey data that may not have a sensible zero point (think about rating scales ranging from 1 to 5), it would not be appropriate to force the best fitting line through the origin. </span></p><p class="c19"><span class="c5">The second command above, summary(model1), provides the following output: </span></p><p class="c2"><span class="c5">Call: </span></p><p class="c10"><span class="c5">lm(formula = MCG ~ Members 1) </span></p><p class="c100"><span class="c5">Residuals: </span></p><p class="c10"><span class="c5">Min 1Q Median 3Q Max </span></p><p class="c2"><span class="c5">-44.961 -6.550 2.954 9.931 29.252 </span></p><p class="c100"><span class="c5">Coefficients: </span></p><p class="c10"><span class="c5">Estimate Std. Error t value Pr(&gt;|t|) </span></p><p class="c10"><span class="c5">Members 1.53610 0.08768 17.52 &lt;2e-16 *** </span></p><p class="c10"><span class="c5">--</span></p><p class="c10"><span class="c5">Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1 </span></p><p class="c100"><span class="c5">Residual standard error: 15.65 on 40 degrees of freedom </span></p><p class="c10"><span class="c5">Multiple R-squared: 0.8847, </span></p><p class="c2"><span class="c5">Adjusted R-squared: 0.8818 </span></p><p class="c10"><span class="c5">F-statistic: 306.9 on 1 and 40 DF, p-value: &lt; 2.2e-16 </span></p><p class="c10"><span class="c5">Wow! That&rsquo;s a lot of information that R has provided for us, and we need to use this information to decide whether we are happy with this model. Being &lsquo;happy&rsquo; with the model can involve many factors, and there is no simple way of deciding. To start with, we will look at the r-squared value, also known as the coefficient of determination. </span></p><p class="c7"><span class="c5">The r squared value &ndash; the coefficient of determination &ndash; represents the proportion of the variation which is accounted for in the dependent variable by the whole set of independent variables (in this case just one independent variable). An r-squared value of 1.0 would mean that the X variable(s), the independent variable(s), perfectly predicted the y, or dependent variable. An r-squared value of zero would indicated that the x variable(s) did not predict the y variable at all. R-squared cannot be negative. The r-squared of .8847 in this example means that the Combined Members variable account for 88.47% of the MCG attendance variable, an excellent result. Note that there is no absolute rule for what makes an rsquared good. Much depends on the purpose of the analysis. In the analysis of human behavior, which is notoriously unpredictable, an r-squared of .20 or .30 may be very good. </span></p><p class="c7"><span class="c5">In the figure below, we have added a line of best fit based on the model to the x-y plot of MCG attendance against total team membership with this command: </span></p><p class="c10"><span class="c1">&gt; abline(model1) </span></p><p class="c7"><span class="c14">While the line of best fit seems to fit the points in the middle, the points on the lower right hand side and also some points towards the top of the graph, appear to be a long way from the line of best fit. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 622.67px;"><img alt="" src="images/image47.png" style="width: 624.00px; height: 622.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="added a line of best fit based on the model to the x-y plot of MCG attendance against total team membership"></span></p><p class="c19"><span class="c59 c14 c57 c23">Adding Another Independent Variable </span></p><p class="c29"><span class="c5">We discussed at the beginning of this chapter the origin of Australian Rules Football in Victoria, where the MCG is located. While most of the teams in the AFL are also Victoria teams, and therefore have a supporter base which can easily access the MCG, a number of the teams are from other states, and their supporters would need to make a significant interstate journey to see their team play at the MCG. For example, the journey from Sydney to Melbourne is around eight hours by car or two by plane, whereas from Perth, where most West Coast&rsquo;s supporter base is located, is close to five hours by air &ndash; and two time zones away. Australia is a really huge country. </span></p><p class="c43"><span class="c5">The dataset doesn&rsquo;t have a variable for interstate teams but fortunately there are only four teams that are interstate: Brisbane, Sydney, Adelaide, and West Coast, abbreviated respectively as &quot;Bris&quot;, &quot;Syd&quot;, &quot;Adel&quot;, and &quot;WC&quot;. We can make a binary coded variable to indicate these interstate teams with a simple command: </span></p><p class="c10"><span class="c5">&gt; away.inter &lt;ifelse(Away==&quot;WC&quot; | </span></p><p class="c56"><span class="c5">Away==&quot;Adel&quot;| Away==&quot;Syd&quot;| Away==&quot;Bris&quot;,1,0) </span></p><p class="c7"><span class="c5">The code above checks the values in the column labeled &lsquo;Away&rsquo;, and if it finds an exact match with one of the names of an interstate team, it stores a value of 1. Otherwise it stores a value of 0. Note that we use a double equals sign for the exact comparison in R, and the vertical bar is used to represent the logical &lsquo;OR&rsquo; operator. These symbols are similar, although not precisely the same, as symbols used to represent logical operators in programming languages such as C and Java. Having created the new &lsquo;Away team is interstate&rsquo; variable, we can use this variable to create a new linear regression model that includes two independent variables. </span></p><p class="c2"><span class="c1">&gt; model2&lt;-lm(MCG~Members+away.inter-1) </span></p><p class="c19"><span class="c1">&gt; summary(model2) </span></p><p class="c100"><span class="c1">Call: </span></p><p class="c2"><span class="c1">lm(formula = MCG ~ Members + away.inter 1) </span></p><p class="c100"><span class="c1">Residuals: </span></p><p class="c10"><span class="c1">Min 1Q Median 3Q Max </span></p><p class="c10"><span class="c1">-30.2003 -8.5061 0.0862 8.5411 23.5687 </span></p><p class="c10"><span class="c1">Coefficients: </span></p><p class="c10"><span class="c1">Estimate Std. Error t value Pr(&gt;|t|) </span></p><p class="c10"><span class="c1">Members 1.69255 0.07962 21.257 &lt; 2e-16 *** </span></p><p class="c10"><span class="c1">away.inter -22.84122 5.02583 -4.545 5.2e-05 *** </span></p><p class="c10"><span class="c1">--</span></p><p class="c10"><span class="c1">Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1 </span></p><p class="c74"><span class="c1">Residual standard error: 12.82 on 39 degrees of freedom </span></p><p class="c10"><span class="c1">Multiple R-squared: 0.9246, Adjusted R-squared: 0.9208 </span></p><p class="c10"><span class="c1">F-statistic: 239.2 on 2 and 39 DF, p-value: &lt; 2.2e-16 </span></p><p class="c10"><span class="c5">Note that the r-squared value is now 0.9246, which is quite a bit higher than the 0.8847 that we observed in the previous model. In this new model, the two independent variables working together account for 92.46% of the dependent variable. So together, the total fan base and the status as an away team are doing a really great job of predicting attendance. This result is also intuitive &ndash; we would expect that football fans, regardless of how devoted they are to their team, are more likely to come to games if they&rsquo;re a moderate car ride away, compared to a plane journey. </span></p><p class="c7"><span class="c5">Because we have two independent variables now, we have to look beyond the r-squared value to understand the situation better. In particular, about one third of the way into the output for the lm() command there is a heading that says &quot;Estimate.&quot; Right below that are slope values for Members and for away.inter. Notice that the slope (sometimes called a &quot;B-weight&quot;) on Members is positive: This makes sense because the more fans the team has the higher the attendance. The slope on away.inter is negative because when this variable is 1 (in the case of interstate teams) the attendance is lower) whereas when this variable is 0 (for local teams), attendance is higher. </span></p><p class="c7"><span class="c5">How can you tell if these slopes or B-weights are actually important contributors to the prediction? You can divide the unstandardized B-weight by its standard error to create a &quot;t value&quot;. The lm() command has done this for you and it is reported in the output above. This &quot;t&quot; is the Student&rsquo;s t-test, described in a previous chapter. As a rule of thumb, if this t value has an absolute value (i.e., ignoring the minus sign if there is one) that is larger than about 2, you can be assured that the independent/predictor variable we are talking about is contributing to the prediction of the dependent </span></p><p class="c19"><span class="c14">variable. In this example we can see that </span><span class="c14 c17">Members </span><span class="c5">has a humongous t value of 21.257, showing that it is very important in the prediction. The away.inter variable has a somewhat more modest, but still important value of -4.545 (again, don&rsquo;t worry about the minus sign when judging the magnitude of the t value). </span></p><p class="c7"><span class="c5">We can keep on adding variables that we think make a difference. How many variables we end up using depends, apart from our ability to think of new variables to measure, somewhat on what we want to use the model for. </span></p><p class="c7"><span class="c5">The model we have developed now has two explanatory variables &ndash; one which can be any positive number, and one which is two levels. We now have what could be considered a very respectable r-squared value, so we could easily leave well enough alone. That is not to say our model is perfect, however &ndash; the graphs we have prepared suggest that the &lsquo;Members&rsquo; effect is actually different if the away team is from interstate rather than from Victoria &ndash; the crowd does not increase with additional combined membership as quickly with an away team, which is in line with what we might expect intuitively. </span></p><p class="c7"><span class="c14">One thing we didn&rsquo;t mention was the actual prediction equation that one might construct from the output of</span><span class="c14 c17">&nbsp;lm()</span><span class="c5">. It is actually very simple and just uses the estimates/B-weights from the output: </span></p><p class="c7"><span class="c1">MCG = (21.257 * Members) ( 4.545 * away.inter) </span></p><p class="c7"><span class="c14">This equation would let us predict the attendance of any game with a good degree of accuracy, assuming that we knew the combined fan base and whether the team was interstate. Interestingly, statisticians are rarely interested in using prediction equations like the one above: They are generally more interested in just knowing that a predictor is important or unimportant. Also, one must be careful with using the </span><span class="c14 c17">slopes/B-weights</span><span class="c5">&nbsp;obtained from a linear regression of a single sample, because they are likely to change if another sample is analyzed just because of the forces of randomness. </span></p><p class="c7"><span class="c59 c14 c57 c23">Conclusion </span></p><p class="c7"><span class="c5">The material we have covered is really only a taste of multiple regression and linear modeling. On the one hand, there are a number of additional factors that may be considered before deciding on a final model. On the other hand, there are a great number of techniques that may be used in specialized circumstances. For example, in trying to model attendance at the MCG, we have seen that the standard model fits the data some of the time but not others, depending on the selection of the explanatory variables. </span></p><p class="c7"><span class="c5">In general, a simple model is a good model, and will keep us from thinking that we are better than we really are. However, there are times when we will want to find as many dependent variables as possible. Contrast the needs of a manager trying to forecast sales to set inventory with an engineer or scientist trying to select parameters for further experimentation. In the first case, the manager needs to avoid a falsely precise estimate which could lead her to be overconfident in the forecast, and either order too much stock or too little. The manager wants to be conservative about deciding that particular variables make a difference to prediction variable. On the other hand the experimenter wants to find as many variables as possible for future research, so is prepared to be optimistic about whether different parameters affect the variables of interest. <br></span></p><p class="c19"><span class="c59 c14 c57 c23">Chapter Challenge </span></p><p class="c7"><span class="c14">We intentionally ignored some of the output of these regression models, for the sake of simplicity. It would be quite valuable for you to understand those missing parts, however. In particular, we ignored the &quot;p-values&quot; associated with the t-tests on the </span><span class="c14 c17">slope/Bweight</span><span class="c5">&nbsp;estimates and we also ignored the overall F-statistic reported at the very bottom of the output. There are tons of great resources on the web for explaining what these are. </span></p><p class="c7"><span class="c5">For a super bonus, you could also investigate the meaning of the &quot;Adjusted&quot; r-squared that appears in the output. </span></p><h3 class="c94" id="h.aj422pb5u42q"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_l8vz6gu7x6pv-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Australian_rules_football&amp;sa=D&amp;source=editors&amp;ust=1751550345473832&amp;usg=AOvVaw2JzrnJKGs-fT3rYHUrIGiO">http://en.wikipedia.org/wiki/Australian_rules_football </a></span></li><li class="c29 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm.&amp;sa=D&amp;source=editors&amp;ust=1751550345474031&amp;usg=AOvVaw35I1OOX-Rmb1xHAA6fB5Lv">http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm. html </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.ddiez.com/teac/r/linear_models.php&amp;sa=D&amp;source=editors&amp;ust=1751550345474177&amp;usg=AOvVaw3FTXx-IYM_twMqpNw2szp_">http://www.ddiez.com/teac/r/linear_models.php </a></span></li></ul><h3 class="c94" id="h.x7d8ze71nayc"><span class="c38 c57 c23 c18">R Functions Used in This Chapter </span></h3><ul class="c32 lst-kix_aonucg4gowwq-0 start"><li class="c7 c13 li-bullet-0"><span class="c14 c17">abline() </span><span class="c5">plots a best fitting line on top of a scatterplot </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">attach() </span><span class="c5">makes a data structure the &quot;focus&quot; of attention </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">getwd() </span><span class="c5">show the current working directory for R </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">ifelse() </span><span class="c5">&nbsp;a conditional test that provides one of two possible outputs </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">lm() &nbsp;</span><span class="c5">&quot;linear models&quot; and for this chapter, multiple regression </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">plot() &nbsp;</span><span class="c5">general purpose graphing function, many uses in R summary produces an overview of an output structure</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c81 c62 c52 title" id="h.mxxsquaf4og"><span class="c5"></span></p><p class="c81 c62 title" id="h.kq6ln1akfh1i"><span class="c70">CHAPTER 17</span><span><br></span><span>Hi Ho, &nbsp;Hi Ho <br>Data Mining We Go<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 560.00px; height: 268.00px;"><img alt="" src="images/image54.png" style="width: 560.00px; height: 560.00px; margin-left: 0.00px; margin-top: -129.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="image of diaper products"></span><span><br></span><span class="c57 c23 c37 c36 c59">Data Mining is an area of research and practice that is focused on discovering novel patterns in data. As usual, R has lots of possibilities for data mining. In this chapter we will begin experimentation with essential data mining techniques by trying out one of the easiest methods to understand: association rules mining. More beer and diapers please! </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c5">Data mining is a term that refers to the use of algorithms and computers to discover novel and interesting patterns within data. One famous example that gets mentioned quite frequently is the supermarket that analyzed patterns of purchasing behavior and found that diapers and beer were often purchased together. The supermarket manager decided to put a beer display close to the diaper aisle and supposedly sold more of both products as a result. Another familiar example comes from online merchant sites that say things like, &quot;people who bought that book were also interested in this book.&quot; By using an algorithm to look at purchasing patterns, vendors are able to create automatic systems that make these kinds of recommendations. </span></p><p class="c29"><span class="c5">Over recent decades, statisticians and computer scientists have developed many different algorithms that can search for patterns in different kinds of data. As computers get faster and the researchers do additional work on making these algorithms more efficient it becomes possible to look through larger and larger data sets looking for promising patterns. Today we have software that can search through massive data haystacks looking for lots of interesting and usable needles. </span></p><p class="c7"><span class="c5">Some people refer to this area of research as machine learning. Machine learning focuses on creating computer algorithms that can use pre-existing inputs to refine and improve their own capabilities for dealing with future inputs. MAchine learning is very different from human learning. When we think of human learning, like learning the alphabet or learning a foreign language, humans can develop flexible and adaptable skills and knowledge that are applicable to a range of different contexts and problems. Machine learning is more about figuring out patterns of incoming information </span></p><p class="c19"><span class="c5">that correspond to a specific result. For example, given lots of examples like this input: 3, 5, 10; output: 150 a machine learning algorithm could figure out on its own that multiplying the input values together produces the output value. </span></p><p class="c7"><span class="c5">Machine learning is not exactly the same thing as data mining and vice versa. Not all data mining techniques rely on what researchers would consider machine learning. Likewise, machine learning is used in areas like robotics that we don&rsquo;t commonly think of when we are thinking of data mining as such. </span></p><p class="c7"><span class="c5">Data mining typically consists of four processes: 1) data preparation, 2) exploratory data analysis, 3) model development, and 4) interpretation of results. Although this sounds like a neat, linear set of steps, there is often a lot of back and forth through these processes, and especially among the first three. The other point that is interesting about these four steps is that Steps 3 and 4 seem like the most fun, but Step 1 usually takes the most amount of time. Step 1 involves making sure that the data are organized in the right way, that missing data fields are filled in, that inaccurate data are located and repaired or deleted, and that data are &quot;recoded&quot; as necessary to make them amenable to the kind of analysis we have in mind. </span></p><p class="c7"><span class="c5">Step 2 is very similar to activities we have done in prior chapters of this book: getting to know the data using histograms and other visualization tools, and looking for preliminary hints that will guide our model choice. The exploration process also involves figuring out the right values for key parameters. We will see some of that activity in this chapter. </span></p><p class="c7"><span class="c5">Step 3 choosing and developing a model is by far the most complex and most interesting of the activities of a data miner. It is here where you test out a selection of the most appropriate data mining techniques. Depending upon the structure of a dataset, there may be dozens of options, and choosing the most promising one has as much art in it as science. </span></p><p class="c7"><span class="c5">For the current chapter we are going to focus on just one data mining technique, albeit one that is quite powerful and applicable to a range of very practical problems. So we will not really have to do Step 3, because we will not have two or more different mining techniques to compare. The technique we will use in this chapter is called &quot;association rules mining&quot; and it is the strategy that was used to find the diapers and beer association described earlier. </span></p><p class="c7"><span class="c5">Step 4 the interpretation of results focuses on making sense out of what the data mining algorithm has produced. This is the most important step from the perspective of the data user, because this is where an actionable conclusion is formed. When we discussed the example of beer and diapers, the interpretation of the association rules that were derived from the grocery purchasing data is what led to the discover of the beer-diapers rule and the use of that rule in reconfiguring the displays in the store. </span></p><p class="c7"><span class="c5">Let&rsquo;s begin by talking a little bit about association rules. Take a look at the figure below with all of the boxes and arrows: </span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 496.00px; height: 477.00px;"><img alt="" src="images/image48.png" style="width: 496.00px; height: 477.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Map of customers to products"></span></p><p class="c109"><span class="c5">From the figure you can see that each supermarket customer has a grocery cart that contains several items from the larger set of items that the grocery store stocks. The association rules algorithm (also sometimes called affinity analysis) tries out many different propositions, such as &quot;if diapers are purchased, then beer is also purchased.&quot; The algorithm uses a dataset of transactions (in the example above, these are the individual carts) to evaluate a long list of these rules for a value called &quot;support.&quot; Support is the proportion of times that a particular pairing occurs across all shopping carts. The algorithm also evaluates another quantity called &quot;confidence,&quot; which is how frequently a particular pair occurs among all the times when the first item is present. If you look back at the figure again, we had support of 0.67 (the diapers-beer association occurred in two out of the three carts) and confidence of 1.0 (&quot;beer&quot; occurred 100% of the time with &quot;diapers&quot;). In practice, both support and confidence are generally much lower than in this example, but even a rule with low support and smallish confidence might reveal purchasing patterns that grocery store managers could use to guide pricing, coupon offers, or advertising strategies. </span></p><p class="c7"><span class="c5">We can get started with association rules mining very easily using the R package known as &quot;arules.&quot; In R-Studio, you can get the arules package ready using the following commands: </span></p><p class="c73"><span class="c44 c23 c18 c36">&gt; install.packages(&quot;arules&quot;) </span></p><p class="c10"><span class="c44 c23 c18 c36">&gt; library(&quot;arules&quot;) </span></p><p class="c7"><span class="c5">We will begin our exploration of association rules mining using a dataset that is built into the arules package. For the sake of familiarity, we will use the Groceries dataset. Note that by using the Groceries data set, we have relieved ourselves of the burden of data preparation, as the authors of the arules package have generously made sure that Groceries is ready to be analyzed. So we are skipping right to Step 2 in our four step process exploratory data analysis. You can make the Groceries data set ready with this command: </span></p><p class="c10"><span class="c44 c23 c18 c36">&gt; data(Groceries) </span></p><p class="c7"><span class="c5">Next, lets run the summary() function on Groceries so that we can see what is in there: </span></p><p class="c73"><span class="c44 c23 c18 c36">&gt; summary(Groceries) </span></p><p class="c19"><span class="c44 c23 c18 c36">transactions as itemMatrix in sparse format with </span></p><p class="c10"><span class="c44 c23 c18 c36">9835 rows (elements/itemsets/transactions) and </span></p><p class="c73"><span class="c44 c23 c18 c36">169 columns (items) and a density of 0.02609146 </span></p><p class="c10"><span class="c44 c23 c18 c36">most frequent items: </span></p><p class="c10"><span class="c44 c23 c18 c36">whole milk other vegetables rolls/buns soda </span></p><p class="c10"><span class="c44 c23 c18 c36">2513 1903 1809 1715 </span></p><p class="c73"><span class="c44 c23 c18 c36">yogurt (Other) </span></p><p class="c10"><span class="c44 c23 c18 c36">1372 34055 </span></p><p class="c131"><span class="c44 c23 c18 c36">element (itemset/transaction) length distribution: </span></p><p class="c10"><span class="c44 c23 c18 c36">sizes </span></p><p class="c10"><span class="c44 c23 c18 c36">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 </span></p><p class="c10"><span class="c44 c23 c18 c36">2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 46 </span></p><p class="c73"><span class="c44 c23 c18 c36">17 18 19 20 21 22 23 24 26 27 28 29 32 </span></p><p class="c10"><span class="c44 c23 c18 c36">29 14 14 9 11 4 6 1 1 1 1 3 1 </span></p><p class="c146"><span class="c44 c23 c18 c36">Min. 1st Qu. Median Mean 3rd Qu. Max. </span></p><p class="c73"><span class="c44 c23 c18 c36">1.000 2.000 3.000 4.409 6.000 32.000 includes extended item information examples: </span></p><p class="c73"><span class="c44 c23 c18 c36">labels level2 level1 </span></p><p class="c10"><span class="c44 c23 c18 c36">1 frankfurter sausage meet and sausage </span></p><p class="c10"><span class="c44 c23 c18 c36">2 sausage sausage meet and sausage </span></p><p class="c73"><span class="c44 c23 c18 c36">3 liver loaf sausage meet and sausage </span></p><p class="c7"><span class="c5">Right after the summary command line we see that Groceries is an itemMatrix object in sparse format. So what we have is a nice, rectangular data structure with 9835 rows and 169 columns, where each row is a list of items that might appear in a grocery cart. The word &quot;matrix&quot; in this case, is just referring to this rectangular data structure. The columns are the individual items. A little later in the output we see that there are 169 columns, which means that there are 169 items. The reason the matrix is called &quot;sparse&quot; is that very few of these items exist in any given grocery basket. By the way, when an item appears in a basket, its cell contains a one, while if an item is not in a basket, its cell contains a zero. So in any given row, most of the cells are zero and very few are one and this is what is meant by sparse. We can see from the Min, Median, Mean, and Max output that every cart has at least one item, half the carts have more than three items, the average number of items in a cart is 4.4 and the maximum number of items in a cart is 32. </span></p><p class="c7"><span class="c5">The output also shows us which items occur in grocery baskets most frequently. If you like working with spreadsheets, you could imagine going to the very bottom of the column that is marked &quot;whole milk&quot; and putting in a formula to sum up all of the ones in that column. You would come up with 2513, indicating that there </span></p><p class="c19"><span class="c5">are 2513 grocery baskets that contain whole milk. Remember that every row/basket that has a one in the whole milk column has whole milk in that basket, whereas a zero would appear if the basket did not contain whole milk. You might wonder what the data field would look like if a grocery cart contained two gallons of whole milk. For the present data mining exercise we can ignore that problem by assuming that any non-zero amount of whole milk is represented by a one. Other data mining techniques could take advantage of knowing the exact amount of a product, but association rules does not need to know that amount just whether the product is present or absent. </span></p><p class="c7"><span class="c14">Another way of inspecting our sparse matrix is with the itemFrequencyPlot() function. This produces a bar graph that is similar in concept to a histogram: it shows the relative frequency of occurrence of different items in the matrix. When using the </span><span class="c14 c17">itemFrequencyPlot() </span><span class="c14">function, you must specify the minimum level of &quot;support&quot; needed to include an item in the plot. Remember the mention of support earlier in the chapter in this case it simply refers to the relative frequency of occurrence of something. We can make a guess as to what level of support to choose based on the results of the </span><span class="c14 c17">summary() </span><span class="c5">function we ran earlier in the chapter. For example, the item &quot;yogurt&quot; appeared in 1372 out of 9835 rows or about 14% of cases. So we can set the support parameter to somewhere around 10%-15% in order to get a manageable number of items: </span></p><p class="c73"><span class="c8">&gt; itemFrequencyPlot(Groceries,support=0.1) </span></p><p class="c7"><span class="c14">This command produces the following plot:</span><span class="c18">&nbsp;<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 408.00px;"><img alt="" src="images/image17.png" style="width: 402.00px; height: 408.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="frequencies of purchases"></span></p><p class="c7"><span class="c5">We can see that yogurt is right around 14% as expected and we also see a few other items not mentioned in the summary such as bottled water and tropical fruit. </span></p><p class="c7"><span class="c5">You should experiment with using different levels of support, just so that you can get a sense of the other common items in the data set. If you show more than about ten items, you will find that the labels on the X-axis start to overlap and obscure one another. Use the &quot;cex.names&quot; parameter to turn down the font size on the labels. </span></p><p class="c19"><span class="c5">This will keep the labels from overlapping at the expense of making the font size much smaller. Here&rsquo;s an example: </span></p><p class="c19"><span class="c26 c17 c23">&gt; itemFrequencyPlot(Groceries,support=0.05,cex.names=0.5) </span></p><p class="c74"><span class="c5">This command yields about 25 items on the X-axis. Without worrying too much about the labels, you can also experiment with lower values of support, just to get a feel for how many items appear at the lower frequencies. We need to guess at a minimum level of support that will give us quite a substantial number of items that can potentially be part of a rule. Nonetheless, it should also be obvious that an item that occurs only very rarely in the grocery baskets is unlikely to be of much use to us in terms of creating meaningful rules. Let&rsquo;s pretend, for example, that the item &quot;Venezuelan Beaver Cheese&quot; only occurred once in the whole set of 9835 carts. Even if we did end up with a rule about this item, it won&rsquo;t apply very often, and is therefore unlikely to be useful to store managers or others. So we want to focus our attention on items that occur with some meaningful frequency in the dataset. Whether this is one percent or half a percent, or something somewhat larger or smaller will depend on the size of the data set and the intended application of the rules. </span></p><p class="c7"><span class="c14">Now we can prepare to generate some rules with the apriori() command. The term &quot;apriori&quot; refers to the specific algorithm that R will use to scan the data set for appropriate rules. Apriori is a very commonly used algorithm and it is quite efficient at finding rules in transaction data. Rules are in the form of &quot;if LHS then RHS.&quot; The acronym LHS means &quot;left hand side&quot; and, naturally, RHS means &quot;right hand side.&quot; So each rule states that when the thing or things on the left hand side of the equation occur(s), the thing on the right hand side occurs a certain percentage of the time. To reiterate a def</span><span class="c67 c36">i</span><span class="c5">nition provided earlier in the chapter, support for a rule refers to the frequency of co-occurrence of both members of the pair, i.e., LHS and RHS together. The confidence of a rule refers to the proportion of the time that LHS and RHS occur together versus the total number of appearances of LHS. For example, if Milk and Butter occur together in 10% of the grocery carts (that is &quot;support&quot;), and Milk (by itself, ignoring Butter) occurs in 25% of the carts, then the confidence of the Milk/Butter rule is 0.10/0.25 = 0.40. </span></p><p class="c7"><span class="c5">There are a couple of other measures that can help us zero in on good association rules such as &quot;lift&quot;and &quot;conviction&quot; but we will put off discussing these until a little later. </span></p><p class="c7"><span class="c5">One last note before we start using apriori(): For most of the work the data miners do with association rules, the RHS part of the equation contains just one item, like &quot;Butter.&quot; On the other hand, LHS can and will contain multiple items. A simple rule might just have Milk in LHS and Butter in RHS, but a more complex rule might have Milk and Bread together in LHS with Butter in RHS. </span></p><p class="c7"><span class="c5">In the spirit of experimentation, we can try out some different parameter values for using the apriori() command, just to see what we will get: </span></p><p class="c9"><span class="c12">&gt; apriori(Groceries,parameter=list(support=0.005,+ confidence=0.5)) </span></p><p class="c91"><span class="c12">parameter specification: </span></p><p class="c10"><span class="c12">confidence minval smax arem aval </span></p><p class="c10"><span class="c12">0.5 0.1 1 none FALSE </span></p><p class="c2"><span class="c12">originalSupport support minlen maxlen target </span></p><p class="c19"><span class="c12">TRUE 0.005 1 10 rules </span></p><p class="c10"><span class="c67 c36">ext </span><span class="c44 c118 c36">FALSE </span></p><p class="c91"><span class="c12">algorithmic control: </span></p><p class="c10"><span class="c12">filter tree heap memopt load sort verbose </span></p><p class="c10"><span class="c12">0.1 TRUE TRUE FALSE TRUE 2 TRUE </span></p><p class="c91"><span class="c12">apriori find association rules with the apriori algorithm </span></p><p class="c10"><span class="c12">version 4.21 (2004.05.09) (c) 1996-2004 Christian Borgelt </span></p><p class="c10"><span class="c12">set item appearances ...[0 item(s)] done [0.00s]. </span></p><p class="c10"><span class="c12">set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. </span></p><p class="c2"><span class="c12">sorting and recoding items ... [120 item(s)] done [0.00s]. </span></p><p class="c10"><span class="c12">creating transaction tree ... done [0.01s]. </span></p><p class="c10"><span class="c12">checking subsets of size 1 2 3 4 done [0.01s]. </span></p><p class="c2"><span class="c12">writing ... [120 rule(s)] done [0.00s]. </span></p><p class="c10"><span class="c12">creating S4 object ... done [0.00s]. </span></p><p class="c10"><span class="c12">set of 120 rules </span></p><p class="c161"><span class="c5">We set up the apriori() command to use a support of 0.005 (half a percent) and confidence of 0.5 (50%) as the minimums. These values are confirmed in the first few lines of output. Some other confirmations, such as the value of &quot;minval&quot; and &quot;smax&quot; are not relevant to us right now they have sensible defaults provided by the apriori() implementation. The &quot;minlen&quot; and &quot;maxlen&quot; parameters also have sensible defaults: these refer to the minimum and maximum length of item set that will be considered in generating rules. Obviously you can&rsquo;t generate a rule unless you have at least one item in an item set, and setting maxlen to 10 ensures that we will not have any rules that contain more than 10 items. If you recall from earlier in the chapter, the average cart only has 4.4 items, so we are not likely to produce rules involving more than 10 items. </span></p><p class="c7"><span class="c5">In fact, a little later in the apriori() output above, we see that the apriori() algorithm only had to examine &quot;subsets of size&quot; one, two three, and four. Apparently no rule in this output contains more than four items. At the very end of the output we see that 120 rules were generated. Later on we will examine ways of making sense out of a large number of rules, but for now let&rsquo;s agree that 120 is too many rules to examine. Let&rsquo;s move our support to one percent and rerun apriori(). This time we will store the resulting rules in a data structure called ruleset: </span></p><p class="c9"><span class="c12">&gt; ruleset &lt;apriori(Groceries,+ parameter=list(support=0.01,confidence=0.5)) </span></p><p class="c135"><span class="c5">If you examine the output from this command, you should find that we have slimmed down to 15 rules, quite a manageable number to examine one by one. We can get a preliminary look at the rules using the summary function, like this: </span></p><p class="c9"><span class="c12">&gt; summary(ruleset) </span></p><p class="c2"><span class="c12">set of 15 rules </span></p><p class="c19"><span class="c12">rule length distribution (lhs + rhs):sizes </span></p><p class="c10"><span class="c67 c36">3 </span><span class="c44 c118 c36">15 </span></p><p class="c89"><span class="c12">Min. 1st Qu. Median Mean 3rd Qu. Max. </span></p><p class="c10"><span class="c12">3 3 3 3 3 3 </span></p><p class="c91"><span class="c12">summary of quality measures: </span></p><p class="c10"><span class="c12">support confidence lift </span></p><p class="c10"><span class="c12">Min. :0.01007 Min. :0.5000 Min. :1.984 </span></p><p class="c2"><span class="c12">1st Qu.:0.01174 1st Qu.:0.5151 1st Qu.:2.036 </span></p><p class="c10"><span class="c12">Median :0.01230 Median :0.5245 Median :2.203 </span></p><p class="c10"><span class="c12">Mean :0.01316 Mean :0.5411 Mean :2.299 </span></p><p class="c10"><span class="c12">3rd Qu.:0.01403 3rd Qu.:0.5718 3rd Qu.:2.432 </span></p><p class="c150"><span class="c12">mining info: </span></p><p class="c10"><span class="c12">data transactions support confidence </span></p><p class="c2"><span class="c12">Groceries 9835 0.01 0.5 </span></p><p class="c19"><span class="c12">Max. :0.02227 Max. :0.5862 Max. :3.030 </span></p><p class="c86"><span class="c5">Looking through this output, we can see that there are 15 rules in total. Under &quot;rule length distribution&quot; it shows that all 15 of the rules have exactly three elements (counting both the LHS and the RHS). Then, under &quot;summary of quality measures,&quot; we have an overview of the distributions of support, confidence, and a new parameter called &quot;lift.&quot; </span></p><p class="c7"><span class="c5">Researchers have done a lot of work trying to come up with ways of measuring how &quot;interesting&quot; a rule is. A more interesting rule may be a more useful rule because it is more novel or unexpected. Lift is one such measure. Without getting into the math, lift takes into account the support for a rule, but also gives more weight to rules where the LHS and/or the RHS occur less frequently. In other words, lift favors situations where LHS and RHS are not abundant but where the relatively few occurrences always happen together. The larger the value of lift, the more &quot;interesting&quot; the rule may be. </span></p><p class="c29"><span class="c5">Now we are ready to take a closer look at the rules we generated. The inspect() command gives us the detailed contents of the dta object generated by apriori(): </span></p><p class="c9"><span class="c44 c23 c36 c49">&gt; inspect(ruleset) </span></p><p class="c22"><span class="c44 c23 c36 c49">lhs rhs support confidence lift </span></p><p class="c22"><span class="c44 c23 c36 c49">1 {curd, </span></p><p class="c22"><span class="c44 c23 c36 c49">yogurt} =&gt; {whole milk} 0.01006609 0.5823529 2.279125 </span></p><p class="c22"><span class="c44 c23 c36 c49">2 {other vegetables, </span></p><p class="c28"><span class="c44 c23 c36 c49">butter} =&gt; {whole milk} 0.01148958 0.5736041 2.244885 </span></p><p class="c22"><span class="c44 c23 c36 c49">3 {other vegetables, </span></p><p class="c22"><span class="c44 c23 c36 c49">domestic eggs} =&gt; {whole milk} 0.01230300 0.5525114 2.162336 </span></p><p class="c22"><span class="c44 c23 c36 c49">4 {yogurt, </span></p><p class="c22"><span class="c44 c23 c36 c49">whipped/sour cream} =&gt; {whole milk} 0.01087951 0.5245098 2.052747 </span></p><p class="c22"><span class="c44 c23 c36 c49">5 {other vegetables, </span></p><p class="c28"><span class="c44 c23 c36 c49">whipped/sour cream} =&gt; {whole milk} 0.01464159 0.5070423 1.984385 </span></p><p class="c22"><span class="c44 c23 c36 c49">6 {pip fruit, </span></p><p class="c19"><span class="c44 c23 c36 c49">other vegetables} =&gt; {whole milk} 0.01352313 0.5175097 2.025351 </span></p><p class="c22"><span class="c44 c23 c36 c49">7 {citrus fruit, </span></p><p class="c22"><span class="c44 c23 c36 c49">root vegetables} =&gt; {other vegetables} 0.01037112 0.5862069 3.029608 </span></p><p class="c22"><span class="c44 c23 c36 c49">8 {tropical fruit, </span></p><p class="c28"><span class="c44 c23 c36 c49">root vegetables} =&gt; {other vegetables} 0.01230300 0.5845411 3.020999 </span></p><p class="c22"><span class="c44 c23 c36 c49">9 {tropical fruit, </span></p><p class="c22"><span class="c44 c23 c36 c49">root vegetables} =&gt; {whole milk} 0.01199797 0.5700483 2.230969 </span></p><p class="c22"><span class="c44 c23 c36 c49">10 {tropical fruit, </span></p><p class="c22"><span class="c44 c23 c36 c49">yogurt} =&gt; {whole milk} 0.01514997 0.5173611 2.024770 </span></p><p class="c22"><span class="c44 c23 c36 c49">11 {root vegetables, </span></p><p class="c28"><span class="c44 c23 c36 c49">yogurt} =&gt; {other vegetables} 0.01291307 0.5000000 2.584078 </span></p><p class="c22"><span class="c44 c23 c36 c49">12 {root vegetables, </span></p><p class="c22"><span class="c44 c23 c36 c49">yogurt} =&gt; {whole milk} 0.01453991 0.5629921 2.203354 </span></p><p class="c22"><span class="c44 c23 c36 c49">13 {root vegetables, </span></p><p class="c22"><span class="c44 c23 c36 c49">rolls/buns} =&gt; {other vegetables} 0.01220132 0.5020921 2.594890 </span></p><p class="c22"><span class="c44 c23 c36 c49">14 {root vegetables, </span></p><p class="c28"><span class="c44 c23 c36 c49">rolls/buns} =&gt; {whole milk} 0.01270971 0.5230126 2.046888 </span></p><p class="c22"><span class="c44 c23 c36 c49">15 {other vegetables, </span></p><p class="c22"><span class="c44 c23 c36 c49">yogurt} =&gt; {whole milk} 0.02226741 0.5128806 2.007235 </span></p><p class="c2"><span class="c5">With apologies for the tiny font size, you can see that each of the 15 rules shows the LHS, the RHS, the support, the confidence, and the lift. Rules 7 and 8 have the highest level of lift: the fruits and vegetables involved in these two rules have a relatively low frequency of occurrence, but their support and confidence are both relatively high. Contrast these two rules with Rule 1, which also has high confidence, but which has low support. The reason for this is that milk is a frequently occurring item, so there is not much novelty to that rule. On the other hand, the combination of fruits, root vegetables, and other vegetables suggest a need to find out more about customers whose carts may contain only vegetarian or vegan items. </span></p><p class="c7"><span class="c5">Now it is possible that we have set our parameters for confidence and support too stringently and as a result have missed some truly novel combinations that might lead us to better insights. We can use a data visualization package to help explore this possibility. The R package called arulesViz has methods of visualizing the rule sets generated by apriori() that can help us examine a larger set of rules. First, install and library the arulesViz package: </span></p><p class="c73"><span class="c44 c23 c18 c36">&gt; install.packages(&quot;arulesViz&quot;) </span></p><p class="c10"><span class="c44 c23 c18 c36">&gt; library(arulesViz) </span></p><p class="c7"><span class="c5">These commands will give the usual raft of status and progress messages. When you run the second command you may find that three or four data objects are &quot;masked.&quot; As before, these warnings generally will not compromise the operation of the package. </span></p><p class="c7"><span class="c5">Now lets return to our apriori() command, but we will be much more lenient this time in our minimum support and confidence parameters: </span></p><p class="c73"><span class="c44 c23 c18 c36">&gt; ruleset &lt;apriori(Groceries,+ parameter=list(support=0.005,confidence=0.35)) </span></p><p class="c7"><span class="c5">We brought support back to half of one percent and confidence down to 35%. When you run this command you should find that you now generate 357 rules. That is way too many rules to examine manually, so let&rsquo;s use the arulesViz package to see what we have. We will use the plot() command that we have also used earlier in the book. You may ask yourself why we needed to library the arulesViz package if we are simply going to use an old command. The answer to this conundrum is that arulesViz has put some plumbing into place so that when plot runs across a data object of type &quot;rules&quot; (as generated by apriori) it will use some of the code that is built into arulesViz to do the work. So by installing arulesViz we have put some custom visualization code in place that can be used by the generic plot() command. The command is very simple: </span></p><p class="c73"><span class="c8">&gt; plot(ruleset) </span></p><p class="c86"><span class="c5">The figure below contains the result:</span></p><p class="c86"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 408.00px;"><img alt="" src="images/image18.png" style="width: 402.00px; height: 408.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Scatterplot for 357 rules"></span></p><p class="c19"><span class="c5">Even though we see a two dimensional plot, we actually have three variables represented here. Support is on the X-axis and confidence is on the Y-axis. All else being equal we would like to have rules that have high support and high confidence. We know, however, that lift serves as a measure of interestingness, and we are also interested in the rules with the highest lift. On this plot, the lift is shown by the darkness of a dot that appears on the plot. The darker the dot, the close the lift of that rule is to 4.0, which appears to be the highest lift value among these 357 rules. </span></p><p class="c7"><span class="c5">The other thing we can see from this plot is that while the support of rules ranges from somewhere below 1% all the way up above 7%, all of the rules with high lift seem to have support below 1%. On the other hand, there are rules with high lift and high confidence, which sounds quite positive. </span></p><p class="c7"><span class="c5">Based on this evidence, lets focus on a smaller set of rules that only have the very highest levels of lift. The following command makes a subset of the larger set of rules by choosing only those rules that have lift higher than 3.5: </span></p><p class="c10"><span class="c23 c18 c36 c44">&gt; goodrules &lt;ruleset[quality(ruleset)$lift &gt; 3.5] </span></p><p class="c7"><span class="c5">Note that the use of the square braces with our data structure ruleset allows us to index only those elements of the data object that meet our criteria. In this case, we use the expression quality(ruleset)$lift to tap into the lift parameter for each rule. The inequality test &gt; 3.5 gives us just those rules with the highest lift. When you run this line of code you should find that goodrules contains just nine rules. Let&rsquo;s inspect those nine rules: </span></p><p class="c10"><span class="c44 c23 c18 c36">&gt; inspect(goodrules) </span></p><p class="c19"><span class="c34 c23">lhs rhs support confidence lift </span></p><p class="c22"><span class="c34 c23">1 {herbs} =&gt; {root vegetables} 0.007015760 0.4312500 3.956477 </span></p><p class="c82"><span class="c34 c23">2 {onions, </span></p><p class="c22"><span class="c34 c23">other vegetables} =&gt; {root vegetables} 0.005693950 0.4000000 3.669776 </span></p><p class="c22"><span class="c34 c23">3 {beef, </span></p><p class="c82"><span class="c34 c23">other vegetables} =&gt; {root vegetables} 0.007930859 0.4020619 3.688692 </span></p><p class="c22"><span class="c34 c23">4 {tropical fruit, </span></p><p class="c82"><span class="c23 c34">curd} =&gt; {yogurt} 0.005287239 0.5148515 3.690645 </span></p><p class="c22"><span class="c34 c23">5 {citrus fruit, </span></p><p class="c82"><span class="c34 c23">pip fruit} =&gt; {tropical fruit} 0.005592272 0.4044118 3.854060 </span></p><p class="c22"><span class="c34 c23">6 {pip fruit, </span></p><p class="c22"><span class="c34 c23">other vegetables, </span></p><p class="c82"><span class="c34 c23">whole milk} =&gt; {root vegetables} 0.005490595 0.4060150 3.724961 </span></p><p class="c22"><span class="c34 c23">7 {citrus fruit, </span></p><p class="c82"><span class="c34 c23">other vegetables, </span></p><p class="c22"><span class="c34 c23">whole milk} =&gt; {root vegetables} 0.005795628 0.4453125 4.085493 </span></p><p class="c22"><span class="c34 c23">8 {root vegetables, </span></p><p class="c82"><span class="c34 c23">whole milk, </span></p><p class="c22"><span class="c34 c23">yogurt} =&gt; {tropical fruit} 0.005693950 0.3916084 3.732043 </span></p><p class="c82"><span class="c34 c23">9 {tropical fruit, </span></p><p class="c22"><span class="c34 c23">other vegetables, </span></p><p class="c82"><span class="c34 c23">whole milk} =&gt; {root vegetables} 0.007015760 0.4107143 3.768074 </span></p><p class="c73"><span class="c5">There we go again with the microscopic font size. When you look over these rules, it seems evidence that shoppers are purchasing particular combinations of items that go together in recipes. The first three rules really seem like soup! Rules four and five seem like a fruit platter with dip. The other four rules may also connect to a recipe, although it is not quite so obvious what. </span></p><p class="c7"><span class="c5">The key takeaway point here is that using a good visualization tool to examine the results of a data mining activity can enhance the process of sorting through the evidence and making sense of it. If we were to present these results to a store manager (and we would certainly do a little more digging before formulating our final conclusions) we might recommend that recipes could be published along with coupons and popular recipes, such as for homemade soup, might want to have all of the ingredients group together in the store along with signs saying, &quot;Mmmm, homemade soup!&quot; </span></p><p class="c7"><span class="c59 c14 c57 c23">Chapter Challenge </span></p><p class="c29"><span class="c14">The </span><span class="c14 c17">arules </span><span class="c14">package contains other data sets, such as the </span><span class="c14 c17">Epub </span><span class="c5">dataset with 3975 transactions from the electronic publication platform of the Vienna University of Economics. Load up that data set, generate some rules, visualize the rules, and choose some interesting ones for further discussion. </span></p><p class="c7"><span class="c59 c14 c57 c23">Data Mining with Rattle </span></p><p class="c7"><span class="c5">A company called Togaware has created a graphical user interface (GUI) for R called Rattle. At this writing (working with R version 3.0.0), one of Rattle&rsquo;s components has gotten out of date and will not work with the latest version of R, particularly on the Mac. It is likely, however, that those involved with the Rattle project will soon update it to be compatible again. Using Rattle simplifies many of the processes described earlier in the chapter. Try going to the Togaware site and following the instructions there for installing Rattle for your particular operating system. </span></p><h3 class="c39" id="h.njz4zqv0m121"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_2j5yuzkhoae5-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Association_rule_learning&amp;sa=D&amp;source=editors&amp;ust=1751550345508089&amp;usg=AOvVaw26Y0BUGqxVe1BUtEnAHaAd">http://en.wikipedia.org/wiki/Association_rule_learning</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11&amp;sa=D&amp;source=editors&amp;ust=1751550345508314&amp;usg=AOvVaw3kbCiHhG6F6YTQzTKodRZf">http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11 a.pdf</a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://journal.r-project.org/archive/2009-2/RJournal_2009-2_Will&amp;sa=D&amp;source=editors&amp;ust=1751550345508619&amp;usg=AOvVaw0g4MLZE21q_X_cxoUYNGGU">http://journal.r-project.org/archive/2009-2/RJournal_2009-2_Will iams.pdf </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.r-bloggers.com/examples-and-resources-on-associati&amp;sa=D&amp;source=editors&amp;ust=1751550345508869&amp;usg=AOvVaw2bjgmKpARnVX4CzgxMJ0Bm">http://www.r-bloggers.com/examples-and-resources-on-associati on-rule-mining-with-r/ </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://rattle.togaware.com&amp;sa=D&amp;source=editors&amp;ust=1751550345508989&amp;usg=AOvVaw3cbqpiV7MdYT-CkPTrfBce">http://rattle.togaware.com </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.statsoft.com/textbook/association-rules/&amp;sa=D&amp;source=editors&amp;ust=1751550345509226&amp;usg=AOvVaw2gKbi1nhLHAaety_vdIHUj">http://www.statsoft.com/textbook/association-rules/ </a></span></li></ul><h3 class="c94" id="h.oi4h4rxqpz1k"><span class="c38 c57 c23 c18">Reference </span></h3><p class="c7"><span class="c5">Michael Hahsler, Kurt Hornik, and Thomas Reutterer (2006) Implications of probabilistic data modeling for mining association rules. In M. Spiliopoulou, R. Kruse, C. Borgelt, A. Nuernberger, and W. Gaul, editors, From Data and Information Analysis to Knowledge Engineering, Studies in Classification, Data Analysis, and Knowledge Organization, pages 598&ndash;605. Springer-Verlag. </span></p><h3 class="c94" id="h.qi9ulah80m2p"><span class="c38 c57 c23 c18">R Functions Used in This Chapter </span></h3><ul class="c32 lst-kix_8femmdvqkxfu-0 start"><li class="c7 c13 li-bullet-0"><span class="c14 c17">apriori() </span><span class="c5">Uses the algorithm of the same name to analyze a transaction data set and generate rules. </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">itemFrequencyPlot() </span><span class="c5">Shows the relative frequency of commonly occurring items in the spare occurrence matrix. </span></li><li class="c0 li-bullet-0"><span class="c14 c17">inspect() </span><span class="c14">Shows the contents of the data object generated by </span><span class="c14 c17">apriori() </span><span class="c5">that generates the association rules </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">install.packages() </span><span class="c5">Loads package from the CRAN repository </span></li><li class="c7 c13 li-bullet-0"><span class="c14 c17">summary() </span><span class="c14">Provides an overview of the contents of a data structure. </span></li></ul><hr style="page-break-before:always;display:none;"><p class="c62 c52 c81 title" id="h.2260m984eshf"><span class="c59 c21 c57 c84"></span></p><p class="c81 c62 title" id="h.m37516d4s2nv"><span class="c70">CHAPTER 18</span><span>&nbsp;<br>What&rsquo;s Your Vector, Victor?<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 525.33px;"><img alt="" src="images/image55.png" style="width: 624.00px; height: 525.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="flight map"></span><span class="c59 c57 c23 c37 c36">In the previous chapter, we explored an unsupervised learning technique known as association rules mining. In this chapter, we will examine a set of supervised learning approaches known as support vector machines (SVMs). SVMs are flexible algorithms that excel at addressing classification problems. </span></p><p class="c19 c52"><span class="c24 c23 c18"></span></p><p class="c19"><span class="c5">From the previous chapter you may remember that data mining techniques fall into two large categories: supervised learning techniques and unsupervised learning techniques. The association rules mining examined in the previous chapter was an unsupervised technique. This means that there was no particular criterion that we were trying to predict, rather we were just looking for patterns that would emerge from the data naturally. </span></p><p class="c7"><span class="c5">In the present chapter we will examine a supervised learning technique called &quot;support vector machines.&quot; Why the technique is called this we will examine shortly. The reason this is considered a supervised learning technique is that we &quot;train&quot; the algorithm on an initial set of data (the &quot;supervised&quot; phase) and then we test it out on a brand new set of data. If the training we accomplished worked well, then the algorithm should be able to predict the right outcome most of the time in the test data. </span></p><p class="c7"><span class="c5">Take the weather as a simple example. Some days are cloudy, some are sunny. The barometer rises some days and fall others. The wind may be strong or weak and it may come from various directions. If we collect data on a bunch of days and use those data to train a machine learning algorithm, the algorithm may find that cloudy days with a falling barometer and the wind from the east may signal that it is likely to rain. Next, we can collect more data on some other days and see how well our algorithm does at predicting rain on those days. The algorithm will make mistakes. The percentage of mistakes is the error rate, and we would like the error rate to be as low as possible. </span></p><p class="c7"><span class="c5">This is the basic strategy of supervised machine learning: Have a substantial number of training cases that the algorithm can use to </span></p><p class="c19"><span class="c5">discover and mimic the underlying pattern and then use the results of that process on a test data set in order to find out how well the algorithm and parameters perform in a &quot;cross validation.&quot; Cross validation, in this instance, refers to the process of verifying that the trained algorithm can carry out its prediction or classification task accurately on novel data. </span></p><p class="c7"><span class="c5">In this chapter, we will develop a &quot;support vector machine&quot; (SVM) to classify emails into spam or not spam. An SVM maps a low dimensional problem into a higher dimensional space with the goal of being able to describe geometric boundaries between different regions. The input data (the independent variables) from a given case are processed through a &quot;mapping&quot; algorithm called a kernel (the kernel is simply a formula that is run on each case&rsquo;s vector of input data), and the resulting kernel output determines the position of that case in multidimensional space. </span></p><p class="c7"><span class="c5">A simple 2D-3D mapping example illustrates how this works: Imagine looking at a photograph of a snow-capped mountain photographed from high above the earth such that the mountain looks like a small, white circle completely surrounded by a region of green trees. Using a pair of scissors, there is no way of cutting the photo on a straight line so that all of the white snow is on one side of the cut and all of the green trees are on the other. In other words there is no simple linear separation function that could correctly separate or classify the white and green points given their 2D position on the photograph. </span></p><p class="c7"><span class="c5">Next, instead of a piece of paper, think about a realistic three dimensional clay model of the mountain. Now all the white points occupy a cone at the peak of the mountain and all of the green points lie at the base of the mountain. Imagine inserting a sheet of cardboard through the clay model in a way that divides the snow capped peak from the green-tree-covered base. It is much easier to do now because the white points are sticking up into the high altitude and the green points are all on the base of the mountain. </span></p><p class="c7"><span class="c5">The position of that piece of cardboard is the planar separation function that divides white points from green points. A support vector machine analysis of this scenario would take the original two dimensional point data and search for a projection into three dimensions that would maximize the spacing between green points and white points. The result of the analysis would be a mathematical description of the position and orientation of the cardboard plane. Given inputs describing a novel data point, the SVM could then map the data into the higher dimensional space and then report whether the point was above the cardboard (a white point) or below the cardboard (a green point). The so called support vectors contain the coefficients that map the input data for each case into the high dimensional space. </span></p><p class="c7"><span class="c5">To get started with support vector machines, we can load one of the R packages that supports this technique. We will use the &quot;kernlab&quot; package. Use the commands below: </span></p><p class="c2"><span class="c5">&gt; install.packages(&quot;kernlab&quot;) </span></p><p class="c10"><span class="c5">&gt; library(kernlab) </span></p><p class="c29"><span class="c5">I found that it was important to use the double quotes in the first command, but not in the second command. The data set that we want to use is built into this package. The data comes from a study of spam emails received by employees at the Hewlett-Packard company. Load the data with the following command: </span></p><p class="c19"><span class="c5">&gt; data(spam) </span></p><p class="c7"><span class="c5">This command does not produce any output. We can now inspect the &quot;spam&quot; dataset with the str() command: </span></p><p class="c73"><span class="c34 c23">&gt; str(spam) </span></p><p class="c22"><span class="c34 c23">&#39;data.frame&#39;: 4601 obs. of 58 variables: </span></p><p class="c82"><span class="c34 c23">$ make : num 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ... </span></p><p class="c22"><span class="c34 c23">$ address : num 0.64 0.28 0 0 0 0 0 0 0 0.12 ... </span></p><p class="c82"><span class="c34 c23">$ all : num 0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ... </span></p><p class="c22"><span class="c34 c23">$ num3d : num 0 0 0 0 0 0 0 0 0 0 ... </span></p><p class="c22"><span class="c36 c48">.</span><span class="c44 c36 c134">.. $ charDollar : num 0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ... </span></p><p class="c22"><span class="c34 c23">$ charHash : num 0 0.048 0.01 0 0 0 0 0 0.022 0 ... </span></p><p class="c22"><span class="c34 c23">$ capitalAve : num 3.76 5.11 9.82 3.54 3.54 ... </span></p><p class="c82"><span class="c34 c23">$ capitalLong : num 61 101 485 40 40 15 4 11 445 43 ... </span></p><p class="c22"><span class="c34 c23">$ capitalTotal : num 278 1028 2259 191 191 ... </span></p><p class="c82"><span class="c34 c23">$ type : Factor w/ 2 levels &quot;nonspam&quot;,&quot;spam&quot;: 2 2 2 2 2 2 2 2 2 2 ... </span></p><p class="c73"><span class="c5">Some of the lines of output have been elided from the material above. You can also use the dim() function to get a quick overview of the data structure: </span></p><p class="c10"><span class="c5">&gt; dim(spam) </span></p><p class="c10"><span class="c5">[1] 4601 58 </span></p><p class="c7"><span class="c5">The dim() function shows the &quot;dimensions&quot; of the data structure. The output of this dim() function shows that the spam data structure has 4601 rows and 58 columns. If you inspect a few of the column names that emerged from the str() command, you may see that each email is coded with respect to its contents. There is lots of information available about the data set here: </span></p><p class="c7"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://archive.ics.uci.edu/ml/datasets/Spambase&amp;sa=D&amp;source=editors&amp;ust=1751550345519885&amp;usg=AOvVaw3yjg16aGykRLCgdEJeKJWw">http://archive.ics.uci.edu/ml/datasets/Spambase </a></span></p><p class="c7"><span class="c5">For example, just before &quot;type&quot; at the end of the output of the str() command on the previous page, we see a variable called &quot;capitalTotal.&quot; This is the total number of capital letters in the whole email. Right after that is the criterion variable, &quot;type,&quot; that indicates whether an email was classified as spam by human experts. Let&rsquo;s explore this variable a bit more: </span></p><p class="c10"><span class="c5">&gt; table(spam$type) </span></p><p class="c10"><span class="c5">nonspam spam </span></p><p class="c2"><span class="c5">2788 1813 </span></p><p class="c29"><span class="c5">We use the table function because type is a factor rather than a numeric variable. The output shows us that there are 2788 messages that were classified by human experts as not spam, and 1813 messages that were classified as spam. What a great dataset! </span></p><p class="c29"><span class="c5">To make the analysis work we need to divide the dataset into a training set and a test set. There is no universal way to do this, but as a rule of thumb, you can use two thirds of the data set to train and the remainder to test. Let&rsquo;s first generate a randomized index that will let us choose cases for our training and test sets. In the following command, we create a new list/vector variable that samples at random from a list of numbers ranging from 1 to the final element index of the spam data (4601). </span></p><p class="c19"><span class="c5">&gt; randIndex &lt;sample(1:dim(spam)[1]) </span></p><p class="c10"><span class="c5">&gt; summary(randIndex) </span></p><p class="c10"><span class="c5">Min. 1st Qu. Median Mean 3rd Qu. Max. </span></p><p class="c2"><span class="c5">1 1151 2301 2301 3451 4601 </span></p><p class="c10"><span class="c5">&gt; length(randIndex) </span></p><p class="c10"><span class="c5">[1] 4601 </span></p><p class="c29"><span class="c5">The output of the summary() and length() commands above show that we have successfully created a list of indices ranging from 1 to 4601 and that the total length of our index list is the same as the number of rows in the spam dataset: 4601. We can confirm that the indices are randomized by looking at the first few cases: </span></p><p class="c10"><span class="c5">&gt; head(randIndex) </span></p><p class="c10"><span class="c5">[1] 2073 769 4565 955 3541 3357 </span></p><p class="c7"><span class="c5">It is important to randomize your selection of cases for the training and test sets in order to ensure that there is no systematic bias in the selection of cases. We have no way of knowing how the original dataset was sorted (if at all) in case it was sorted on some variable of interest we do not just want to take the first 2/3rds of the cases as the training set. </span></p><p class="c7"><span class="c5">Next, let&rsquo;s calculate the &quot;cut point&quot; that would divide the spam dataset into a two thirds training set and a one third test set: </span></p><p class="c2"><span class="c5">&gt; cutPoint2_3 &lt;floor(2 * dim(spam)[1]/3) </span></p><p class="c10"><span class="c5">&gt; cutPoint2_3 </span></p><p class="c10"><span class="c5">[1] 3067 </span></p><p class="c169"><span class="c5">The first command in this group calculates the two-thirds cut point based on the number of rows in spam (the expression dim(spam)[1] gives the number of rows in the spam dataset). The second command reveals that that cut point is 3067 rows into the data set, which seems very sensible given that there are 4601 rows in total. Note that the floor() function chops off any decimal part of the calculation. We want to get rid of any decimal because an index variable needs to be an integer. </span></p><p class="c7"><span class="c5">Now we are ready to generate our test and training sets from the original spam dataset. First we will build our training set from the first 3067 rows: </span></p><p class="c10"><span class="c5">&gt; trainData &lt;spam[randIndex[1:cutPoint2_3],] </span></p><p class="c7"><span class="c5">We make the new data set, called trainData, using the randomized set of indices that we created in the randIndex list, but only using the first 3067 elements of randIndex (The inner expression in square brackets, 1:cutPoint2_3, does the job of selecting the first 3067 elements. From here you should be able to imagine the command for creating the test set: </span></p><p class="c2"><span class="c5">&gt; testData &lt;spam[randIndex[(cutPoint2_3+1):dim(spam)[1]],] </span></p><p class="c29"><span class="c5">The inner expression now selects the rows from 3068 all the way up to 4601 for a total of 1534 rows. So now we have two separate data sets, representing a two-thirds training and one third test breakdown of the original data. We are now in good shape to train our support vector model. The following command generates a model based on the training data set: </span></p><p class="c128"><span class="c5">&gt; svmOutput &lt;ksvm(type ~ ., data=trainData, kernel=&quot;rbfdot&quot;,kpar=&quot;automatic&quot;,C=5,cross=3, prob.model=TRUE) </span></p><p class="c2"><span class="c5">Using automatic sigma estimation (sigest) for RBF or laplace kernel </span></p><p class="c29"><span class="c5">Let&rsquo;s examine this command in some detail. The first argument, &quot;type ~ .&quot;, specifies the model we want to test. Using the word &quot;type&quot; in this expression means that we want to have the &quot;type&quot; variable (i.e., whether the message is spam or non-spam) as the outcome variable that our model predicts. The tilde character (&quot;~&quot;) in an R expression simply separates the left hand side of the expression from the right hand side. Finally, the dot character (&quot;.&quot;) is a shorthand that tell R to us all of the other variables in the dataframe to try to predict &quot;type.&quot; </span></p><p class="c7"><span class="c5">The &quot;data&quot; parameter let&rsquo;s us specify which dataframe to use in the analysis, In this case, we have specified that the procedure should use the trainData training set that we developed. </span></p><p class="c7"><span class="c5">The next parameter is an interesting one: kernel=&quot;rbfdot&quot;. You will remember from the earlier discussion that the kernel is the customizable part of the SVM algorithm that lets us project the low dimensional problem into higher dimensional space. In this case, the rbfdot designation refers to the &quot;radial basis function.&quot; One simple way of thinking about the radial basis function is that if you think of a point on a regular x,y coordinate system the distance from the origin to the point is like a radius of a circle. The &quot;dot&quot; in the name refers to the mathematical idea of a &quot;dot product,&quot; which is a way of multiplying vectors together to come up with a single number such as a distance value. In simplified terms, the radial basis function kernel takes the set of inputs from each row in a dataset and calculates a distance value based on the combination of the many variables in the row. The weighting of the different variables in the row is adjusted by the algorithm in order to get the maximum separation of distance between the spam cases and the non-spam cases. </span></p><p class="c7"><span class="c5">The &quot;kpar&quot; argument refers to a variety of parameters that can be used to control the operation of the radial basis function kernel. In this case we are depending on the good will of the designers of this algorithm by specifying &quot;automatic.&quot; The designers came up with some &quot;heuristics&quot; (guesses) to establish the appropriate parameters without user intervention. </span></p><p class="c7"><span class="c5">The C argument refers to the so called &quot;cost of constraints.&quot; Remember back to our example of the the white top on the green mountain? When we put the piece of cardboard (the planar separation function) through the mountain, what if we happen to get one green point on the white side or one white point on the green side? This is a kind of mistake that influences how the algorithm places the piece of cardboard. We can force these mistakes to be more or less &quot;costly,&quot; and thus to have more influence on the position of our piece of cardboard and the margin of separation that it defines. We can get a large margin of separation but possibly with a few mistakes by specifying a small value of C. If we specify a large value of C we may possibly get fewer mistakes, but on at the cost of having the cardboard cut a very close margin between the green and white points the cardboard might get stuck into the mountain at a very weird angle just to make sure that all of the green points and white points are separated. On the other hand if we have a low value of C we will get a generalizable model, but one that makes more classification mistakes. </span></p><p class="c19"><span class="c5">In the next argument, we have specified &quot;cross=3.&quot; Cross refers to the cross validation model that the algorithm uses. In this case, our choice of the final parameter, &quot;prob.model=TRUE,&quot; dictates that we use a so called three-fold cross validation in order to generate the probabilities associate with whether a message is or isn&rsquo;t a spam message. Cross validation is important for avoiding the problem of overfitting. In theory, many of the algorithms used in data mining can be pushed to the point where they essentially memorize the input data and can perfectly replicate the outcome data in the training set. The only problem with this is that the model base don the memorization of the training data will almost never generalize to other data sets. In effect, if we push the algorithm too hard, it will become too specialized to the training data and we won&rsquo;t be able to use it anywhere else. By using k-fold (in this case three fold) crossvalidation, we can rein in the fitting function so that it does not work so hard and so that it does creat a model that is more likely to generalize to other data. </span></p><p class="c7"><span class="c5">Let&rsquo;s have a look at what our output structure contains: </span></p><p class="c2"><span class="c5">&gt; svmOutput </span></p><p class="c10"><span class="c5">Support Vector Machine object of class &quot;ksvm&quot; </span></p><p class="c10"><span class="c5">SV type: C-svc (classification) </span></p><p class="c10"><span class="c5">parameter : cost C = 5 </span></p><p class="c10"><span class="c5">Gaussian Radial Basis kernel function. </span></p><p class="c10"><span class="c5">Hyperparameter : sigma = 0.0287825580201687 </span></p><p class="c10"><span class="c5">Number of Support Vectors : 953 </span></p><p class="c10"><span class="c5">Objective Function Value : -1750.51 </span></p><p class="c10"><span class="c5">Training error : 0.027388 </span></p><p class="c10"><span class="c5">Cross validation error : 0.076296 </span></p><p class="c2"><span class="c5">Probability model included. </span></p><p class="c29"><span class="c5">Most of this is technical detail that will not necessarily affect how we use the SVM output, but a few things are worth pointing out. First, the sigma parameter mentioned was estimated for us by the algorithm because we used the &quot;automatic&quot; option. Thank goodness for that as it would have taken a lot of experimentation to choose a reasonable value without the help of the algorithm. Next, note the large number of support vectors. These are the lists of weights that help to project the variables in each row into a higher dimensional space. The &quot;training error&quot; at about 2.7% is quite low. Naturally, the cross-validation error is higher, as a set of parameters never perform as well on subsequent data sets as they do with the original training set. Even so, a 7.6% cross validation error rate is pretty good for a variety of prediction purposes. </span></p><p class="c7"><span class="c5">We can take a closer look at these support vectors with the following command: </span></p><p class="c10"><span class="c5">&gt; hist(alpha(svmOutput)[[1]]) </span></p><p class="c7"><span class="c5">The alpha() accessor reveals the values of the support vectors. Note that these are stored in a nested list, hence the need for the [[1]] expression to access the first list in the list of lists. Because the particular dataset we are using only has two classes (spam or not spam), we only need one set of support vectors. If the &quot;type&quot; criterion variable had more than two levels (e.g., spam, not sure, and not spam), we would need additional support vectors to be able to classify the cases into more than two groups. The histogram output reveals the range of the support vectors from 0 to 5: </span></p><p class="c146"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 408.00px;"><img alt="" src="images/image28.png" style="width: 402.00px; height: 408.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Histogram of alpha(svmOutput)[[1]]"></span></p><p class="c146"><span class="c5">The maximum value of the support vector is equal to the cost parameter that we discussed earlier. We can see that about half of the support vectors are at this maximum value while the rest cluster around zero. Those support vectors at the maximum represent the most difficult cases to classify. WIth respect to our mountain metaphor, these are the white points that are below the piece of cardboard and the green points that are above it. </span></p><p class="c19 c52"><span class="c5"></span></p><p class="c19"><span class="c5">If we increase the cost parameter we can get fewer of these problem points, but at only at the cost of increasing our cross validation error: <br></span></p><p class="c42"><span class="c14 c17">&gt; svmOutput &lt;ksvm(type ~ ., data=trainData, kernel=&quot;rbfdot&quot;,kpar=&quot;automatic&quot;,</span><span class="c14 c80 c68">C=50</span><span class="c1">,cross=3,pro b.model=TRUE)<br> </span></p><p class="c42"><span class="c1">&gt; svmOutput </span></p><p class="c42"><span class="c1">Support Vector Machine object of class &quot;ksvm&quot; </span></p><p class="c42"><span class="c1">SV type: C-svc (classification) </span></p><p class="c42"><span class="c1">parameter : cost C = 50 </span></p><p class="c42"><span class="c1">Gaussian Radial Basis kernel function.<br> </span></p><p class="c42"><span class="c1">Hyperparameter : sigma = 0.0299992970259353 </span></p><p class="c42"><span class="c1">Number of Support Vectors : 850 </span></p><p class="c42"><span class="c1">Objective Function Value : -6894.635 </span></p><p class="c42"><span class="c1">Training error : 0.008803<br>Cross validation error : 0.085424 </span></p><p class="c42"><span class="c14 c17">Probability model included. </span><span class="c5"><br></span></p><p class="c19"><span class="c14">In the first command above, the </span><span class="c14 c17">C=50</span><span class="c14">&nbsp;is bolded to show what we changed from the earlier command. The output here shows that our training error went way down, to 0.88%, but that our crossvalidation error went up from 7.6% in our earlier model to 8.5% in this model. We can again get a histogram of the support vectors to show what has happened: <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 367.00px;"><img alt="" src="images/image2.png" style="width: 402.00px; height: 408.00px; margin-left: 0.00px; margin-top: -23.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c19"><span class="c5">Now there are only about 100 cases where the support vector is at the maxed out value (in this case 50, because we set C=50 in the svm command). Again, these are the hard cases that the model could not get to be on the right side of the cardboard (or that were right on the cardboard). Meanwhile, the many cases with the support vector value near zero represent the combinations of parameters that make a case lie very far from the piece of cardboard. These cases were so easy to classify that they really made no contribution to &quot;positioning&quot; the hyperplane that separates the spam cases from the non-spam cases. </span></p><p class="c43"><span class="c5">We can poke our way into this a little more deeply by looking at a couple of instructive cases. First, let&rsquo;s find the index numbers of a few of the support vectors that were near zero: </span></p><p class="c162"><span class="c12">&gt; alphaindex(svmOutput)[[1]][alpha(svmOutput)[[1]]&lt;0.05] </span></p><p class="c9"><span class="c12">[1] 90 98 289 497 634 745 1055 1479 1530 1544 1646 1830 1948 2520 2754 </span></p><p class="c135"><span class="c5">This monster of a command is not as bad as it looks. We are tapping into a new part of the svmOutput object, this time using the alphaindex() accessor function. Remember that we have 850 support vectors in this output. Now imagine two lists of 850 right next to each other: the first is the list of support vectors themselves, we get at that list with the alpha() accessor function. The second list, lined right up next to the first list, is a set of indices into the original training dataset, trainData. The left hand part of the expression in the command above let&rsquo;s us access these indices. The right hand part of the expression, where it says alpha(svmOutput)[[1]]&lt;0.05, is a conditional expression that let&rsquo;s us pick from the index list just those cases with a very small support vector value. You can see the output above, just underneat the command: about 15 indices were returned. Just pick off the first one, 90, and take a look at the individual case it refers to: </span></p><p class="c19"><span class="c12">&gt; trainData[90,] </span></p><p class="c10"><span class="c12">make address all num3d our over remove </span></p><p class="c10"><span class="c12">0 0 0 0 0 0 0 </span></p><p class="c2"><span class="c12">internet order mail receive will </span></p><p class="c10"><span class="c12">0 0 0 0 0 </span></p><p class="c10"><span class="c67 c36">.</span><span class="c44 c36 c118">..charExclamation charDollar charHash capitalAve </span></p><p class="c10"><span class="c12">1.123 0 0 2.6 </span></p><p class="c10"><span class="c12">capitalLong capitalTotal </span></p><p class="c2"><span class="c12">16 26 </span></p><p class="c10"><span class="c12">type </span></p><p class="c10"><span class="c12">nonspam </span></p><p class="c135"><span class="c5">The command requested row 90 from the trainData training set. A few of the lines of the output were left off for ease of reading and almost all of the variables thus left out were zero. Note the very last two lines of the output, where this record is identified as a non-spam email. So this was a very easy case to classify because it has virtually none of the markers that a spam email usually has (for example, as shown above, no mention of internet, order, or mail). You can contrast this case with one of the hard cases by running this command: </span></p><p class="c73"><span class="c12">&gt; alphaindex(svmOutput)[[1]][alpha(svmOutput)[[1]]==50] </span></p><p class="c73"><span class="c5">You will get a list of the 92 indices of cases where the support vector was &quot;maxed out&quot; to the level of the cost function (remember C=50 from the latest run of the svm() command). Pick any of those cases and display it, like this: </span></p><p class="c2"><span class="c5">&gt; trainData[11,] </span></p><p class="c29"><span class="c5">This particular record did not have many suspicious keywords, but it did have long strings of capital letters that made it hard to classify (it was a non-spam case, by the way). You can check out a few of them to see if you can spot why each case may have been difficult for the classifier to place. </span></p><p class="c7"><span class="c5">The real acid test for our support vector model, however, will be to use the support vectors we generated through this training process to predict the outcomes in a novel data set. Fortunately, because we prepared carefully, we have the testData training set ready to go. The following commands with give us some output known as a &quot;confusion matrix:&quot; </span></p><p class="c10"><span class="c44 c23 c18 c36">&gt; svmPred &lt;predict(svmOutput, testData, type=&quot;votes&quot;) </span></p><p class="c73"><span class="c44 c23 c18 c36">&gt; compTable &lt;data.frame(testData[,58],svmPred[1,]) </span></p><p class="c10"><span class="c44 c23 c18 c36">&gt; table(compTable) </span></p><p class="c10"><span class="c44 c23 c18 c36">svmPred.1... </span></p><p class="c73"><span class="c44 c23 c18 c36">testData...58. 0 1 </span></p><p class="c10"><span class="c44 c23 c18 c36">nonspam 38 854 </span></p><p class="c10"><span class="c44 c23 c18 c36">spam 574 68 </span></p><p class="c19"><span class="c5">The first command in the block above uses our model output from before, namely svmOutput, as the parameters for prediction. It uses the &quot;testData,&quot; which the support vectors have never seen before, to generate predictions, and it requests &quot;votes&quot; from the prediction process. We could also look at probabilities and other types of model output, but for a simple analysis of whether the svm is generating good predictions, votes will make our lives easier. </span></p><p class="c7"><span class="c5">The output from the predict() command is a two dimensional list. You should use the str() command to examine its structure. basically there are two lists of &quot;vote&quot; values side by side. Both lists are 1534 elements long, corresponding to the 1534 cases in our testData object. The lefthand list has one for a non-spam vote and zero for a spam vote. Because this is a two-class problem, the other list has just the opposite. We can use either one because they are mirror images of each other. </span></p><p class="c7"><span class="c5">In the second command above, we make a little dataframe, called compTable, with two variables in it: The first variable is the 58th column in the test data, which is the last column containing the &quot;type&quot; variable (a factor indicating spam or non-spam). Remember that this type variable is the human judgments from the original dataset , so it is our ground truth. The second variable is the first column in our votes data structure (svmPred), so it contains ones for non-spam predictions and zeros for spam predictions. </span></p><p class="c7"><span class="c5">Finally, applying the table() command to our new dataframe (compTable) gives us the confusion matrix as output. Along the main diagonal we see the erroneous classifications 38 cases that were not spam, but were classified as spam by the support vector matrix and 68 cases that were spam, but were classified as non spam by the support vector matrix. On the counter-diagonal, we see 854 cases that were correctly classified as non-spam and 574 cases that were correctly classified as spam. </span></p><p class="c7"><span class="c5">Overall, it looks like we did a pretty good job. There are a bunch of different ways of calculating the accuracy of the prediction, depending upon what you are most interested in. The simplest way is to sum the 68 + 38 = 106 error cases and divided by the 1534 total cases for an total error rate of about 6.9%. Interestingly, that is a tad better than the 8.5% error rate we got from the k-fold crossvalidation in the run of svm() that created the model we are testing. Keep in mind, though, that we may be more interested in certain kinds of error than other kinds. For example, consider which is worse, an email that gets mistakenly quarantined because it is not really spam, or a spam email that gets through to someone&rsquo;s inbox? It really depends on the situation, but you can see that you might want to give more consideration to either the 68 misclassification errors or the other set of 38 misclassification errors. </span></p><h3 class="c94" id="h.cwpwzgz2ydtn"><span class="c38 c57 c23 c18">Chapter Challenge </span></h3><p class="c7"><span class="c5">Look up the term &quot;confusion matrix&quot; and then follow-up on some other terms such as Type I error, Type II error, sensitivity, and specificity. Think about how the support vector machine model could be modified to do better at either sensitivity or specificity. </span></p><p class="c7"><span class="c14">For a super challenge, try using another dataset with the kernlab svm technology. There is a dataset called </span><span class="c14 c17">promotergene </span><span class="c14">that is built into the </span><span class="c14 c17">kernlab </span><span class="c14">package. You could also load up your own data set and try creating an </span><span class="c14 c17">svm </span><span class="c5">model from that. </span></p><h3 class="c39" id="h.t9vjs981vv7x"><span class="c38 c57 c23 c18">Sources </span></h3><ul class="c32 lst-kix_g8didx5tqsy8-0 start"><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/svmbasic/sv&amp;sa=D&amp;source=editors&amp;ust=1751550345550408&amp;usg=AOvVaw3KJpO9gfISQjDyQMEcFBq1">http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/svmbasic/sv mbasic_notes.pdf</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org/web/packages/kernlab/kernlab.pdf&amp;sa=D&amp;source=editors&amp;ust=1751550345550760&amp;usg=AOvVaw0uBkU5_CRrUVJPaS4FImby">http://cran.r-project.org/web/packages/kernlab/kernlab.pdf</a></span><span class="c11">&nbsp;</span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://en.wikipedia.org/wiki/Confusion_matrix&amp;sa=D&amp;source=editors&amp;ust=1751550345551003&amp;usg=AOvVaw3rLq-hnIlOxxAUaOFJuR8F">http://en.wikipedia.org/wiki/Confusion_matrix </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://stackoverflow.com/questions/9480605/what-is-the-relatio&amp;sa=D&amp;source=editors&amp;ust=1751550345551380&amp;usg=AOvVaw2xcPmyWjLWMvS2udhgwjr5">http://stackoverflow.com/questions/9480605/what-is-the-relatio n-between-the-number-of-support-vectors-and-training-data-and </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.louisaslett.com/Courses/Data_Mining/ST4003-Lab7&amp;sa=D&amp;source=editors&amp;ust=1751550345551713&amp;usg=AOvVaw124pPKANo5TuETr_oqCt5j">http://www.louisaslett.com/Courses/Data_Mining/ST4003-Lab7 -Introduction_to_Support_Vector_Machines.pdf </a></span></li><li class="c7 c13 li-bullet-0"><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.jstatsoft.org/v11/i09/paper&amp;sa=D&amp;source=editors&amp;ust=1751550345551935&amp;usg=AOvVaw1DKG-pzl4EbDBxbcVxGzRT">http://www.jstatsoft.org/v11/i09/paper</a></span></li></ul><p class="c7 c52"><span class="c5"></span></p><p class="c148 title" id="h.gg0jbrrhvz81"><span class="c70">APPENDIX A </span><span class="c38 c57 c23 c84"><br>Installing R and R-Studio</span></p><p class="c7"><span class="c14">As mentioned in previous chapters, R is an open source program, meaning that the source code that is used to create a copy of R to run on a Mac, Windows, or Linux computer is available for all to inspect and modify. If your computer has the Windows&reg;, Mac-OSX&reg; or a Linux operating system, there is a version of R waiting for you at </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://cran.r-project.org&amp;sa=D&amp;source=editors&amp;ust=1751550345552883&amp;usg=AOvVaw26l0EdcYeV7-Kfl1ym9iPr">http://cran.r-project.org</a></span><span class="c14">/. Download and install your own copy. If you sometimes have difficulties with installing new software and you need some help, there is a wonderful little book by Thomas P. Hogan called, </span><span class="c14 c46">Bare Bones R: A Brief Introductory Guide </span><span class="c14">that you might want to buy or borrow from your library. There are lots of sites online that also give help with installing R, although many of them are not oriented towards the inexperienced user. I searched online using the term &quot;help installing R&quot; and I got a few good hits. One site that was quite informative for installing R on Windows was at &quot;readthedocs.org,&quot; and you can try to access it at this TinyUrl: </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://tinyurl.com/872ngt&amp;sa=D&amp;source=editors&amp;ust=1751550345553978&amp;usg=AOvVaw3y5RKLVV_9C02_YlUQ0qgG">http://tinyurl.com/872ngt</a></span><span class="c40 c61">t</span><span class="c14">. For Mac users there is a video by Jeremy Taylor at Vimeo.com, </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://vimeo.com/36697971&amp;sa=D&amp;source=editors&amp;ust=1751550345554222&amp;usg=AOvVaw3vrqcsPNqxpG4cuG1G0O4m">http://vimeo.com/36697971</a></span><span class="c5">, that outlines both the initial installation on a Mac and a number of other optional steps for getting started. YouTube also has videos that provide brief tutorials for installing R. Try searching for &quot;install R&quot; in the YouTube search box. The rest of this chapter assumes that you have installed R and can run it on your computer as shown in the screenshot above. (Note that this screenshot is from the Mac version of R: if you are running Windows or Linux your R screen may appear slightly different from this.) Just for fun, one of the first things you can do when you have R running is to click on the color wheel and customize the appearance of R. This screenshot uses Syracuse orange as a background color. The screenshot also shows a simple command to type that shows the most basic method of interaction with R.</span></p><p class="c7"><span class="c14">Perhaps the most challenging aspect of installing R-Studio is having to install R first, but if you&rsquo;ve already done that, then R-Studio should be a piece of cake. Make sure that you have the latest version of R installed before you begin with the installation of R-studio. There is ample documentation on the R-studio website, </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.rstudio.org/&amp;sa=D&amp;source=editors&amp;ust=1751550345556055&amp;usg=AOvVaw0iaODBdo1L7IMaea-LIxkJ">http://www.rstudio.org/</a></span><span class="c14">, so if you follow the instructions there, you should have minimal difficulty. If you reach a page where you are asked to choose between installing Rstudio server and installing R-studio as a desktop application on your computer, choose the latter. For now you want the desktop/single user version. If you run into any difficulties or you just want some additional guidance about R-studio, you may want to have a look at the book entitled, </span><span class="c14 c46">Getting Started with R-studio</span><span class="c14">, by John Verzani (2011, Sebastopol, CA: O&rsquo;Reilly Media). The first chapter of that book has a general orientation to R and R-studio as well as a guide to installing and updating R-studio. There is also a YouTube video that introduces R-studio here: </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=http://www.youtube.com/watch?v%3D7sAmqkZ3Be8&amp;sa=D&amp;source=editors&amp;ust=1751550345557225&amp;usg=AOvVaw0SrT-bRhM9ZbX6KYKWa5ij">http://www.youtube.com/watch?v=7sAmqkZ3Be8</a></span><span class="c40 c61">&nbsp;</span><span class="c5">Be aware if you search for other YouTube videos that there is a disk recovery program as well a music group that share the R-Studio name: You will get a number of these videos if you search on &quot;R-Studio&quot; without any other search terms. </span></p><p class="c7 c52"><span class="c5"></span></p><p class="c7 c52"><span class="c5"></span></p><div><p class="c62 c52 c139"><span class="c24 c23 c54"></span></p></div></body></html>