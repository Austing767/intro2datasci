<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch11-popularity-contest" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Popularity Contest</title>

  <section xml:id="popularity-comparison">
    <title>Popularity Comparison</title>

  <introduction>
				<p>
					<image source='twitter-wannabviral.png' decorative="yes"/><em>Which topic on Twitter is more popular, Lady Gaga or Oprah Winfrey? This may not seem like an important question, depending upon your view of older popular culture, but if we can make the comparison for these two topics, we can make it for any two topics. Certainly in the case of the next presidential election, or a corruption scandal in the local news, or an international crisis, it could be a worthwhile goal to be able to analyze social media in a systematic way. And on the surface, the answer to the question seems trivial: Just add up who has more tweets. Surprisingly, in order to answer the question in an accurate and reliable way, this won’t work, at least not very well. Instead, one must consider many of the vexing questions that made inferential statistics necessary.</em>
				</p>
  </introduction>



				<p>
					Recall from the last chapter that there’s a whole area of statistics concerned with "arrival" times and similar phenomena, dating back to a famous study by Ladislaus von Bortkiewicz of horsemen who died after being kicked by their horses. Von Bortkiewicz studied each of 14 cavalry corps over a period of 20 years, noting when horsemen died each year. The distribution of the "arrival" of kickdeaths turns out to have many similarities to other arrival time data, such as the arrival of buses or subway cars at a station, the arrival of customers at a cash register, or the occurrence of telephone calls at a particular exchange. All of these kinds of events fit what is known as a "<term>Poisson Distribution</term>" (named after Simeon Denis Poisson, who published it). Let’s find out if the arrival times of tweets comprise a Poisson distribution.
				</p>

				<p>
					Let’s say we retrieved one hour’s worth of Lady Gaga tweets and a similar amount of Oprah Winfrey tweets and just counted them up. What if it just happened to be a slow news day for Oprah? It really wouldn’t be a fair comparison. What if most of Lady Gaga’s tweets happen at midnight or on Saturdays? We could expand our sampling time, maybe to a day or a week. This could certainly help: Generally speaking, the bigger the sample, the more representative it is of the whole population, assuming it is not collected in a biased way. This approach defines popularity as the number of tweets over a fixed period of time. Its success depends upon the choice of a sufficiently large period of time, that the tweets are collected for the two topics at the same time, and that the span of time chosen happens to be equally favorable for both two topics.
				</p>

				<p>
					Another approach to the popularity comparison would build upon what we learned in the previous chapter about how arrival times (and the delays between them) fit into the Poisson distribution. In this alternative definition of the popularity of a topic, we could suggest that if the arrival curve is "steeper" for the first topic in contrast to the second topic, then the first topic is more active and therefore more popular. Another way of saying the same thing is that for the more popular topic, the likely delay until the arrival of the next tweet is shorter than for the less popular topic. You could also say that for a given interval of time, say ten minutes, the number of arrivals for the first topic would be higher than for the second topic. Assuming that the arrival delays fit a Poisson distribution, these are all equivalent ways of capturing the comparison between the two topics.
				</p>

				<p>
					Just as we did in the chapter entitled, "Sample in a Jar," we can use a random number generator in R to illustrate these kinds of differences more concretely. The relevant function for the Poisson distribution is rpois(), "random poisson." The <idx><c>rpois()</c></idx><term>rpois()</term> function will generate a stream of random numbers that roughly fit the Poisson distribution. The fit gets better as you ask for a larger and larger sample. The first argument to rpois() is how many random numbers you want to generate and the second number is the average delay between arrivals that you want the random number generator to try to come close to. We can look at a few of these numbers and then use a histogram function to visualize the results:
				</p>

				<pre>
					&gt; rpois(10,3)
				
					[1] 5 4 4 2 0 3 6 2 3 3
					
					&gt; mean(rpois(100,3))

					[1] 2.99

					&gt; var(rpois(100,3))

					[1] 3.028182

					&gt; hist(rpois(1000,3))
				</pre>

				<figure>
					<image source='histogram-rpois-3.png'/>
					<caption>Histogram of hist(rpois(1000,3))</caption> 
				</figure>
				
				<p>
					In the first command above, we generate a small sample of n=10 arrival delays, with a hoped-for mean of 3 seconds of delay, just to see what kind of numbers we get. You can see that all of the numbers are small integers, ranging from 0 to 6. In the second command we double check these results with a slightly larger sample of n=100 to see if rpois() will hit the mean we asked for. In that run it came out to 2.99, which was pretty darned close. If you run this command yourself you will find that your result will vary a bit each time: it will sometimes be slightly larger than three and occasionally a little less than three (or whatever mean you specify).
				</p>

				<p>
					This is expected because of the random number generator. In the third command we run yet another sample of 100 random data points, this time analyzing them with the <idx><c>var()</c></idx><term>var()</term> function (which calculates the variance; see the chapter entitled "Beer, Farms, and Peas"). It is a curious fact of Poisson distributions that the mean and the variance of the "ideal" (i.e., the theoretical) distribution are the same. In practice, for a small sample, they may be different.
				</p>

				<p>
					In the final command, we ask for a histogram of an even larger sample of n=1000. The histogram shows the most common value hanging right around three seconds of delay with a nice tail that points rightwards and out to about 10 seconds of delay. You can think of this as one possible example of what you might observe of the average delay time between tweets was about three seconds. Note how similar the shape of this histogram is to what we observed with real tweets in the last chapter.
				</p>

				<p>
					Compare the histogram on the previous page to the one on the next page that was generated with this command:
				</p>

				<p>
					<sage language="r">
						<input>
							hist(rpois(1000,10))
						</input>
					</sage>
				</p>

				<p>
					It is pretty easy to see the different shape and position of this histogram, which has a mean arrival delay of about ten seconds. First of all, there are not nearly as many zero length delays. Secondly, the most frequent value is now about 10 (as opposed to two in the previous histogram). Finally, the longest delay is now over 20 seconds (instead of 10 for the previous histogram). One other thing to try is this:
				</p>

				<p>
					<sage language="r">
						<input>
							sum(rpois(1000,10)&lt;=10)
						</input>
					</sage>
				</p>

				<pre>
					[1] 597
				</pre>

				<figure>
					<image source="histogram-rpois-10.png"/>
					<caption>Histogram of hist(rpois(1000,10))</caption>
				</figure>

				<p>
					This command generated 1000 new random numbers, using the Poisson distribution and also with a hoped-for mean of 10, just like in the histogram on the next page. Using the "&lt;=" inequality test and the sum() function, we then counted up how many events were less than or equal to 10, and this turned out to be 597 events. As a fraction of the total of n=1000 data points that rpois() generated, that is 0.597, or 59.7%.
				</p>

				<p>
					We can look at the same kind of data in terms of the probability of arrival within a certain amount of time. Because rpois() generates delay times directly (rather than us having to calculate them from neighboring arrival times), we will need a slightly different function than the ArrivalProbabilities() that we wrote and used in the previous chapter. We’ll call this function "DelayProbability" (the code is at the end of this chapter):
				</p>

				<pre>
					&gt; DelayProbability(rpois(100,10),1,20)
				</pre>

				<pre>
					[1] 0.00 0.00 0.00 0.03 0.06 0.09 0.21 0.33 0.48 0.61 0.73 0.82 0.92

					[14] 0.96 0.97 0.98 0.99 1.00 1.00 1.00
				</pre>

				<p>
					At the heart of that command is the rpois() function, requesting 100 points with a mean of 10. The other two parameters are the increment, in this case one second, and the maximum delay time, in this case 20 seconds. The output from this function is a sorted list of cumulative probabilities for the times ranging from 1 second to 20 seconds. Of course, what we would really like to do is compare these probabilities to those we would get if the average delay was three seconds instead of ten seconds. We’re going to use two cool tricks for creating this next plot. First, we will use the <idx><c>points()</c></idx><term>points()</term> command to add points to an existing plot. Second, we will use the col= parameter to specify two different colors for the points that we plot. Here’s the code that creates a plot and then adds more points to it:
				</p>

				<pre>
					&gt; plot(DelayProbability(rpois(100,10),1,20), col=2)

					&gt; points(DelayProbability(rpois(100,3),1,20), col=3)

				</pre>

				<p>
					Again, the heart of each of these lines of code is the rpois() function that is generating random Poisson arrival delays for us. Our parameters for increment (1 second) and maximum (20 seconds) are the same for both lines. The first line uses col=2, which gives us red points, and the second gives us col=3, which yields green points:
				</p>

				<figure>
					<image source="graph-delay-probability.png"/>
					<caption>&gt; plot(DelayProbability(rpois(100,10),1,20), col=2) &gt; points(DelayProbability(rpois(100,3),1,20), col=3) </caption>
				</figure>

				<p>
					This plot clearly shows that the green points have a "steeper" profile. We are more likely to have earlier arrivals for the 3-second delay data than we are for the 10-second data. If these were real tweets, the green tweets would be piling in much faster than the red tweets. Here’s a reminder on how to read this plot: Look at a value on the X-axis, for example "5." Then look where the dot is and trace leftward to the Y-axis. For the red dot, the probability value at time (x) equal 4 is about 0.10. So for the red data there is about a 10% chance that the next event will occur within five time units (we’ve been calling them seconds, but they could really be anything, as long as you use the units consistently throughout the whole example). For the green data there is about a 85% chance that the next event will occur within four time units. The fact that the green curve rises more steeply than the red curve means that <em>for these two samples only</em> the green stuff is arriving <em>much more often</em> than the red stuff.
				</p>

				<p>
					The reason we emphasized the point "for these samples only" is that we know from prior chapters that every sample of data you collect varies by at least a little bit and sometimes by quite a lot. A sample is just a snapshot, after all, and things can and do change from sample to sample. We can illustrate this by running and plotting multiple samples, much as we did in the earlier chapter:
				</p>

				<sage language="r">
  					<input>
    					<xi:include href="../code/delay-probability.r" parse="text"/>
    
   						 plot(DelayProbability(rpois(100,10), 1, 20))
    						for (i in 1:15) {
      							points(DelayProbability(rpois(100,10), 1, 20))
    						}
  					</input>
				</sage>

				<p>
					This is the first time we have used the "for loop" in R, so let’s walk through it. A "<idx>for loop</idx><term>for loop</term>" is one of the basic constructions that computer scientists use to "iterate" or repeatedly run a chunk of code. In R, a for loop runs the code that is between the curly braces a certain number of times. The number of times R runs the code depends on the expression inside the parentheses that immediately follow the "for."
				</p>

				<p>
					In the example above, the expression "i in 1:15" creates a new data object, called i, and then puts the number 1 in it. Then, the “for loop” keeps adding one to the value of i, until i reaches 15. Each time that it does this, it runs the code between the curly braces. The expression "in 1:15" tells R to start with one and count up to 15. The data object i, which is just a plain old integer, could also have been used within the curly braces if we had needed it, but it doesn’t have to be used within the curly braces if it is not needed. In this case we didn’t need it. The code inside the curly braces just runs a new random sample of 100 Poisson points with a hoped for mean of 10.
				</p>

				<figure>
					<image source="delay-probability-multiples.png"/>
					<caption>Plot using for loop</caption>
				</figure>

				<p>
					When you consider the two command lines on the previous page together you can see that we initiate a <c>plot()</c> on the first line of code, using similar parameters to before (random poisson numbers with a mean of 10, fed into our probability calculator, which goes in increments of 1 second up to 20 seconds). In the second line we add more points to the same plot, by running exactly 15 additional copies of the same code. Using rpois() ensures that we have new random numbers each time:
				</p>

				<p>
					Now instead of just one smooth curve we have a bunch of curves, and that these curves vary quite a lot. In fact, if we take the example of 10 seconds (on the X-axis), we can see that in one case the probability of a new event in 10 seconds could be as low as 0.50, while in another case the probability is as high as about 0.70.
				</p>

				<p>
					This shows why we can’t just rely on one sample for making our judgments. We need to know something about the uncertainty that surrounds a given sample. Fortunately, R gives us additional tools to help us figure this situation out. First of all, even though we had loads of fun programming the DelayProbability() function, there is a quicker way to get information about what we ideally <em>expect</em> from a Poisson distribution. The function <idx><c>ppois()</c></idx><term>ppois()</term> gives us the <em>theoretical</em> probability of observing a certain delay time, given a particular mean. For example:
				</p>

				<p>
					<sage language="r">
						<input>
							ppois(3, lambda=10)
						</input>
					</sage>
				</p>


				<p>
					So you can read this as: There is a 1% chance of observing a delay of 3 or less in a Poisson distribution with mean equal to 10. Note that in statistical terminology, "<idx>lambda</idx><term>lambda</term>" is the term used for the mean of a Poisson distribution. We’ve provided the named parameter "lambda=10" in the example above just to make sure that R does not get confused about what parameter we are controlling when we say "10." The ppois() function does have other parameters that we have not used here. Now, using a for loop, we could get a list of several of these theoretical probabilities:
				</p>

				<p>
					<sage language="r">
						<input>
							plot(1,20,xlim=c(0,20),ylim=c(0,1))

							for (i in 1:20) {points(i,ppois(i,lambda=10)) }
						</input>
					</sage>
				</p>

				<p>
					We are using a little code trick in the first command line above by creating a nearly empty set of axes with the plot() function, and then filling in the points in the second line using the points() function. This gives the following plot:
				</p>

				<figure>
					<image source="graph-logistic.png"/>
					<caption>Poisson curve showing the chance of an event happening within a certain time.</caption>
				</figure>

				<p>
					You may notice that this plot looks a lot like the ones earlier in this chapter as well as somewhat similar to the probability plot in the previous chapter. When we say the "<idx>theoretical distribution</idx><term>theoretical distribution</term>" we are talking about the ideal Poisson distribution that would be generated by the complex equation that Mr. Poisson invented a couple of centuries ago. Another way to think about it is this: Instead of just having a small sample of points, which we know has a lot of randomness in it, what if we had a truly humongous sample with zillions of data points? The curve in the plot above is just about what we would observe for a truly humongous sample (where most of the biases up or down cancel themselves out because of the large number of points).
				</p>

				<p>
					So this is the ideal, based on the mathematical theory of the Poisson distribution, or what we would be likely to observe if we created a really large sample. We know that real samples, of reasonable amounts of data, like 100 points or 1000 points or even 10,000 points, will not hit the ideal exactly, because some samples will come out a little higher and others a little lower.
				</p>

				<p>
					We also know, from the histograms and output earlier in the chapter, that we can look at the mean of a sample, or the count of events less than or equal to the mean, or the arrival probabilities in the graph on this page, and in <em>each case we are looking at different versions of the same information</em>. Check out these five commands:
				</p>

				<p>
					<sage language="r">
						<input>
							mean(rpois(100000,10))
						</input>
					</sage>
				</p>

				<p>
					<sage language="r">
						<input>
							var(rpois(100000,10))
						</input>
					</sage>
					
				</p>

				<p>
					<sage language="r">
						<input>
							sum(rpois(100000,10)&lt;=10)/100000
						</input>
					</sage>
				</p>

				<p>
					<sage language="r">
						<input>
							ppois(10,lambda=10)
						</input>

					</sage>
				</p>

				<p>
					<sage language="r">
						<input>
							qpois(0.58303,lambda=10)
						</input>
					</sage>
				</p>

				<p>
					In the first command, we confirm that for a very large random sample of n=100,000 with a desired mean of 10, the actual mean of the random sample is almost exactly 10. Likewise, for another large random sample with a desired mean of 10, the variance is 10. In the next command, we use the inequality test and the sum() function again to learn that the probability of observing a value of 10 or less in a very large sample is about 0.59 (note that the sum() function yielded 58,638 and we divided by 100,000 to get the reported value of 0.58638). Likewise, when we ask for the theoretical distribution with ppois() of observing 10 or less in a sample with a mean of 10, we get a probability of 0.58303, which is darned close to the empirical result from the previous command. Finally, if we ask the <idx><c>qpois()</c></idx><term>qpios()</term> function what is the <em>threshold value</em> for a probability of 0.58303 is in a Poisson sample with mean of 10, we get back the answer: 10. You may see that qpois() does the reverse of what ppois() does. For fun, try this formula on the R command line:
				</p>

				<p>
					<sage language="r">
						<input>
							qpois(ppois(10, lambda=10), lambda=10)
						</input>
					</sage>
				</p>

				<p>
					Here’s one last point to cap off this thinking. Even with a sample of 100,000 there is some variation in samples. That’s why the 0.58638 from the sum() function above does not exactly match the theoretical 0.58303 from the ppois() function above. We can ask R to tell us how much variation there is around one of these probabilities using the <idx><c>poisson.test()</c></idx><term>poisson.test()</term> function like this:
				</p>

				<p>
					<sage language="r">
						<input>
							poisson.test(58638, 100000)
						</input>
					</sage>
				</p>

				<note>
					<p>
						95 percent confidence interval:
						0.5816434 0.5911456
					</p>
				</note>

				<p>
					We’ve truncated a little of the output in the interests of space: What you have left is the upper and lower bounds on a 95% confidence interval. Here’s what a confidence interval is: For 95% of the samples that we could generate using rpois(), using a sample size of 100,000, and a desired mean of 10, we will get a result that lies between 0.5816434 and 0.5911456 (remember that this resulting proportion is calculated as the total number of events whose delay time is 10 or less). So we know what would happen for 95% of the rpois() samples, but the assumption that statisticians also make is that <em>if</em> a natural phenomenon, like the arrival time of tweets, also fits the Poisson distribution, that this same confidence interval would be operative. So while we know that we got 0.58638 in one sample on the previous page, it is likely that future samples will vary by a little bit (about 1%). Just to get a feel for what happens to the confidence interval with smaller samples, look at these:
				</p>

				<p>
					<sage language="r">
						<input>
							poisson.test(5863, 10000)
						</input>
					</sage>
				</p>

				<note>
					<p>
						(5863, 10000)
						95 percent confidence interval:
						0.5713874 0.6015033
					</p>
				</note>

				<p>
					<sage language="r">
						<input>
							poisson.test(586, 1000)
						</input>
					</sage>
				</p>

				<note>
					<p>
						(586, 1000)
						95 percent confidence interval:
						0.5395084 0.6354261
					</p>
				</note>

				<p>
					<sage language="r">
						<input>
							poisson.test(58, 100)
						</input>
					</sage>
				</p>

				<note>
					<p>
						(58, 100)
						95 percent confidence interval:
						0.4404183 0.7497845
					</p>
				</note>

				<p>
					We’ve listed the parameters that changed in each of the three commands above, just to emphasize that in each case we’ve reduced the sample size by a factor of 10. By the time we get to the bottom look how wide the confidence interval gets. With a sample of 100 events, of which 58 had delays of 10 seconds or less, the confidence interval around the proportion of 0.58 ranges from a low of 0.44 to a high of 0.75! That’s huge! The confidence interval gets wider and wider as we get <em>less and less confident about the accuracy of our estimate.</em> In the case of a small sample of 100 events, the confidence interval is very wide, showing that we have a lot of uncertainty about our estimate that 58 events out of 100 will have arrival delays of 10 or less. Note that you can filter out the rest of the stuff that poisson.test() generates by asking specifically for the "conf.int" in the output that is returned:
				</p>

				<p>
					<sage language="r">
						<input>
							poisson.test(58, 100)$conf.int
						</input>
					</sage>
				</p>

				<note>
					[1] 0.4404183 0.7497845
					
					<term>attr(,"conf.level")</term>
					[1] 0.95
				</note>

				<p>
					The bolded part of the note above shows how we used the $ notation to get a report of just the bit of output that we wanted from poisson.test(). This output reports the exact same confidence interval that we saw on the previous page, along with a reminder in the final two lines that we are looking at a 95% confidence interval.
				</p>

				<p>
					At this point we have all of the knowledge and tools we need to compare two sets of arrival rates. Let’s grab a couple of sets of tweets and extract the information we need. Open the HWK Assignment for this Chapter in R-Studio. First, we will import the dataset GagaData.xlsl into the environment. Name the variable for importing tweetDF.
				</p>

				<p>
					Next, we need to sort the tweets by arrival time, That is, of course, unless you accepted the Chapter Challenge in the previous chapter and built the sorting into your TweetFrame() function.
				</p>

				<pre>
				# Sort tweets by creation time
				sortweetDF &lt;- tweetDF[order(as.integer(tweetDF$created)), ]
				</pre>

				<p>
				The dataset is first sorted to ensure that tweets are in chronological order. This is necessary for meaningful inter-arrival time calculations.
				</p>

				<pre>
				# Compute inter-arrival times (delays between tweets)
				eventDelays &lt;- as.integer(diff(sortweetDF$created))
				</pre>

				<p>
				The <code>diff()</code> function calculates the time difference between each pair of successive tweets. These differences are converted to integers and stored in a vector called <code>eventDelays</code>.
				</p>

				<pre>
				# Calculate mean delay
				mean_delay &lt;- mean(eventDelays)
				print(mean_delay)
				# Example output: [1] 30.53707
				</pre>

				<p>
				This tells us that, on average, Lady Gaga posts a tweet approximately every 30.54 seconds.
				</p>

				<pre>
				# Count how many delays are less than or equal to 31 seconds
				within_31 &lt;- sum(eventDelays &lt;= 31)
				print(within_31)
				# Example output: [1] 333
				</pre>

				<p>
				Out of 500 tweets, 333 arrived within 31 seconds of the previous one. This means about 66.6% of her tweets are tightly clustered in time.
				</p>

				<pre>
				# Confidence interval using Poisson test
				conf_interval &lt;- poisson.test(within_31, length(eventDelays))$conf.int
				print(conf_interval)
				# Example output: [1] 0.5963808 0.7415144
				</pre>

				<p>
				Using a Poisson test, we compute a 95% confidence interval for the proportion of tweets that arrive within 31 seconds. The interval ranges from approximately 59.6% to 74.1%, suggesting that if we sampled another 500 tweets, we’d likely see a similar range of clustering.
				</p>

				<p>
				This wide interval reflects the uncertainty in estimating the proportion from a finite sample. While the central estimate is 66.6%, the true value could reasonably fall anywhere within that band.
				</p>


				<p>
				Using a Poisson test, we compute a 95% confidence interval for the proportion of tweets that arrive within 31 seconds. The interval ranges from approximately 59.6% to 74.1%, suggesting that if we sampled another 500 tweets, we’d likely see a similar range of clustering.
				</p>

				<p>
				This wide interval reflects the uncertainty in estimating the proportion from a finite sample. While the central estimate is 66.6%, the true value could reasonably fall anywhere within that band.
				</p>
				<p>
				Now let’s get the same data for Oprah:
				</p>

				<p>
				Import the OprahData.xlsx file into a variable named <code>tweetDF</code>.
				</p>

				<pre>
				sortweetDF &lt;- tweetDF[order(as.integer(tweetDF$created)), ]
				</pre>

				<pre>
				eventDelays &lt;- as.integer(diff(sortweetDF$created))
				</pre>

				<pre>
				mean(eventDelays)
				# Output: [1] 423.01
				</pre>

				<p>
				Hmm, I guess we know who is the boss here! Now let’s finish the job:
				</p>

				<pre>
				sum(eventDelays &lt;= 31)
				# Output: [1] 73
				</pre>

				<pre>
				poisson.test(73, 500)$conf.int
				# Output: [1] 0.1144407 0.1835731
				</pre>

				<pre>
				attr(,"conf.level")
				# Output: [1] 0.95
				</pre>

				<p>
				So, for Oprah’s tweets, the mean inter-arrival delay is much longer—about 423 seconds. Only 73 of the 500 tweets (about 14.6%) were posted within 31 seconds of each other. According to the Poisson test, the 95% confidence interval for this proportion ranges from roughly 11.4% to 18.4%, showing far less clustering than in Lady Gaga’s tweet pattern.
				</p>


				<p>
					The sum() function, above, calculates that only 73 out of Oprah’s sample of 500 tweets arrive in an interval of 31 or less. We use 31, the mean of the Lady Gaga sample, because we need to have a common basis of comparison. So for Oprah, the proportion of events that occur in the 31 second time frame is, 73/500 = 0.146, or about 14.6%. That’s a lot lower than the 66.6% of Lady Gaga tweets, for sure, but we need to look at the confidence interval around that value. So the poisson.test() function just above for Oprah reports that the 95% confidence interval runs from about 11.4% to 18.4%. Note that this confidence interval does not overlap at all with the confidence interval for Lady Gaga, so we have a very strong sense that these two rates are statistically quite distinctive in other words, this is a difference that was not caused by the random influences that sampling always creates. We can make a bar graph to summarize these differences. We’ll use the <idx><c>barplot2()</c></idx><term>barplot2()</term> function, which is in a package called <idx><c>gplots()</c></idx><term>gplots()</term>. If you created the EnsurePackage() function a couple of chapters ago, you can use that. Otherwise make sure to load gplots manually:
				</p>

				<pre>
					&gt; EnsurePackage("gplots")
				</pre>

				<pre>
					&gt; barplot2(c(0.666,0.146), +
				</pre>

				<pre>
					ci.l=c(0.596,0.114), + ci.u=c(0.742,0.184), +
				</pre>

				<pre>
					plot.ci=TRUE, + names.arg=c("Gaga","Oprah"))
				</pre>

				<p>
					This is not a particularly efficient way to use the barplots() function, because we are supplying our data by typing it in, using the c() function to create short vectors of values on the command line. On the first line, we supply a list of the means from the two samples, expressed as proportions. On the next two lines we first provide the lower limits of the confidence intervals and then the upper limits. The plot.ci=TRUE parameter asks barplot2() to put confidence interval whiskers on each bar. The final line provides labels to put underneath the bars. What we get is below.
				</p>

				<figure>
					<image source="gaga-oprah.png"/>
					<caption>Gaga vs Obra Tweets</caption>
				</figure>

				<p>
					This is not an especially attractive bar plot, but it does represent the information we wanted to display accurately. And with the assistance of this plot, it is easy to see both the substantial difference between the two bars and the fact that the confidence intervals do not overlap.
				</p>

				<p>
					For one final confirmation of our results, we can ask the poisson.text() function to evaluate our two samples together. This code provides the same information to poisson.test() as before, but now provides the event counts as short lists describing the two samples, with 333 events (under 31 seconds) for Lady Gaga and 73 events for Oprah, in both cases out of 500 events:
				</p>
<pre>
poisson.test(c(333, 73), c(500, 500))
</pre>

<pre>
Comparison of Poisson rates

data: c(333, 73) time base: c(500, 500)

count1 = 333, expected count1 = 203, p-value &lt; 2.2e-16

alternative hypothesis: true rate ratio is not equal to 1

95 percent confidence interval:

3.531401 5.960511

sample estimates:

rate ratio 4.561644
</pre>

<p>
This Poisson test compares the rate of tweet clustering (within 31 seconds) between Lady Gaga and Oprah. With a p-value less than 2.2e-16, the result is statistically significant. The estimated rate ratio is 4.56, meaning Lady Gaga is over four and a half times more likely to tweet in rapid succession than Oprah. The 95% confidence interval for the true rate ratio ranges from 3.53 to 5.96.
</p>


				<p>
					Let’s walk through this output line by line. Right after the command, we get a brief confirmation from the function that we’re comparing two event rates in this test rather than just evaluating a single rate: "Comparison of Poisson rates." The next line confirms the data we provided. The next line, that begins with "count1 = 333" confirms the basis of of the comparison and then shows a "pooled" count that is the weighted average of 333 and 73. The pvalue on that same line represents the position of a probability tail for "false positives." Together with the information on the next line, "alternative hypothesis," this constitutes what statisticians call a "null hypothesis significance test." Although this is widely used in academic research, it contains less useful information than confidence intervals and we will ignore it for now.
				</p>

				<p>
					The next line, "95% confidence interval," is a label for the most important information, which is on the line that follows. The values of 3.53 and 5.96 represent the upper and lower limits of the 95% <em>confidence interval around the observed rate ratio</em> of 4.56 (reported on the final line). So, for 95% of samples that we might draw from twitter, the ratio of the Gaga/Oprah rates might be as low as 3.53 and as high as 5.96. So we can be pretty sure (95% confidence) that Lady Gaga gets tweets at least 3.5 times as fast as Oprah. Because the confidence interval does not include 1, which would be the same thing as saying that the two rates are identical, we can be pretty certain that the observed rate ratio of 4.56 is not a statistical fluke.
				</p>

				<p>
					For this comparison, we chose two topics that had very distinctive event rates. As the bar chart on the previous page attests, there was a substantial difference between the two samples in the rates of arrival of new tweets. The statistical test confirmed this for us, and although the ability to calculate and visualize the confidence intervals was helpful, we probably could have guessed that such a large difference over a total of 1000 tweets was not a result due to sampling error.
				</p>

				<p>
					With other topics and other comparisons, however, the results will not be as clear cut. After completing the chapter challenge on the next page, we compared the "#obama" hashtag to the "#romney" hashtag. Over samples of 250 tweets each, Obama had 159 events at or under the mean, while Romney had only 128, for a ratio of 1.24 in Obama’s favor. The confidence interval told a different story, however: the lower bound of the confidence interval was 0.978, very close to, but slightly below one. This signifies that we can’t rule out the possibility that the two rates are, in fact, equal and that the slightly higher rate (1.24 to 1) that we observed for Obama in this one sample might have come about due to sampling error. When a confidence interval overlaps the point where we consider something to be a "null result" (in this case a ratio of 1:1) we have to take seriously the possibility that peculiarities of the sample(s) we drew created the observed difference, and that a new set of samples might show the opposite of what we found this time.
				</p>

				<p>
					<term>Chapter Challenge</term>
				</p>

				<p>
					Write a function that takes two search strings as arguments and that returns the results of a Poisson rate ratio test on the arrival rates of tweets on the two topics. Your function should first run the necessary Twitter searches, then sort the tweets by ascending time of arrival and calculate the two vectors of time differentials. Use the mean of one of these vectors as the basis for comparison and for each vector, count how many events are at or below the mean. Use this information and the numbers of tweets requested to run the poisson.test() rate comparison.
				</p>

			</section>

			<section xml:id="sources-7">
				<title>Sources </title>

				<p>
					<em><term>Barplots</term></em>
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph">http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph =54</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatGraphCo">http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatGraphCo urse/graphscourse.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=gplots:barplot2">http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=gplots:barplot2</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					<em><term>Poisson Distribution</term></em>
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://books.google.com/books?id=ZKswvkqhygYC&amp;printsec=fr">http://books.google.com/books?id=ZKswvkqhygYC&amp;printsec=fr ontcover</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.khanacademy.org/math/probability/v/poisson-process-1">http://www.khanacademy.org/math/probability/v/poisson-process-1</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.khanacademy.org/math/probability/v/poisson-process-2">Khan Academy Poisson Process 2</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/Poisson.html">The Poisson Distribution</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/poisson.test.html">Exact Poisson tests</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://stats.stackexchange.com/questions/10926/how-to-calculat">How to calculate confidence interval for count data in R?</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

			</section>

			<section xml:id="r-functions-used-in-this-chapter-2">
				<title>R Functions Used in this Chapter </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										as.integer() Coerces another data type to integer if possible
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										barplot2() Creates a bar graph
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										c() Concatenates items to make a list
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										diff() Calculates time difference on neighboring cases
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										EnsurePackage() Custom function, install() and require() package
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										for() Creates a loop, repeating execution of code
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										hist() Creates a frequency histogram
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										mean() Calculates the arithmetic mean
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										order() Provides a list of indices reflecting a new sort order
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										plot() Begins an X-Y plot
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										points() Adds points to a plot started with plot()
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										poisson.test() Confidence intervals for poisson events or ratios
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										qpois() Does the inverse of ppois(): Probability into threshold
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										rpois() Generates random numbers fitting a Poisson distribution
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										TweetFrame() Custom procedure yielding a dataset of tweets
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										ppois() Returns a cumulative probability for particular threshold
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										sum() Adds together a list of numbers
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										var() Calculates variance of a list of numbers
									</p>
									</blockquote>
					</li>

				</ul></p>

				<exercise label="mc-poisson-shape" xml:id="mc-poisson-shape">
					<statement>
						<p>
							The Poisson distribution has a characteristic shape that would be described as:
						</p>
					</statement>
					<choices>
						<choice>
							<statement>
								<p>
									Negatively (left) skewed
								</p>
							</statement>
							<feedback>
								<p>
									Not quite. Think about where the tail would be when events are rare.
								</p>
							</feedback>
						</choice>
						<choice correct ="yes">
							<statement>
								<p>
									Positively (right) skewed
								</p>
							</statement>
							<feedback>
								<p>
									Correct! The Poisson distribution is typically positively skewed, especially when the mean (λ) is small. As λ increases, the distribution becomes more symmetric, but it still retains a slight right skew.
								</p>
							</feedback>
						</choice>
						<choice>
							<statement>
								<p>
									Symmetric (not skewed)
								</p>
							</statement>
							<feedback>
								<p>
									No, the Poisson distribution is not symmetric, particularly when the mean is small. Only when the mean becomes large does the distribution approach symmetry, resembling a normal distribution.
								</p>
							</feedback>
						</choice>
						<choice>
							<statement>
								<p>
									None of these
								</p>
							</statement>
							<feedback>
								<p>
									Not quite. One of the listed options correctly describes the typical shape of the Poisson distribution, especially when the mean is low. Consider the direction of skewness in the distribution.
								</p>
							</feedback>
						</choice>
					</choices>
				</exercise>

			</section>

						<section xml:id="r-script-create-vector-of-probabilities-from-delay-times">
				<title> <term>R Script Create Vector of Probabilities From Delay Times</term> </title>

					<p>
					<sage language="r">
						<input>
							&#35; Like ArrivalProbability, but works with unsorted list of delay times
							DelayProbability &lt;- function(delays, increment, max) &#123;
							&#35; Initialize an empty vector
							plist &lt;- NULL

							&#35; Probability is defined over the size of this sample of arrival times
							delayLen &lt;- length(delays)
							&#35; Probability is defined over the size of this sample of arrival times
							delayLen &lt;- length(delays)

							&#35; May not be necessary, but checks for input mistake
							if (increment &gt; max) &#123;
								return(NULL)
							&#125;

							for (i in seq(increment, max, by = increment)) &#123;
								&#35; logical test &lt;= i provides list of TRUEs and FALSEs
								&#35; of length = timeLen, then sum() counts the TRUEs
								plist &lt;- c(plist, (sum(delays &lt;= i) / delayLen))
							&#125;

							return(plist)
							&#125;
						</input>
					</sage>
					</p>



      		</section>
</chapter>