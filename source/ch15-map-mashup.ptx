<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch15-map-mashUp" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Map MashUp</title>

  <section xml:id="ch15-intro-mashup">
    <title>What is a Mashup?</title>

    <introduction>
      <p>
        <image source="topo-cover.png"/>
        <em>Much of what we have accomplished so far has focused on the standard rectangular dataset: one neat table with rows and columns well defined. Yet much of the power of data science comes from bringing together different sources of data in complementary ways. In this chapter we combine different sources of data to make a unique product that transcends any one source.</em>
      </p>
    </introduction>

    <p>
      The term <idx>mashup</idx><term>mashup</term> originated in the music business decades ago related to the practice of overlaying one music recording on top of another one. The term has entered general usage to mean anything that brings together disparate influences or elements. In the application development area, mashup often refers to bringing together various sources of data to create a new product with unique value. There's even a non-profit group called the Open Mashup Alliance that develops standards and methods for creating new mashups.
    </p>

    <p>
      One example of a mashup is <url href="http://www.housingmaps.com">HousingMaps</url>, a web application that grabs apartment rental listings from the classified advertising service Craigslist and plots them on an interactive map that shows the location of each listing. If you have ever used Craigslist you know that it provides a very basic text-based interface and that the capability to find listings on a map would be welcome.
    </p>

    <p>
      In this chapter we tackle a similar problem. Using some address data from government records, we call the Google <idx>geocoding</idx><term>geocoding</term> API over the web to find the latitude and longitude of each address. Then we plot these latitudes and longitudes on a map of the U.S. This activities reuses skills we learned in the previous chapter for reading in data files, adds some new skills related to calling web APIs, and introduces us to a new type of data, namely the shapefiles that provide the basis for electronic maps.
    </p>
  </section>

  <section xml:id="ch15-map-data">
    <title>Working with Map Data in R</title>
    <p>
      Let's look at the new stuff first. The Internet is full of <idx>shapefiles</idx><term>shapefiles</term> that contain mapping data. Shapefiles are a partially proprietary, partially open format supported by a California software company called SRI. Shapefile is actually an umbrella term that covers several different file types. Because the R community has provided some packages to help deal with shapefiles, we don't need too much information about the details. The most important thing to know is that shapefiles contain points, polygons, and "<idx>polylines</idx><term>polylines</term>." Everyone knows what a point and a polygon are, but a polyline is a term used by computer scientist to refer to a multi-segment line. In many graphics applications it is much easier to approximate a curved line with many tiny connected straight lines than it is to draw a truly curved line. If you think of a road or a river on a map, you will have a good idea of a polyline.
    </p>

    <p>
    The U.S. Census Bureau provides shapefiles at various levels of detail for geographic boundaries across the United States. These files are useful for mapping everything from states to voting districts. If you'd like to explore them yourself,  visit the <url href="https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html">Cartographic Boundary Files - Shapefile</url> URL. These downloads typically come as a .zip archive containing several files: the main shapefile (.shp) and supporting files like the .dbf, which includes labels and metadata.

    In this example, we’ve already loaded a simplified shapefile of U.S. congressional districts into the variable <c>usShape</c> for convenience. If you’d prefer to work with it yourself, you can download the same file, unzip it, and load it into R.
    </p>

    <p>
      To get started, we’ll need two R packages: <idx>ggplot2</idx><c>ggplot2</c> and <idx>sf</idx><c>sf</c>. The <c>sf</c> package, which stands for simple features, provides a standardized and modern approach to handling spatial data in R. Developed by Edzer Pebesma and others, it follows the Simple Features standard (SFA), making spatial data manipulation easier and more consistent. This package has largely replaced older mapping tools like <c>maptools</c>, offering a simpler, more powerful interface for working with geographic data. The <c>ggplot2</c> package, developed by Hadley Wickham, is part of the tidyverse and provides a powerful, layered grammar for creating graphics in R. While not specifically built for mapping, <c>ggplot2</c> works seamlessly with spatial data, especially when combined with the <c>sf</c> package,to produce clear and customizable visualizations. In this chapter, we only scratch the surface of what modern mapping libraries can do; entire books have been written on their capabilities for data visualization.
    </p>


    <sage language="r">
      <input>
        <xi:include href="../code/usShape.r" parse ="text"/>
summary(usShape)
library(ggplot2)
ggplot(usShape) +
geom_sf(fill = "lightblue", color = "black") +
coord_sf(xlim = c(-185, -50), ylim = c(15, 75), expand = FALSE)
        

      </input>
    </sage>
    

    <p>
      This last command gives us a simple plot of the amount of shapes that our shapefile contains. Here is the plot:
    </p>

    <figure>
      <image source="graph-US-squash.png"/>
    </figure>

    <p>
      This is funny looking! The ranges output from the summary() command gives us a hint as to why. The longitude of the elements in our map range from -179 to nearly 180: this covers pretty much the whole of the planet. The reason is that the map contains shapes for Hawaii and Alaska. Both states have far western longitudes, but the Aleutian peninsula in Alaska extends so far that it crosses over the longitude line where -180 and 180 meet in the Pacific Ocean. As a result, the continental U.S. is super squashed. We can specify a more limited area of the map to consider by using the xlim and ylim parameters. The following command:
    </p>
    <sage language="r">
      <input>
        <xi:include href="../code/usShape.r" parse ="text"/>
library(ggplot2)
ggplot(usShape) +
geom_sf(fill = "lightblue", color = "black") +
coord_sf(xlim = c(-130, -65), ylim = c(20, 55), expand = FALSE)
      </input>
    </sage>

    <p>
      ...gives a plot that shows the continental U.S. more in its typical proportions.
    </p>

    <p>
      <image source="graph2-US.png"/> So now we have some map data stored away and ready to use. The PBSmapping package gives us the capability of adding points to an existing map. For now, let's demonstrate this with a made up point somewhere in Texas:
    </p>

    <sage language = "r">
      <input>
        <xi:include href="../code/usShape.r" parse ="text"/>
library(ggplot2)

# Create a simple point at (100, 30)
pointData &lt;- data.frame(EID = 1, X = -100, Y = 30)
point_sf &lt;- st_as_sf(pointData, coords = c("X", "Y"), crs = 4326)

# Combine map and point using ggplot2
ggplot(usShape) +
geom_sf(fill = "lightblue", color = "black") +
geom_sf(data = point_sf, color = "red", size = 3) +
coord_sf(xlim = c(-130, -50), ylim = c(20, 55), expand = FALSE) +
theme_minimal()

      </input>
    </sage>

    <figure>
      <image source="graph-US.png"/>
    </figure>

    <p>
      You might notice a small red dot in southern Texas. We added that by manually creating a point at a specific longitude and latitude—X = -100, Y = 30. First, we placed this information into a data frame, then converted it into a spatial object using the sf package, which is the modern standard for working with geospatial data in R.
    </p>

    <p>
      In this case, we used st_as_sf() to turn our data into a spatial feature object. This allowed us to plot the point directly on the map alongside the U.S. congressional district boundaries. Because both the point and the map use the same geographic coordinate system (WGS84), we didn’t need to worry about applying a projection or transformation, the point appears exactly where it should on the map.
    </p>

    <note>
      <p>Also note that we didn’t need to transform the point using a mapping projection. Since both the point and the map use geographic coordinates (longitude and latitude) in the WGS84 system, everything aligns correctly without extra steps. If you recall from geography, a projection is a way to represent the curved surface of the Earth on a flat map. But in this case, the map is already using a standard flattened projection, so our point appears in the correct location without needing further adjustment.</p>
    </note>
  </section>

  <section xml:id="ch15-data-acquisition">
    <title>Acquiring and Preparing Address Data</title>
    <p>
      Next, we need a source of points to add to our map. This could be anything that we're interested in: the locations of restaurants, crime scenes, colleges, etc. In Google a search for filetype:xls or filetype;csv with appropriate additional search terms can provide interesting data sources. You may also have mailing lists of customers or clients. The most important thing is that we will need street address, city, and state in order to geocode the addresses. For this example, we searched for "housing street address list filetype:csv", and this turned up a data set of small businesses that have contracts with the U.S. Department of Health and Human services. Let's read this in using read.csv():
    </p>

    <p>
      &gt; dhhsAddrs &lt;read.csv("DHHS_Contracts.csv")
    </p>

    <p>
      &gt; str(dhhsAddrs)
    </p>

    <p>
      'data.frame': 599 obs. of 10 variables:
    </p>

    <p>
      $ Contract.Number : Factor w/ 285 levels "26301D0054","500000049",..: 125 125 125 279 164 247 19 242 275 70 ...
    </p>

    <p>
      $ Contractor.Name : Factor w/ 245 levels "2020 COMPANY LIMITED LIABILITY COMPANY",..: 1 1 1 2 2 3 4 6 5 7 ...
    </p>

    <p>
      $ Contractor.Address : Factor w/ 244 levels "1 CHASE SQUARE 10TH FLR, ROCHESTER, NY ",..: 116 116 116 117 117 136 230 194 64 164 ...
    </p>

    <p>
      $ Description.Of.Requirement: Factor w/ 468 levels "9TH SOW DEFINTIZE THE LETTER CONTRACT",..: 55 55 55 292 172 354 308 157 221 340 ...
    </p>

    <p>
      $ Dollars.Obligated : Factor w/ 586 levels " $1,000,000.00 ",..: 342 561 335 314 294 2 250 275 421 21 ...
    </p>

    <p>
      $ NAICS.Code : Factor w/ 55 levels "323119","334310",..: 26 26 26 25 10 38 33 29 27 35 ...
    </p>

    <p>
      $ Ultimate.Completion.Date : Factor w/ 206 levels "1-Aug-2011","1-Feb-2013",..: 149 149 149 10 175 161 124 37 150 91 ...
    </p>

    <p>
      $ Contract.Specialist : Factor w/ 95 levels "ALAN FREDERICKS",..: 14 14 60 54 16 90 55 25 58 57 ...
    </p>

    <p>
      $ Contract.Specialist.Email : Factor w/ 102 levels "410-786-8622",..: 16 16 64 59 40 98 60 29 62 62 ... <sub>$ X : logi NA NA NA NA NA NA ...</sub>
    </p>

    <p>
      It sounds like a crazy 60s song again! Anyhow, we read in a comma separated data set with 599 rows and 10 variables. The most important field we have there is Contractor.Address. This contains the street addresses that we need to geocode. We note, however, that the data type for these is Factor rather than character string. So we need to convert that:
    </p>

    <p>
      &gt; dhhsAddrs$strAddr &lt;+
    </p>

    <p>
      as.character(dhhsAddrs$Contractor.Address)
    </p>

    <p>
      &gt; mode(dhhsAddrs$strAddr)
    </p>

    <p>
      [1] "character"
    </p>

    <p>
      &gt; tail(dhhsAddrs$strAddr,4)
    </p>

    <p>
      [1] "1717 W BROADWAY, MADISON, WI "
    </p>

    <p>
      [2] "1717 W BROADWAY, MADISON, WI "
    </p>

    <p>
      [3] "1717 W BROADWAY, MADISON, WI "
    </p>

    <p>
      [4] "789 HOWARD AVE, NEW HAVEN, CT, "
    </p>

    <p>
      This looks pretty good: Our new column, called dhhsAddrs, is character string data, converted from the factor labels of the original Contractor.Address column. Now we need to geocode these.
    </p>
  </section>

  <section xml:id="ch15-geocoding-api">
    <title>Using the Google Geocoding Web API</title>
    <p>
      We will use the Google geocoding application programming interface (API) which is pretty easy to use, does not require an account or application ID, and allows about 2500 address conversions per day. The API can be accessed over the web, using what is called an HTTP GET request. Note that the Terms of Service for the Google geocoding API are very specific about how the interface can be used most notably on the point that the geocodes must be used on Google maps. Make sure you read the Terms of Service before you create any software applications that use the geocoding service. See the link in the bibliography at the end of the chapter. The bibliography has a link to an article with dozens of other geocoding APIs if you disagree with Google's Terms of Service.
    </p>

    <p>
      These acronyms probably look familiar. HTTP is the Hyper Text Transfer Protocol, and it is the standard method for requesting and receiving web page data. A GET request consists of information that is included in the URL string to specify some details about the information we are hoping to get back from the request. Here is an example GET request to the Google geocoding API:
    </p>

    <p>
      <url href="http://maps.googleapis.com/maps/api/geocode/json?address=1">http://maps.googleapis.com/maps/api/geocode/json?address=1 600+Pennsylvania+Avenue,+Washington,+DC&amp;sensor=false</url>
    </p>

    <p>
      The first part of this should look familiar: The <url href="http://maps.googleapis.com">http://maps.googleapis.com</url> part of the URL specifies the domain name just like a regular web page. The next part of the URL, "/maps/api/geocode" tells Google which API we want to use. Then the "json" indicates that we would like to receive our result in "Java Script Object Notation" (JSON) which is a structured, but human readable way of sending back some data. The address appears next, and we are apparently looking for the White House at 1600 Pennsylvania Avenue in Washington, DC. Finally, sensor=false is a required parameter indicating that we are not sending our request from a mobile phone. You can type that whole URL into the address field of any web browser and you should get a sensible result back. The JSON notation is not beautiful, but you will see that it makes sense and provides the names of individual data items along with their values. Here's a small excerpt that shows the key parts of the data object that we are trying to get our hands on:
    </p>

    <p>
      {
    </p>

    <p>
      "results" : [
    </p>

    <p>
      {
    </p>

    <p>
      "address_components" : [
    </p>

    <p>
      "geometry" : {
    </p>

    <p>
      "location" : {
    </p>

    <p>
      "lat" : 38.89788009999999,
    </p>

    <p>
      "lng" : -77.03664780
    </p>

    <p>
      },
    </p>

    <p>
      "status" : "OK"
    </p>

    <p>
      }
    </p>

    <p>
      There's tons more data in the JSON object that Google returned, and fortunately there is an R package, called <idx>JSONIO</idx><term>JSONIO</term>, that will extract the data we need from the structure without having to parse it ourselves. In order to get R to send the HTTP GET requests to google, we will also need to use the <idx>RCurl</idx><term>RCurl</term> package. This will give us a single command to send the request and receive the result back, essentially doing all of the quirky steps that a web browser takes care of automatically for us. To get started, install.packages() and library() the two packages we will need RCurl and JSONIO.
    </p>
  </section>

  <section xml:id="ch15-building-functions">
    <title>Building the Geocoding Functions in R</title>
    <p>
      Next, we will create a new helper function to take the address field and turn it into the URL that we need:
    </p>

    <p>
      # Format an URL for the Google Geocode API
    </p>

    <p>
      MakeGeoURL &lt;- function(address)
    </p>

    <p>
      {
    </p>

    <p>
      root &lt;- "http://maps.google.com/maps/api/geocode/"
    </p>

    <p>
      url &lt;- paste(root, "json?address=", +
    </p>

    <p>
      address, "&amp;sensor=false", sep = "")
    </p>

    <p>
      return(URLencode(url))
    </p>

    <p>
      }
    </p>

    <p>
      There are three simple steps here. The first line initializes the beginning part of the URL into a string called root. Then we use paste() to glue together the separate parts of the strong (note the sep="" so we don't get spaces between the parts). This creates a string that looks almost like the one in the White House example two pages ago. The final step converts the string to a legal URL using a utility function called URLencode() that RCurl provides. Let's try it:
    </p>

    <p>
      &gt; MakeGeoURL( + "1600 Pennsylvania Avenue, Washington, DC")
    </p>

    <p>
      [1] "http://maps.google.com/maps/api/geocode/json?add ress=1600%20Pennsylvania%20Avenue,%20Washington,% 20DC&amp;sensor=false"
    </p>

    <p>
      Looks good! Just slightly different than the original example (%20 instead of the plus character) but hopefully that won't make a difference. Remember that you can type this function at the command line or you can create it in the script editing window in the upper left hand pane of R-Studio. The latter is the better way to go and if you click the "Source on Save" checkmark, R-Studio will make sure to update R's stored version of your function every time you save the script file.
    </p>

    <p>
      Now we are ready to use our new function, MakeGeoURL(), in another function that will actually request the data from the Google API:
    </p>

    <p>
      Addr2latlng &lt;- function(address)
    </p>

    <p>
      {
    </p>

    <p>
      url &lt;- MakeGeoURL(address)
    </p>

    <p>
      apiResult &lt;- getURL(url)
    </p>

    <p>
      geoStruct &lt;- fromJSON(apiResult, +
    </p>

    <p>
      simplify = FALSE)
    </p>

    <p>
      lat &lt;- NA
    </p>

    <p>
      lng &lt;- NA
    </p>

    <p>
      try(lat &lt;- +
    </p>

    <p>
      geoStruct$results[[1]]$geometry$location$lat)
    </p>

    <p>
      try(lng &lt;- +
    </p>

    <p>
      geoStruct$results[[1]]$geometry$location$lng)
    </p>

    <p>
      return(c(lat, lng))
    </p>

    <p>
      }
    </p>

    <p>
      We have defined this function to receive an address string as its only argument. The first thing it does is to pass the URL string to MakeGeoURL() to develop the formatted URL. Then the function passes the URL to getURL(), which actually does the work of sending the request out onto the Internet. The getURL() function is part of the RCurl package. This step is just like typing a URL into the address box of your browser.
    </p>

    <p>
      We capture the result in an object called "apiResult". If we were to stop and look inside this, we would find the JSON structure that appeared a couple of pages ago. We can pass this structure to the function fromJSON() we put the result in an object called geoStruct. This is a regular R data frame such that we can access any individual element using regular $ notation and the array index [[1]]. In other instances, a JSON object may contain a whole list of data structures, but in this case there is just one. If you compare the variable names "geometry", "location", "lat" and "lng" to the JSON example a few pages ago you will find that they match perfectly. The fromJSON() function in the JSONIO package has done all the heavy lifting of breaking the JSON structure into its component pieces.
    </p>

    <p>
      Note that this is the first time we have encountered the try() function. When programmers expect the possibility of an error, they will frequently use methods that are tolerant of errors or that catch errors before they disrupt the code. If our call to getURL() returns something unusual that we aren't expecting, then the JSON structure may not contain the fields that we want. By surrounding our command to assign the lat and lng variables with a try() function, we can avoid stopping the flow of the code if there is an error. Because we initialized lat and lng to NA above, this function will return a two item list with both items being NA if an error occurs in accessing the JSON structure. There are more elegant ways to accomplish this same goal. For example, the Google API puts an error code in the JSON structure and we could choose to interpret that instead. We will leave that to the chapter challenge!
    </p>

    <p>
      In the last step, our new Addr2latlng() function returns a two item list containing the latitude and longitude. We can test it out right now:
    </p>

    <p>
      &gt; testData &lt;- Addr2latlng( +
    </p>

    <p>
      "1600 Pennsylvania Avenue, Washington, DC")
    </p>

    <p>
      &gt; str(testData)
    </p>

    <p>
      num [1:2] 38.9 -77
    </p>

    <p>
      Perfect! we called our new function Addr2latlng() with the address of the Whitehouse and got back a list with two numeric items containing the latitude and longitude associated with that address. With just a few lines of R code we have harnessed the power of Google's extensive geocoding capability to convert a brief text street address into mapping coordinates.
    </p>
  </section>

  <section xml:id="ch15-final-map">
    <title>Executing the Mashup and Visualizing Results</title>
    <p>
      At this point there isn't too much left to do. We have to create a looping mechanism so that we can process the whole list of addresses in our DHHS data set. We have some small design choices here. It would be possible to attach new fields to the existing dataframe. Instead, the following code keeps everything pretty simple by receiving a list of addresses and returning a new data frame with X, Y, and EID ready to feed into our mapping software:
    </p>

    <p>
      # Process a whole list of addresses
    </p>

    <p>
      ProcessAddrList &lt;- function(addrList)
    </p>

    <p>
      {
    </p>

    <p>
      resultDF &lt;- data.frame(atext=character(), +
    </p>

    <p>
      X=numeric(),Y=numeric(),EID=numeric())
    </p>

    <p>
      i &lt;- 1
    </p>

    <p>
      for (addr in addrList)
    </p>

    <p>
      {
    </p>

    <p>
      latlng = Addr2latlng(addr)
    </p>

    <p>
      resultDF &lt;- rbind(resultDF,+
    </p>

    <p>
      data.frame(atext=addr, + X=latlng[[2]],Y=latlng[[1]], EID=i))
    </p>

    <p>
      i &lt;- i + 1
    </p>

    <p>
      }
    </p>

    <p>
      return(resultDF)
    </p>

    <p>
      }
    </p>

    <p>
      This new function takes one argument, the list of addresses, which should be character strings. In the first step we make an empty dataframe for use in the loop. In the second step we initialize a scalar index variable called i to the number 1. We will increment this in the loop and use it as our EID.
    </p>

    <p>
      Then we have the for loop. We are using a neat construct here called "in". The expression "addr in addrList" creates a new variable called addr. Every time that R goes through the loop it assigns to addr the next item in addrList. When addrList runs out of items, the for loop ends. Very handy!
    </p>

    <p>
      Inside the for loop the first thing we do is to call the function that we developed earlier: Addr2latlng(). This performs one conversion of an address to a geocode (latitude and longitude) as described earlier. We pass addr to it as add contains the text address for this time around the loop. We put the result in a new variable called latlng. Remember that this is a two item list.
    </p>

    <p>
      The next statement, starting with "resultDF &lt;- rbind" is the heart of this function. Recall that rbind() sticks a new row on the end of a dataframe. So in the arguments to rbind() we supply our earlier version of resultDF (which starts empty and grows from there) plus a new row of data. The new row of data includes the text address (not strictly necessary but handy for diagnostic purposes), the "X" that our mapping software expects (this is the longitude), the "Y" that the mapping software expects, and the event ID, EID, that the mapping software expects.
    </p>

    <p>
      At the end of the for loop, we increment the index i so that we will have the next number next time around for EID. Once the for loop is done we simply return the dataframe object, resultDF. Piece of cake!
    </p>

    <p>
      Now let's try it and plot our points:
    </p>

    <p>
      &gt; dhhsPoints &lt;- ProcessAddrList(dhhsAddrs$strAddr)
    </p>

    <p>
      &gt; dhhsPoints &lt;- dhhsPoints[!is.na(dhhsPoints$X),]
    </p>

    <p>
      &gt; eventData &lt;- as.EventData(dhhsPoints,projection=NA)
    </p>

    <p>
      &gt; addPoints(eventData,col="red",cex=.5)
    </p>

    <p>
      The second command above is the only one of the four that may seem unfamiliar. The as.EventData() coercion is picky and will not process the dataframe if there are any fields that are NA. To get rid of those rows that do not have complete latitude and longitude data, we use is.na() to test whether the X value on a given row is NA. We use the ! (not) operator to reverse the sense of this test. So the only rows that will survive this step are those where X is not NA. The plot below shows the results.
    </p>

    <figure>
      <image source="graph-US-sites.png"/>
    </figure>

    <p>
      If you like conspiracy theories, there is some support in this graph: The vast majority of the companies that have contracts with DHHS are in the Washington, DC area, with a little trickle of additional companies heading up the eastern seaboard as far as Boston and southern New Hampshire. Elsewhere in the country there are a few companies here and there, particularly near the large cities of the east and south east.
    </p>
  </section>

  <section xml:id="ch15-chapter-exercises">
    <title>Test Yourself Map MashUp</title>
      <exercise label="mc-mashup-geocoding">
        <statement>
            <p>What is the primary advantage of using a mashup (like combining address data with Google's geocoding API) compared to analyzing a single dataset?</p>
        </statement>
        <choices>
            <choice>
                <statement>
                    <p>It reduces the need for data cleaning since APIs automatically standardize formats.</p>
                </statement>
                <feedback>
                    <p>Not quite. While APIs can help, the chapter emphasizes that mashups <strong>combine complementary data sources</strong> (e.g., addresses + maps) to create new value, not just simplify cleaning.</p>
                </feedback>
            </choice>
            <choice correct="yes">
                <statement>
                    <p>It creates a new product with unique value by integrating disparate data sources (e.g., addresses + maps).</p>
                </statement>
                <feedback>
                    <p>Correct! The chapter defines mashups as bringing together different data sources (like Craigslist listings + maps) to transcend the limitations of any single source (Section 15.1).</p>
                </feedback>
            </choice>
            <choice>
                <statement>
                    <p>It eliminates the need for statistical analysis since visualizations (e.g., maps) replace calculations.</p>
                </statement>
                <feedback>
                    <p>Incorrect. Mashups <strong>augment</strong> analysis (e.g., geocoding enables spatial visualization), but don’t replace statistical methods (Section 15.3–15.5).</p>
                </feedback>
            </choice>
            <choice>
                <statement>
                    <p>It guarantees higher data accuracy because Google’s API is more reliable than local datasets.</p>
                </statement>
                <feedback>
                    <p>Nope. The chapter notes that APIs like Google’s have <strong>usage limits</strong> (2,500/day) and legal terms (Section 15.4), but doesn’t claim they’re inherently more accurate.</p>
                </feedback>
            </choice>
        </choices>
    </exercise>

      <p>Chapter Challenge:</p>
      <p>
        Improve the Addr2latlng() function so that it checks for errors returned from the Google API. This will require going to the Google API site, looking more closely at the JSON structure, and reviewing the error codes that Google mentions in the documentation. You may also learn enough that you can repair some of the addresses that failed.
      </p>
      <p>
        Using some of the parameters on plotPolys() you could adjust the zoom level and look at different parts of the country in more detail. If you remember that the original DHHS data also had the monetary size of the contract in it, it would also be interesting to change the size or color of the dots depending on the amount of money in the Dollars.Obligated field. This would require running addPoints() in a loop and setting the col= or cex= parameters for each new point.
      </p>
      <p>
        If you get that far and are still itching for another challenge, try improving the map. One idea for this was mentioned above: you could change the size or color of the dots based on the size of the contract received. An even better (and much harder) idea would be to sum the total dollar amount being given within each state and then color each state according to how much DHHS money it receives. This would require delving into the shapefile data quite substantially so that you could understand how to find the outline of a state and fill it in with a color.
      </p>
  </section>

  <section xml:id="ch15-r-functions">
    <title>R Functions Used in This Chapter</title>
    <p>
      <ul>
        <li>
          
            <p>
              Addr2latlng() A custom function built for this chapter
            </p>
          
        </li>
        <li>
          
            <p>
              addPoints() Place more points on an existing plot
            </p>
          
        </li>
        <li>
          
            <p>
              as.character() Coerces data into a character string
            </p>
          
        </li>
        <li>
          
            <p>
              as.EventData() Coerce a regular dataframe into an EventData object for use with PBSmapping routines.
            </p>
          
        </li>
        <li>
          
            <p>
              data.frame() Takes individual variables and ties them together
            </p>
          
        </li>
        <li>
          
            <p>
              for() Runs a loop, iterating a certain number of times depending upon the expression provided in parentheses
            </p>
          
        </li>
        <li>
          
            <p>
              fromJSON() Takes JSON data as input and provides a regular R dataframe as output
            </p>
          
        </li>
        <li>
          
            <p>
              function() Creates a new function
            </p>
          
        </li>
        <li>
          
            <p>
              importShapeFile() Gets shapefile data from a set of ESRI compatible polygon data files
            </p>
          
        </li>
        <li>
          
            <p>
              MakeGeoURL() Custom helper function built for this chapter
            </p>
          
        </li>
        <li>
          
            <p>
              paste() Glues strings together
            </p>
          
        </li>
        <li>
          
            <p>
              plotPolys() Plots a map from shape data
            </p>
          
        </li>
        <li>
          
            <p>
              rbind() Binds new rows on a dataframe
            </p>
          
        </li>
        <li>
          
            <p>
              return() Specifies the object that should be returned from a function
            </p>
          
        </li>
        <li>
          
            <p>
              URLencode() Formats a character string so it can be used in an HTTP request
            </p>
          
        </li>
      </ul>
    </p>
  </section>

</chapter>