<?xml version="1.0" encoding="UTF-8" ?>
<!-- Generated by Pandoc using pretext.lua -->
<pretext>
<article>



	<subsubsection xml:id="version-4-an-introduction-to-data-science">
		<title>VERSION 4: AN INTRODUCTION TO<!-- linebreak -->Data Science</title>


		<subsubsection xml:id="by-jan-pearce-berea-college-v4-2020-by-jan-pearce">
			<title>by Jan Pearce, Berea College<!-- linebreak --><!-- linebreak --><term>© V4 2020 by Jan Pearce</term></title>

			<p>
				Forked from © V3 2012, 2013 by Jeffrey Stanton, Portions © 2013 by Robert De Graaf
			</p>

			<figure>
	<image source="media/image20.png"/>
				<caption>A world cloud of the words in this textbook created by Jan Pearce on May 7, 2020 at https://wordart.com/</caption>
</figure>


			<paragraphs xml:id="section">
				<title></title>


				<paragraphs xml:id="open-source-license">
					<title>Open Source License</title>

					<p>
						This book is distributed under the Creative Commons AttributionNonCommercial-ShareAlike 3.0 license. You are free to copy, distribute, and transmit this work. You are free to add or adapt the work. You must attribute the work to the author(s) listed above. You may not use this work or derivative works for commercial purposes. If you alter, transform, or build upon this work you may distribute the resulting work only under the same or similar license. For additional details, please see: <url href="http://creativecommons.org/licenses/by-nc-sa/3.0/">http://creativecommons.org/licenses/by-nc-sa/3.0/</url>
					</p>

				</paragraphs>

				<paragraphs xml:id="a-brief-history-of-this-book">
					<title>A Brief History of This Book</title>

					<p>
						The original version of this book was developed for the Certificate of Data Science program at Syracuse University’s School of Information Studies by the original author, Jeffrey Stanton. A PDF version of version 3 of this book and code examples used in version 3 of the book were once available at: <url href="http://jsresearch.net/wiki/projects/teachdatascience">http://jsresearch.net/wiki/projects/teachdatascience</url> and were forked to create this version. The material provided in this book is provided "as is" with no warranty or guarantees with respect to its accuracy or suitability for any purpose. The original authors thanked Ashish Verma for help with revisions to the chapter on Twitter.
					</p>

					<p>
						Preface<!-- linebreak -->Data Science: Many Skills
					</p>

					<p>
						<em><term>Data Science refers to an emerging area of work concerned with the collection, preparation, analysis, visualization, management, and preservation of large collections of information. Although the name Data Science seems to connect most strongly with areas such as databases and computer science, many different kinds of skills including non-mathematical skills are needed.</term></em>
					</p>

				</paragraphs>
			</paragraphs>
		</subsubsection>

		<subsection xml:id="overview">
			<title>Overview </title>

			<p><ol>
				<li>
							<blockquote>
											<p>
								Data science includes data analysis as an important component of the skill set required for many jobs in this area, but is not the only necessary skill.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								A brief case study of a supermarket point of sale system illustrates the many challenges involved in data science work.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								Data scientists play active roles in the design and implementation work of four related areas: data architecture, data acquisition, data analysis, and data archiving.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								Key skills highlighted by the brief case study include communication skills, data analysis skills, and ethical reasoning skills.
							</p>
							</blockquote>
				</li>

			</ol></p>

			<p>
				<em>Word frequencies from the definitions in a Shakespeare glossary in a bar graph format aka a Pareto graph format where the height of the bar indicates the word frequency.</em><image source='media/image48.png'/>
			</p>

			<figure>
	<image source="media/image20.png"/>
				<caption>A world cloud of the words in this textbook created by Jan Pearce on May 7, 2020 at https://wordart.com/</caption>
</figure>

			<p>
				<em>Word frequencies of this text book in a word cloud format where the relative size of the word indicates its frequency. While professional data scientists do need skills with mathematics and statistics, much of the data in the world is unstructured and non-numeric.</em>
			</p>

			<p>
				For some, the term "Data Science" evokes images of statisticians in white lab coats staring fixedly at blinking computer screens filled with scrolling numbers. Nothing could be further from the truth. First of all, statisticians do not wear lab coats: this fashion statement is reserved for biologists, doctors, and others who have to keep their clothes clean in environments filled with unusual fluids. Second, much of the data in the world is non-numeric and unstructured. In this context, unstructured means that the data are not arranged in neat rows and columns. Think of a web page full of photographs and short messages among friends: very few numbers to work with there. While it is certainly true that companies, schools, and governments use plenty of numeric information, sales of products, grade point averages, and tax assessments are a few examples there is lots of other information in the world that mathematicians and statisticians look at and cringe. So, while it is always useful to have great math skills, there is much to be accomplished in the world of data science for those of us who are presently more comfortable working with words, lists, photographs, sounds, and other kinds of information.
			</p>

			<p>
				In addition, data science is much more than simply analyzing data. There are many people who enjoy analyzing data and who could happily spend all day looking at histograms and averages, but for those who prefer other activities, data science offers a range of roles and requires a range of skills. Let’s consider this idea by thinking about some of the data involved in buying a box of cereal.
			</p>

			<p>
				Whatever your cereal preferences are: fruity, chocolaty, fibrous, or nutty you prepare for the purchase by writing "cereal" on your grocery list. Already your planned purchase is a piece of data, albeit a pencil scribble on the back on an envelope that only you can read. When you get to the grocery store, you use your data as a reminder to grab that jumbo box of FruityChocoBoms off the shelf and put it in your cart. At the checkout line the cashier scans the barcode on your box and the cash register logs the price. Back in the warehouse, a computer tells the stock manager that it is time to request another order from the distributor, as your purchase was one of the last boxes in the store. You also have a coupon for your big box and the cashier scans that, giving you a predetermined discount. At the end of the week, a report of all the scanned manufacturer coupons gets uploaded to the cereal company so that they can issue a reimbursement to the grocery store for all of the coupon discounts they have handed out to customers. Finally, at the end of the month, a store manager looks at a colorful collection of pie charts showing all of the different kinds of cereal that were sold, and on the basis of strong sales of fruity cereals, decides to offer more varieties of these on the store’s limited shelf space next month.
			</p>

			<p>
				So the small piece of information that began as a scribble on your grocery list ended up in many different places, but most notably on the desk of a manager as an aid to decision making. On the trip from your pencil to the manager's desk, the data went through many transformations. In addition to the computers where the data may have stopped by or stayed on for the long term, lots of other pieces of hardware such as the barcode scanner were involved in collecting, manipulating, transmitting, and storing the data. In addition, many different pieces of software were used to organize, aggregate, visualize, and present the data. Finally, many different "human systems" were involved in working with the data. People decided which systems to buy and install, who should get access to what kinds of data, and what would happen to the data after its immediate purpose was fulfilled. The personnel of the grocery chain and its partners made a thousand other detailed decisions and negotiations before the scenario described above could become reality.
			</p>

			<p>
				Obviously data scientists are not involved in all of these steps. Data scientists don’t design and build computers or barcode readers, for instance. So where would the data scientists play the most valuable role? Generally speaking, data scientists play the most active roles in the four A’s of data: data architecture, data acquisition, data analysis, and data archiving. Using our cereal example, let’s look at them one by one. First, with respect to architecture, it was important in the design of the "point of sale" system (what retailers call their cash registers and related gear) to think through in advance how different people would make use of the data coming through the system. The system architect, for example, had a keen appreciation that both the stock manager and the store manager would need to use the data scanned at the registers, albeit for somewhat different purposes. A data scientist would help the system architect by providing input on how the data would need to be routed and organized to support the analysis, visualization, and presentation of the data to the appropriate people.
			</p>

			<p>
				Next, acquisition focuses on how the data are collected, and, importantly, how the data are represented prior to analysis and presentation. For example, each barcode represents a number that, by itself, is not very descriptive of the product it represents. At what point after the barcode scanner does its job should the number be associated with a text description of the product or its price or its net weight or its packaging type? Different barcodes are used for the same product (for example, for different sized boxes of cereal). When should we make note that purchase X and purchase Y are the same product, just in different packages? Representing, transforming, grouping, and linking the data are all tasks that need to occur before the data can be profitably analyzed, and these are all tasks in which the data scientist is actively involved.
			</p>

			<p>
				The analysis phase is where data scientists are most heavily involved. In this context we are using analysis to include summarization of the data, using portions of data (samples) to make inferences about the larger context, and visualization of the data by presenting it in tables, graphs, and even animations. Although there are many technical, mathematical, and statistical aspects to these activities, keep in mind that the ultimate audience for data analysis is always a person or people. These people are the "data users," and fulfilling their needs is the primary job of a data scientist. This point highlights the need for excellent communication skills in data science. The most sophisticated statistical analysis ever developed will be useless unless the results can be effectively communicated to the data user.
			</p>

			<p>
				Finally, the data scientist must become involved in the archiving of the data. Preservation of collected data in a form that makes it highly reusable what you might think of as "data curation" is a difficult challenge because it is so hard to anticipate all of the future uses of the data. For example, when the developers of Twitter were working on how to store tweets, they probably never anticipated that tweets would be used to pinpoint earthquakes and tsunamis, but they had enough foresight to realize that "geocodes" data that shows the geographical location from which a tweet was sent could be a useful element to store with the data.
			</p>

			<p>
				All in all, our cereal box and grocery store example helps to highlight where data scientists get involved and the skills they need. Here are some of the skills that the example suggested:
			</p>

			<p><ul>
				<li>
							<blockquote>
											<p>
								<term>Learning the application domain</term><!-- linebreak -->The data scientist must quickly learn how the data will be used in a particular context.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								<term>Communicating with data users<!-- linebreak --></term>A data scientist must possess strong skills for learning the needs and preferences of users. Translating back and forth between the technical terms of computing and statistics and the vocabulary of the application domain is a critical skill.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								<term>Seeing the big picture of a complex system</term><!-- linebreak -->After developing an understanding of the application domain, the data scientist must imagine how data will move around among all of the relevant systems and people.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								<term>Knowing how data can be represented</term><!-- linebreak -->Data scientists must have a clear understanding about how data can be stored and linked, as well as about "metadata" (data that describes how other data are arranged).
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								<term>Data transformation and analysis</term><!-- linebreak -->When data becomes available for the use of decision makers, data scientists must know how to transform, summarize, and make inferences from the data. As noted above, being able to communicate the results of analyses to users is also a critical skill here.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								<term>Visualization and presentation</term><!-- linebreak -->Although numbers often have the edge in precision and detail, a good data display (e.g., a bar chart) can often be a more effective means of communicating results to data users.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								<term>Attention to quality<!-- linebreak --></term>No matter how good a set of data may be, there is no such thing as perfect data. Data scientists must know the limitations of the data they work with, know how to quantify its accuracy, and be able to make suggestions for improving the quality of the data in the future.
							</p>
							</blockquote>
				</li>

				<li>
							<blockquote>
											<p>
								<term>Ethical reasoning<!-- linebreak --></term>If data are important enough to collect, they are often important enough to affect people’s lives. Data scientists must understand important ethical issues such as privacy, and must be able to communicate the limitations of data to try to prevent misuse of data or analytical results.
							</p>
							</blockquote>
				</li>

			</ul></p>

			<p>
				The skills and capabilities noted above are just the tip of the iceberg, of course, but notice what a wide range is represented here. While a keen understanding of numbers and mathematics is important, particularly for data analysis, the data scientist also needs to have excellent communication skills, be a great systems thinker, have a good eye for visual displays, and be highly capable of thinking critically about how data will be used to make decisions and affect people’s lives. Of course there are very few people who are good at all of these things, so some of the people interested in data will specialize in one area, while others will become experts in another area. This highlights the importance of teamwork, as well.
			</p>

			<p>
				In this Introduction to Data Science eBook, a series of data problems of increasing complexity is used to illustrate the skills and capabilities needed by data scientists. The open source data analysis program known as "R" and its graphical user interface companion "R-Studio" are used to work with real data examples to illustrate both the challenges of data science and some of the techniques used to address those challenges. To the greatest extent possible, real datasets reflecting important contemporary issues are used as the basis of the discussions.
			</p>

			<p>
				No one book can cover the wide range of activities and capabilities involved in a field as diverse and broad as data science. Throughout this book references we reference guides and additional resources that provide the interested reader access to additional information. In the open source spirit of "R" and "R Studio" these are, wherever possible, web-based and free. In fact, one of the guides that appears most frequently in these pages is "Wikipedia," the free, online, user sourced encyclopedia. Although some teachers and librarians have legitimate complaints and concerns about Wikipedia, and it is admittedly not perfect, it is a very useful learning resource. Because it is free, because it covers about 50 times more topics than a printed encyclopedia, and because it keeps up with fast moving topics (like data science) better than printed encyclopedias, Wikipedia is very useful for getting a quick introduction to a topic. You can’t become an expert on a topic by only consulting Wikipedia, but you can certainly become smarter by starting there.
			</p>

			<p>
				Another very useful resource is Khan Academy. Thousands of people around the world use Khan Academy as a refresher course for a range of topics or as a quick introduction to a topic that they never studied before. All of the lessons at Khan Academy are free, and if you log in, you can do exercises and keep track of your progress.
			</p>

			<p>
				At the end of each chapter of this book, sources and other resources show the key topics relevant to the chapter. These sources provide a great place to start if you want to learn more about any of the topics that chapter does not explain in detail.
			</p>

			<p>
				It is essential to have access to the Internet while you are reading, so that you can follow some of the many links this book provides. Also, as you move into the sections in the book where open source software such as the R data analysis system is used, you will need to have access to the Internet in order to create and run programs.
			</p>

			<p>
				One last thing: The book presents topics in an order that should work well for people with little or no experience in computer science or statistics. Hopefully, there’s something here for everyone and, after all, you can’t beat the price of $0!
			</p>


			<subsubsection xml:id="sources">
				<title>Sources </title>

				<p><ul>
					<li>
									<p>
										<url href="http://en.wikipedia.org/wiki/E-Science">Wikipedia: E-Science</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://en.wikipedia.org/wiki/E-Science_librarianship">Wikipedia: E-Science Llibrarianship</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://en.wikipedia.org/wiki/Wikipedia:Size_comparisons">Wikipedia: Wikipedia: Size comparisons</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://en.wikipedia.org/wiki/Statistician">http://en.wikipedia.org/wiki/Statistician</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://en.wikipedia.org/wiki/Visualization_(computer_graphics)">http://en.wikipedia.org/wiki/Visualization_(computer_graphics)</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://www.khanacademy.org/">Khan Academy</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://en.wikipedia.org/wiki/E-Science_librarianship">http://en.wikipedia.org/wiki/E-Science_librarianship</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://www.readwriteweb.com/hack/2011/09/unlocking-big-dat">Unlocking Big Data with R by David Smith</url>
									</p>
					</li>

					<li>
									<p>
										<url href="http://rstudio.org/">R-Studio</url>
									</p>
					</li>

				</ul></p>

				<p>
					CHAPTER 1<!-- linebreak -->About Data<!-- linebreak --><image source='images/image9.png'/>
				</p>


				<p>
					CHAPTER 2<!-- linebreak -->Identifying Data Problems
				</p>

				<p>
					CHAPTER 3<!-- linebreak -->R, RStudio, and<!-- linebreak -->R-Markdown
				</p>



				<p>
					CHAPTER 4<!-- linebreak -->Follow the Data
				</p>


				<p>
					CHAPTER 5<!-- linebreak -->Rows and Columns
				</p>


				<p>
					CHAPTER 6<!-- linebreak -->Beer, Farms, and Peas
				</p>


				<p>
					CHAPTER 7<!-- linebreak -->Sample in a Jar
				</p>


				<p>
					CHAPTER 8<!-- linebreak -->Big Data? Big Deal<!-- linebreak --><image source='media/image3.png'/>
				</p>


				<p>
					CHAPTER 9<!-- linebreak -->Onward with R-Studio
				</p>


				<p>
					CHAPTER 10<!-- linebreak -->Tweet, Tweet<image source='media/image46.png'/>
				</p>


				<p>
					CHAPTER 11<!-- linebreak -->Popularity Contest<!-- linebreak --><image source='media/image57.png'/>In the previous chapter we found that arrival times of tweets on a given topic seem to fit a Poisson distribution, which is a distribution of probabilities that a certain number of unrelated events (like Tweets) occur within a certain period of time when they occur with a known average rate. Armed with that knowledge we can now develop a test to compare two different Twitter topics to see which one is more popular (or at least which one has a higher posting rate). We will use our knowledge of sampling distributions to understand the logic of the test.
				</p>

				<p>
					CHAPTER 12<!-- linebreak -->String Theory<!-- linebreak --><image source='media/image65.png'/><!-- linebreak --><em>Prior chapters focused on statistical analysis of tweet arrival times and built on earlier knowledge of samples and distributions. This chapter switches gears to focus on manipulating so-called "unstructured" data, which in most cases means natural language texts. Tweets are again a useful source of data for this because tweets are mainly a short (280 characters or less) character strings.</em><!-- linebreak --><!-- linebreak -->Yoiks, that last chapter was very challenging! Lots of numbers, lots of statistical concepts, lots of graphs. Let’s take a break from all that (temporarily) and focus on a different kind of data for a while. If you think about the Internet, and specifically about the World Wide Web for a while, you will realize: 1) That there are zillions of web pages; and 2) That most of the information on those web pages is "unstructured," in the sense that it does not consist of nice rows and columns of numeric data with measurements of time or other attributes. Instead, most of the data spread out across the Internet is text, digital photographs, or digital videos. These last two categories are interesting, but we will have to postpone consideration of them while we consider the question of text.
				</p>

				<p>
					CHAPTER 13<!-- linebreak -->Word Perfect<!-- linebreak --><image source='media/image10.png'/><em>In the previous chapter we mastered some of the most basic and important functions for examining and manipulating text. Now we are in a position to analyze the actual words that appear in text documents. Some of the most basic functions of the Internet, such as keyword search, are accomplished by analyzing the "content" i.e., the words in a body of text.</em>
				</p>

				<p>
					CHAPTER 14<!-- linebreak -->Storage Wars
				</p>

				<p>
					Before now we have only used small amounts of data that we typed in ourselves, or somewhat larger amounts that we extracted from Twitter. The world is full of other sources of data, however, and we need to examine how to get them into R, or at least how to make them accessible for manipulation in R. In this chapter, we examine various ways that data are stored, and how to access them.
				</p>

				<p>
					Older technologists who have watched the evolution of technology over recent decades remember a time when storage was expensive and it had to be hoarded like gold. Over the last few years, however, the accelerating trend of Moore’s Law has made data storage almost "too cheap to meter" (as they used to predict about nuclear power). Although this opens many opportunities, it also means that people keep data around for a long time, since it doesn’t make sense to delete anything, and they may keep data around in many different formats. As a result, the world is full of different data formats, some of which are proprietary designed and owned by a single company such as SAS and some of which are open, such as the lowly but infinitely useful "comma separated variable," or CSV format.
				</p>

				<p>
					In fact, one of the basic dividing lines in data formats is whether data are human readable or not. Formats that are not human readable, often called binary formats, are very efficient in terms of how much data they can pack in per kilobyte, but are also squirrelly in the sense that it is hard to see what is going on inside of the format. As you might expect, human readable formats are inefficient from a storage standpoint, but easy to diagnose when something goes wrong. For high volume applications, such as credit card processing, the data that is exchanged between systems is almost universally in binary formats. When a data set is archived for later reuse, for example in the case of government data sets available to the public, they are usually available in multiple formats, at least one of which is a human readable format.
				</p>

				<p>
					Another dividing line, as mentioned above is between proprietary and open formats. One of the most common ways of storing and sharing small datasets is as Microsoft Excel spreadsheets. Although
				</p>

				<p>
					this is a proprietary format, owned by Microsoft, it has also become a kind of informal and ubiquitous standard. Dozens of different software applications can read Excel formats (there are several different formats that match different versions of Excel). In contrast, the OpenDocument format is an open format, managed by a standards consortium, that anyone can use without worrying what the owner might do. OpenDocument format is based on XML, which stands for Extensible markup language. XML is a whole topic in and of itself, but briefly it is a data exchange format designed specifically to work on the Internet and is both human and machine readable. XML is managed by the W3C consortium, which is responsible for developing and maintaining the many standards and protocols that support the web.
				</p>

				<p>
					As an open source program with many contributors, R offers a wide variety of methods of connecting with external data sources. This is both a blessing and a curse. There is a solution to almost any data access problem you can imagine with R, but there is also a dizzying array of options available such that it is not always obvious what to choose. We’ll tackle this problem in two different ways. In the first half of this chapter we will look at methods for importing existing datasets. These may exist on a local computer or on the Internet but the characteristic they share in common is that they are contained (usually) within one single file. The main trick here is to choose the right command to import that data into R. In the second half of the chapter, we will consider a different strategy, namely linking to a "source" of data that is not a file. Many data sources, particularly databases, exist not as a single discrete file, but rather as a system. The system provides methods or calls to "query" data from the system, but from the perspective of the user (and of R) the data never really take the form of a file.
				</p>

				<p>
					The first and easiest strategy for getting data into R is to use the data import dialog in R-Studio. In the upper right hand pane of RStudio, the "Workspace" tab gives views of currently available data objects, but also has a set of buttons at the top for managing the work space. One of the choices there is the "Import Dataset" button: This enables a drop down menu where one choice is to import, "From Text File..." If you click this option and choose an appropriate file you will get a screen like this: <image source='media/image15.png'/><!-- linebreak -->The most important stuff is on the left side. Heading controls whether or not the first line of the text file is treated as containing variable names. The separator drop down gives a choice of different characters that separate the fields/columns in the data. RStudio tries to guess the appropriate choice here based on a scan of
				</p>

				<p>
					the data. In this case it guessed right by choosing "tab-delimited." As mentioned above, tab-delimited and comma-delimited are the two most common formats used for interchange between data programs.The next drop down is for "Decimal" and this option accounts for the fact the a dot is used in the U.S. while a comma may be used for the decimal point in Europe and elsewhere. Finally, the "Quote" drop down controls which character is used to contain quoted string/text data. The most common method is double quotes.
				</p>

				<p>
					Of course, we skipped ahead a bit here because we assumed that an appropriate file of data was available. It might be useful to see some examples of human readable data:
				</p>

				<p>
					Name Age Gender
				</p>

				<p>
					"Fred" 22 "M"
				</p>

				<p>
					"Ginger" 21 "F"
				</p>

				<p>
					Of course you can’t see the tab characters on the screen, but there is one tab character in between each pair of values. In each case, for both command tab-delimited, one line equals one row. The end of a line is marked, invisibly, with a so-called "newline" character. On occasion you may run into differences between different operating systems on how this end of line designation is encoded.
				</p>

				<p>
					The above is a very simple example of a comma-delimited file where the first row contains a "header," i.e. the information about the names of variables. The second and subsequent rows contain actual data. Each field is separated by a comma, and the text strings are enclosed in double quotes. The same file tab-delimited might look like this:
				</p>

				<p>
					Name Age Gender
				</p>

				<p>
					"Fred" 22 "M"
				</p>

				<p>
					"Ginger" 21 "F"
				</p>

				<p>
					Files containing comma or tab delimited data are ubiquitous across the Internet, but sometimes we would like to gain direct access to binary files in other formats. There are a variety of packages that one might use to access binary data. A comprehensive access list appears here:
				</p>

				<p>
					<url href="http://cran.r-project.org/doc/manuals/R-data.html">http://cran.r-project.org/doc/manuals/R-data.html</url>
				</p>

				<p>
					This page shows a range of methods for obtaining data from a wide variety of programs and formats. Because Excel is such a widely used program for small, informal data sets, we will use it as an example here to illustrate both the power and the pitfalls of accessing binary data with R. Down near the bottom of the page mentioned just above there are several paragraphs of discussion of how to access Excel files with R. In fact, the first sentence mentions that one of the most commonly asked data questions about R is how to access Excel data.
				</p>

				<p>
					Interestingly, this is one area where Mac and Linux users are at a disadvantage relative to Windows users. This is perhaps because Excel is a Microsoft product, originally written to be native to Windows, and as a result it is easier to create tools that work with Windows. One example noted here is the package called RODBC. The acronym ODBC stands for Open Database Connection, and this is a Windows facility for exchanging data among Windows programs. Although there is a proprietary ODBC driver available for the Mac, most Mac users will want to try a different method for getting access to Excel data.
				</p>

				<p>
					Another Windows-only package for R is called xlsReadWrite. This package provides convenient one-command calls for importing data directly from Excel spreadsheets or exporting it directly to spreadsheets. There are also more detailed commands that allow manipulating individual cells.
				</p>

				<p>
					Two additional packages, xlsx and XLConnect, supposedly will work with the Mac, but at the time of this writing both of these packages had version incompatibilities that made it impossible to install the packages directly into R. Note that the vast majority of packages provide the raw "source code" and so it is theoretically possible, but generally highly time consuming, to "compile" your own copies of these packages to create your own installation.
				</p>

				<p>
					Fortunately, a general purpose data manipulation package called gdata provides essential facilities for importing spreadsheet files. In the example that follows, we will use a function from gdata to read Excel data directly from a website. The gdata package is a kind of "Swiss Army Knife" package containing many different functions for accessing and manipulating data. For example, you may recall that R uses the value "NA" to represent missing data. Frequently, however, it is the case that data sets contain other values, such as 999, to represent missing data. The gdata package has several functions that find and transform these values to be consistent with R’s strategy for handling missing data.
				</p>

				<p>
					Begin by using install.package() and library() functions to prepare the gdata package for use:
				</p>

				<p>
					&gt; install.packages("gdata")
				</p>

				<p>
					# ... lots of output here
				</p>

				<p>
					&gt; library("gdata")
				</p>

				<p>
					gdata: read.xls support for 'XLS' (Excel 97-2004) files
				</p>

				<p>
					gdata: ENABLED.
				</p>

				<p>
					gdata: read.xls support for 'XLSX' (Excel 2007+) files ENABLED.
				</p>

				<p>
					Of course, you could also use the EnsurePackage() function that we developed in an earlier chapter, but it was important here to see the output from the library() function. Note that the gdata package reported some diagnostics about the different versions of Excel data that it supports. Note that this is one of the major drawbacks of binary data formats, particularly proprietary ones: you have to make sure that you have the right software to access the different versions of data that you might encounter. In this case it looks like we are covered for the early versions of Excel (97-2004) as well as later versions of Excel (2007+). We must always be on the lookout, however, for data that is stored in even newer versions of Excel that may not be supported by gdata or other packages.
				</p>

				<p>
					Now that gdata is installed, we can use the read.xls() function that it provides. The documentation for the gdata package and the read.xls() function is located here:
				</p>

				<p>
					<url href="http://cran.r-project.org/web/packages/gdata/gdata.pdf">http://cran.r-project.org/web/packages/gdata/gdata.pdf</url>
				</p>

				<p>
					A review of the documentation reveals that the only required argument to this function is the location of the XLS file, and that this location can be a pathname, a web location with http, or an Internet location with ftp (file transfer protocol, a way of sending and receiving files without using a web browser). If you hearken back to
				</p>

				<p>
					a very early chapter in this book, you may remember that we accessed some census data that had population counts for all the different U.S. states. For this example, we are going to read the Excel file containing that data directly into a dataframe using the read.xls() function:
				</p>

				<p>
					&gt; testFrame&lt;-read.xls( + "http://www.census.gov/popest/data/state/totals/2011/ tables/NST-EST2011-01.xls")
				</p>

				<p>
					trying URL 'http://www.census.gov/popest/data/state/totals/2011/ tables/NST-EST2011-01.xls'
				</p>

				<p>
					Content type 'application/vnd.ms-excel' length 31232 bytes (30 Kb)
				</p>

				<p>
					opened URL
				</p>

				<p>
					==================================================
				</p>

				<p>
					downloaded 30 Kb
				</p>

				<p>
					The command in the first three lines above provides the URL of the Excel file to the read.xls() function. The subsequent lines of output show the function attempting to open the URL, succeeding, and downloading 30 kilobytes of data.
				</p>

				<p>
					Next, let’s take a look at what we got back. In R-Studio we can click on the name of the dataframe in the upper right hand data pane or we can use this command:
				</p>

				<p>
					&gt; View(testFrame)
				</p>

				<p>
					Either method will show the contents of the dataframe in the upper left hand window of R-Studio. Alternatively, we could use the str() function to create a summary of the structure of testFrame:
				</p>

				<p>
					&gt; str(testFrame)
				</p>

				<p>
					'data.frame': 65 obs. of 10 variables:
				</p>

				<p>
					$ <sub>table.with.row.headers.in.column.A.and.column.hea</sub> ders.in.rows.3.through.4...leading.dots.indicate. sub.parts.: Factor w/ 65 levels "",".Alabama",..: 62 53 1 64 55 54 60 65 2 3 ...
				</p>

				<p>
					$ X : Factor w/ 60 levels "","1,052,567",..: 1 59 60 27 38 47 10 49 32 50 ...
				</p>

				<p>
					$ X.1 : Factor w/ 59 levels "","1,052,567",..: 1 1 59 27 38 47 10 49 32 50 ...
				</p>

				<p>
					$ X.2 : Factor w/ 60 levels "","1,052,528",..: 1 60 21 28 39 48 10 51 33 50 ...
				</p>

				<p>
					$ X.3 : Factor w/ 59 levels "","1,051,302",..: 1 1 21 28 38 48 10 50 33 51 ...
				</p>

				<p>
					$ X.4 : logi NA NA NA NA NA NA ...
				</p>

				<p>
					$ X.5 : logi NA NA NA NA NA NA ...
				</p>

				<p>
					$ X.6 : logi NA NA NA NA NA NA ...
				</p>

				<p>
					$ X.7 : logi NA NA NA NA NA NA ...
				</p>

				<p>
					$ X.8 : logi NA NA NA NA NA NA ...
				</p>

				<p>
					The last few lines are reminiscent of that late 60s song entitled, ""Na Na Hey Hey Kiss Him Goodbye." Setting aside all the NA NA NA NAs, however, the overall structure is 65 observations of 10 variables, signifying that the spreadsheet contained 65 rows and 10 columns of data. The variable names that follow are pretty bizarre. The first variable name is: "table.with.row.headers.in.column.A.and.column.headers.in.rows. 3.through.4...leading.dots.indicate.sub.parts."
				</p>

				<p>
					What a mess! It is clear that read.xls() treated the upper leftmost cell as a variable label, but was flummoxed by the fact that this was really just a note to human users of the spreadsheet (the variable labels, such as they are, came on lower rows of the spreadsheet). Subsequent variable names include X, X.1, and X.2: clearly the read.xls() function did not have an easy time getting the variable names out of this file.
				</p>

				<p>
					The other worrisome finding from str() is that all of our data are "factors." This indicates that R did not see the incoming data as numbers, but rather as character strings that it interpreted as factor data. Again, this is a side effect of the fact that some of the first cells that read.xls() encountered were text rather than numeric. The numbers came much later in the sheet. This also underscores the idea that it is much better to export a data set in a more regular, structured format such as CSV rather than in the original spreadsheet format. Clearly we have some work to do if we are to make use of these data as numeric population values.
				</p>

				<p>
					First, we will use an easy trick to get rid of stuff we don’t need. The Census bureau put in three header rows that we can eliminate like this:
				</p>

				<p>
					&gt; testFrame&lt;-testFrame[-1:-3,]
				</p>

				<p>
					The minus sign used inside the square brackets refers to the index of rows that should be eliminated from the dataframe. So the notation -1:-3 gets rid of the first three rows. We also leave the column designator empty so that we can keep all columns for now. So the interpretation of all of the notation within the square brackets is that rows 1 through 3 should be dropped, all other rows should be included, and all columns should be included. We assign the result back to the same data object thereby replacing the original with our new, smaller, cleaner version.
				</p>

				<p>
					Next, we know that of the ten variables we got from read.xls(), only the first five are useful to us (the last five seem to be blank). So this command keeps the first five columns of the dataframe:
				</p>

				<p>
					&gt; testFrame&lt;-testFrame[,1:5]
				</p>

				<p>
					In the same vein, the tail() function shows us that the last few rows just contained some census bureau notes:
				</p>

				<p>
					&gt; tail(testFrame,5)
				</p>

				<p>
					So we can safely eliminate those like this:
				</p>

				<p>
					&gt; testFrame&lt;-testFrame[-58:-62,]
				</p>

				<p>
					If you’re alert you will notice that we could have combined some of these commands, but for the sake of clarity we have done each operation individually. The result (which you can check in the upper right hand pane of R-Studio) is a dataframe with 57 rows and five observations. Now we are ready to perform a couple of data transformations. Before we start these, let’s give our first column a more reasonable name:
				</p>

				<p>
					&gt; testFrame$region &lt;- testFrame[,1]
				</p>

				<p>
					We’ve used a little hack here to avoid typing out the ridiculously long name of that first variable/column. We’ve used the column notation in the square brackets on the right hand side of the expression to refer to the first column (the one with the ridiculous name) and simply copied the data into a new column entitled "region." Let’s also remove the offending column with the stupid name so that it does not cause us problems later on:
				</p>

				<p>
					&gt; testFrame&lt;-testFrame[,-1]
				</p>

				<p>
					Next, we can change formats and data types as needed. We can remove the dots from in front of the state names very easily with str_replace():
				</p>

				<p>
					&gt; testFrame$region &lt;str_replace( +
				</p>

				<p>
					testFrame$region,"\\.","")
				</p>

				<p>
					Don’t forget that str_replace() is part of the stringr package, and you will have to use install.packages() and library() to load it if it is not already in place. The two backslashes in the string expression above are called "escape characters" and they force the dot that follows to be treated as a literal dot rather than as a wildcard character. The dot on its own is a wildcard that matches one instance of any character.
				</p>

				<p>
					Next, we can use str_replace_all() and as.numeric() to convert the data contained in the population columns to usable numbers. Remember that those columns are now represented as R "factors" and what we are doing is taking apart the factor labels (which are basically character strings that look like this: "308,745,538") and making them into numbers. This is sufficiently repetitive that we could probably benefit by created our own function call to do it:
				</p>

				<p>
					# Numberize() Gets rid of commas and other junk and
				</p>

				<p>
					# converts to numbers
				</p>

				<p>
					# Assumes that the inputVector is a list of data that
				</p>

				<p>
					# can be treated as character strings
				</p>

				<p>
					Numberize &lt;- function(inputVector)<!-- linebreak -->{
				</p>

				<p>
					# Get rid of commas
				</p>

				<p>
					inputVector&lt;-str_replace_all(inputVector,",","")
				</p>

				<p>
					# Get rid of spaces
				</p>

				<p>
					inputVector&lt;-str_replace_all(inputVector," ","") return(as.numeric(inputVector))
				</p>

				<p>
					}
				</p>

				<p>
					This function is flexible in that it will deal with both unwanted commas and spaces, and will convert strings into numbers whether they are integers or not (i.e., possibly with digits after the decimal point). So we can now run this a few times to create new vectors on the dataframe that contain the numeric values we wanted:
				</p>

				<p>
					testFrame$april10census &lt;-Numberize(testFrame$X)
				</p>

				<p>
					testFrame$april10base &lt;-Numberize(testFrame$X.1)
				</p>

				<p>
					testFrame$july10pop &lt;-Numberize(testFrame$X.2)
				</p>

				<p>
					testFrame$july11pop &lt;-Numberize(testFrame$X.3)
				</p>

				<p>
					By the way, the choice of variable names for the new columns in the dataframe was based on an examination of the original data set that was imported by read.xls(). You can (and should) confirm that the new columns on the dataframe are numeric. You can use str() to accomplish this.
				</p>

				<p>
					We’ve spent half a chapter so far just looking at one method of importing data from an external file (either on the web or local storage). A lot of our time was spent conditioning the data we got in order to make it usable for later analysis. Herein lies a very important lesson (or perhaps two). An important, and sometimes time consuming aspect of what data scientists do is to make sure that data are "fit for the purpose" to which they are going to be put. We had the convenience of importing a nice data set directly from the web with one simple command, and yet getting those data actually ready to analyze took several additional steps.
				</p>

				<p>
					A related lesson is that it is important and valuable to try to automate as many of these steps as possible. So when we saw that numbers had gotten stored as factor labels, we moved immediately to create a general function that would convert these to numbers. Not only does this save a lot of future typing, it prevents mistakes from creeping into our processes.
				</p>

				<p>
					Now we are ready to consider the other strategy for getting access to data: querying it from external databases. Depending upon your familiarity with computer programming and databases, you may notice that the abstraction is quite a bit different here. Earlier in the chapter we had a file (a rather messy one) that contained a complete copy of the data that we wanted, and we read that file into R and stored it in our local computer’s memory (and possibly later on the hard disk for safekeeping). This is a good and reasonable strategy for small to medium sized datasets, let’s say just for the sake of argument anything up to 100 megabytes.
				</p>

				<p>
					But what if the data you want to work with is really large too large to represent in your computer’s memory all at once and too large to store on your own hard drive. This situation could occur even with smaller datasets if the data owner did not want people making complete copies of their data, but rather wanted everyone who was using it to work from one "official" version of the data. Similarly, if data do need to be shared among multiple users, it is much better to have them in a database that was designed for this purpose: For the most part R is a poor choice for maintaining data that must be used simultaneously by more than one user. For these reasons, it becomes necessary to do one or both of the following things:
				</p>

				<p><ol>
					<li>
									<blockquote>
														<p>
										Allow R to send messages to the large, remote database asking
									</p>
									</blockquote>
					</li>

				</ol></p>

				<blockquote>
									<p>
					for summaries, subsets, or samples of the data.
				</p>
				</blockquote>

				<p><ol>
					<li>
									<blockquote>
														<p>
										Allow R to send computation requests to a distributed data processing system asking for the results of calculations performed on the large remote database.
									</p>
									</blockquote>
					</li>

				</ol></p>

				<p>
					Like most contemporary programming languages, R provides several methods for performing these two tasks. The strategy is the same across most of these methods: a package for R provides a "client" that can connect up to the database server. The R client supports sending commands mostly in SQL, structured query language to the database server. The database server returns a result to the R client, which places it in an R data object (typically a data frame) for use in further processing or visualization.
				</p>

				<p>
					The R community has developed a range of client software to enable R to connect up with other databases. Here are the major databases for which R has client software:
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										RMySQL Connects to MySQL, perhaps the most popular open source database in the world. MySQL is the M in "LAMP" which is the acronym for Linux, Apache, MySQL, and PHP. Together, these four elements provide a complete solution for data driven web applications.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										ROracle Connects with the widely used Oracle commercial database package. Oracle is probably the most widely used commercial database package. Ironically, Oracle acquired Sun Microsystems a few years ago and Sun developers predominate in development and control of the open source MySQL system,
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										RPostgreSQL Connects with the well-developed, full featured PostgreSQL (sometimes just called Postgres) database system. PostgreSQL is a much more venerable system than MySQL and has a much larger developer community. Unlike MySQL, which is effectively now controlled by Oracle, PostgreSQL has a developer community that is independent of any company and a licensing scheme that allows anybody to modify and reuse the code.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										RSQlite Connects with SQlite, another open source, independently developed database system. As the name suggests, SQlite has a very light "code footprint" meaning that it is fast and compact.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										RMongo Connects with the MongoDB system, which is the only system here that does not use SQL. Instead, MongoDB uses JavaScript to access data. As such it is well suited for web development applications.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										RODBC Connects with ODBC compliant databases, which include Microsoft’s SQLserver, Microsoft Access, and Microsoft Excel, among others. Note that these applications are native to Windows and Windows server, and as such the support for Linux and Mac OS is limited.
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					For demonstration purposes, we will use RMySQL. This requires installing a copy of MySQL on your computer. Use your web browser to go to this page:
				</p>

				<p>
					<url href="http://dev.mysql.com/downloads/">http://dev.mysql.com/downloads/</url>
				</p>

				<p>
					Then look for the "MySQL Community Server." The term community in this context refers to the free, open source developer community version of MySQL. Note that there are also commercial versions of SQL developed and marketed by various companies including Oracle.
				</p>

				<p>
					Download the version of MySQL Community Server that is most appropriate for your computer’s operating system and install it. Note that unlike user applications, such as a word processor, there is no real user interface to server software like the MySQL Community Server. Instead, this software runs in the "background" providing services that other programs can use. This is the essence of the client-server idea. In many cases the server is on some remote computer to which we do not have physical access. In this case, we will run the server on our local computer so that we can complete the demonstration.
				</p>

				<p>
					On the Mac installation used in preparation of this chapter, after installing the MySQL server software, it was also important to install the "MySQL Preference Pane," in order to provide a simple graphical interface for turning the server on and off. Because we are just doing a demonstration here, and we want to avoid future security problems, it is probably sensible to turn MySQL server off when we are done with the demonstration. In Windows, you can use MySQL Workbench to control the server settings on your local computer.
				</p>

				<p>
					Returning to R, use install.packages() and library() to prepare the RMySQL package for use. If everything is working the way it should, you should be able to run the following command from the command line:
				</p>

				<p>
					&gt; con &lt;- dbConnect(dbDriver("MySQL"), dbname = "test")
				</p>

				<p>
					The dbConnect() function establishes a linkage or "connection" between R and the database we want to use. This underscores the point that we are connecting to an "external" resource and we must therefore manage the connection. If there were security controls involved, this is where we would provide the necessary information to establish that we were authorized users of the database. In this case, because we are on a "local server" of MySQL, we don’t need to provide these. The dbDriver() function provided as an argument to dbConnect specifies that we want to use a MySQL client. The database name specified as dbname="test" is just a placeholder at this point. We can use the dbListTables() function to see what tables are accessible to us (for our purposes, a table is just like a dataframe, but it is stored inside the database system):
				</p>

				<p>
					&gt; dbListTables(con)
				</p>

				<p>
					character(0)
				</p>

				<p>
					The response "character(0)" means that there is an empty list, so no tables are available to us. This is not surprising, because we just installed MySQL and have not used it for anything yet. Unless you have another database available to import into MySQL, we can just use the census data we obtained earlier in the chapter to create a table in MySQL:
				</p>

				<p>
					&gt; dbWriteTable(con, "census", testFrame, +
				</p>

				<p>
					overwrite = TRUE)
				</p>

				<p>
					[1] TRUE
				</p>

				<p>
					Take note of the arguments supplied to the dbWriteTable() function. The first argument provides the database connection that we established with the dbConnect() function. The "census" argument gives our new table in MySQL a name. We use testFrame as the source of data as noted above a dataframe and a relational database table are very similar in structure. Finally, we provide the argument overwrite=TRUE, which was not really needed in this case because we know that there were no existing tables but could be important in other operations where we need to make sure to replace any old table that may have been left around from previous work. The function returns the logical value TRUE to signal that it was able to finish the request that we made. This is important in programming new functions because we can use the signal of success or failure to guide subsequent steps and provide error or success messages.
				</p>

				<p>
					Now if we run dbListTables() we should see our new table:
				</p>

				<p>
					&gt; dbListTables(con)
				</p>

				<p>
					[1] "census"
				</p>

				<p>
					Now we can run an SQL query on our table:
				</p>

				<p>
					&gt; dbGetQuery(con, "SELECT region, july11pop FROM census WHERE july11pop&lt;1000000")
				</p>

				<p>
					region july11pop
				</p>

				<p>
					1 Alaska 722718
				</p>

				<p>
					2 Delaware 907135
				</p>

				<p>
					3 District of Columbia 617996
				</p>

				<p>
					4 Montana 998199
				</p>

				<p>
					5 North Dakota 683932
				</p>

				<p>
					6 South Dakota 824082
				</p>

				<p>
					7 Vermont 626431
				</p>

				<p>
					8 Wyoming 568158
				</p>

				<p>
					Note that the dbGetQuery() call shown above breaks onto two lines, but the string starting with SELECT has to be typed all on one line. The capitalized words in that string are the SQL commands. It is beyond the scope of this chapter to give an SQL tutorial, but, briefly, SELECT chooses a subset of the table and the fields named after select are the ones that will appear in the result. The FROM command choose the table(s) where the data should come from. The WHERE command specified a condition, in this case that we only wanted rows where the July 2011 population was less than one million. SQL is a powerful and flexible language and this just scratches the surface.
				</p>

				<p>
					In this case we did not assign the results of dbGetQuery() to another data object, so the results were just echoed to the R console. But it would be easy to assign the results to a dataframe and then use that dataframe for subsequent calculations or visualizations.
				</p>

				<p>
					To emphasize a point made above, the normal motivation for accessing data through MySQL or another database system is that a large database exists on a remote server. Rather than having our own complete copy of those data, we can use dbConnect(), dbGetQuery() and other database functions to access the remote data through SQL. We can also use SQL to specify subsets of the data, to preprocess the data with sorts and other operations, and to create summaries of the data. SQL is also particularly well suited to "joining" data from multiple tables to make new combinations. In the present example, we only used one table, it was a very small table, and we had created it ourselves in R from an Excel source, so none of these were very good motivations for storing our data in MySQL, but this was only a demonstration.
				</p>

				<p>
					The next step beyond remote databases is toward distributed computing across a "cluster" of computers. This combines the remote
				</p>

				<p>
					access to data that we just demonstrated with additional computational capabilities. At this writing, one of the most popular systems for large scale distributed storage and computing is "Hadoop" (named after the toy elephant of the young son of the developer).
				</p>

				<p>
					Hadoop is not a single thing, but is rather a combination of pieces of software called a library. Hadoop is developed and maintained by the same people who maintain the Apache open source web server. There are about a dozen different parts of the Hadoop framework, but the Hadoop Distributed Files System (HDFS) and Hadoop MapReduce framework are two of the most important frameworks.
				</p>

				<p>
					HDFS is easy to explain. Imagine your computer and several other computers at your home or workplace. If we could get them all to work together, we could call them a "cluster" and we could theoretically get more use out of them by taking advantage of all of the storage and computing power they have as a group. Running HDFS, we can treat this cluster of computers as one big hard drive. If we have a really large file too big to fit on any one of the computers HDFS can divide up the file and store its different parts in different storage areas without us having to worry about the details. With a proper configuration of computer hardware, such as an IT department could supply, HDFS can provide an enormous amount of "throughput" (i.e., a very fast capability for reading and writing data) as well as redundancy and failure tolerance.
				</p>

				<p>
					MapReduce is a bit more complicated, but it follows the same logic of trying to divide up work across multiple computers. The term MapReduce is used because there are two big processes involved: map and reduce. For the map operation, a big job is broken up into lots of separate parts. For example, if we wanted to create a search index for all of the files on a company’s intranet servers, we could break up the whole indexing task into a bunch of separate jobs. Each job might take care of indexing the files on one server.
				</p>

				<p>
					In the end, though, we don’t want dozens or hundreds of different search indices. We want one big one that covers all of the files our company owns. This is where the reduce operation comes in. As all of the individual indexing jobs finish up, a reduce operation combines them into one big job. This combining process works on the basis of a so-called "key." In the search indexing example, some of the small jobs might have found files that contained the word "fish." As each small job finishes, it mentioned whether or not fish appeared in a document and perhaps how many times fish appeared. The reduce operation uses fish as a key to match up the results from all of the different jobs, thus creating an aggregated summary listing all of the documents that contained fish. Later, if anyone searched on the word fish, this list could be used to direct them to documents that contained the word.
				</p>

				<p>
					In short, "map" takes a process that the user specifies and an indication of which data it applies to, and divides the processing into as many separate chunks as possible. As the results of each chunk become available, "reduce" combines them and eventually creates and returns one aggregated result.
				</p>

				<p>
					Recently, an organization called RevolutionAnalytics has developed an R interface or "wrapper" for Hadoop that they call RHadoop. This package is still a work in progress in the sense that it does not appear in the standard CRAN package archive, not because there is anything wrong with it, but rather because RevolutionAnalytics wants to continue to develop it without having to provide stable versions for the R community. There is a nice tutorial here:
				</p>

				<p>
					<url href="https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial">https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial</url>
				</p>

				<p>
					We will break open the first example presented in the tutorial just to provide further illustration of the use of MapReduce. As with our MySQL example, this is a rather trivial activity that would not normally require the use of a large cluster of computers, but it does show how MapReduce can be put to use.
				</p>

				<p>
					The tutorial example first demonstrates how a repetitive operation is accomplished in R without the use of MapReduce. In prior chapters we have used several variations of the apply() function. The lapply() or list-apply is one of the simplest. You provide an input vector of values and a function to apply to each element, and the lapply() function does the heavy lifting. The example in the RHadoop tutorial squares each integer from one to 10. This first command fills a vector with the input data:
				</p>

				<p>
					&gt; small.ints &lt;1:10
				</p>

				<p>
					&gt; small.ints
				</p>

				<p>
					[1] 1 2 3 4 5 6 7 8 9 10
				</p>

				<p>
					Next we can apply the "squaring function" (basically just using the ^ operator) to each element of the list:
				</p>

				<p>
					&gt; out &lt;lapply(small.ints, function(x) x^2)
				</p>

				<p>
					&gt; out
				</p>

				<p>
					[[1]]
				</p>

				<p>
					[1] 1
				</p>

				<p>
					[[2]]
				</p>

				<p>
					[1] 4
				</p>

				<p>
					... (shows all of the values up to [[10]] [1] 100)
				</p>

				<p>
					In the first command above, we have used lapply() to perform a function on the input vector small.ints. We have defined the function as taking the value x and returning the value x^2. The result is a list of ten vectors (each with just one element) containing the squares of the input values. Because this is such a small problem, R was able to accomplish it in a tiny fraction of a second.
				</p>

				<p>
					After installing both Hadoop and RHadoop which, again, is not an official package, and therefore has to be installed manually we can perform this same operation with two commands:
				</p>

				<p>
					&gt; small.ints &lt;- to.dfs(1:10)
				</p>

				<p>
					&gt; out &lt;- mapreduce(input = small.ints, +
				</p>

				<p>
					map = function(k,v) keyval(v, v^2))
				</p>

				<p>
					In the first command, we again create a list of integers from one to ten. But rather than simply storing them in a vector, we are using the "distributed file system" or dfs class that is provided by RHadoop. Note that in most cases we would not need to create this ourselves because our large dataset would already exist on the HDFS (Hadoop Distributed FIle System). We would have connected to
				</p>

				<p>
					HDFS and selected the necessary data much as we did earlier in this chapter with dbConnect().
				</p>

				<p>
					In the second command, we are doing essentially the same thing as we did with lapply(). We provide the input data structure (which, again is a dfs class data object, a kind of pointer to the data stored by Hadoop in the cluster). We also provide a "map function" which is the process that we want to apply to each element in our data set. Notice that the function takes two arguments, k and v. The k refers to the "key" that we mentioned earlier in the chapter. We actually don’t need the key in this example because we are not supplying a reduce function. There is in fact, no aggregation or combining activity that needs to occur because our input list (the integers) and the output list (the squares of those integers) are lists of the same size. If we had needed to aggregate the results of the map function, say by creating a mean or a sum, we would have had to provide a "reduce function" that would do the job.
				</p>

				<p>
					The keyval() function, for which there is no documentation at this writing, is characterized as a "helper" function in the tutorial. In this case it is clear that the first argument to keyval, "v" is the integer to which the process must be applied, and the second argument, "v^2" is the squaring function that is applied to each argument. The data returned by mapreduce() is functionally equivalent to that returned by lapply(), i.e., a list of the squares of the integers from 1 to 10.
				</p>

				<p>
					Obviously there is no point in harnessing the power of a cluster of computers to calculate something that could be done with a pencil and a paper in a few seconds. If, however, the operation was more complex and the list of input data had millions of elements, the use of lapply() would be impractical as it would take your computer quite a long time to finish the job. On the other hand, the second strategy of using mapreduce() could run the job in a fraction of a second, given a sufficient supply of computers and storage.
				</p>

				<p>
					On a related note, Amazon, the giant online retailer, provides virtual computer clusters that can be used for exactly this kind of work. Amazon’s product is called the Elastic Compute Cloud or EC2, and it is possible to create a small cluster of Linux computers for only a few cents per hour.
				</p>

				<p>
					To summarize this chapter, although there are many analytical problems that require only a small amount of data, the wide availability of larger data sets has added new challenges to data science. As a single user program running on a local computer, R is well suited for work by a single analyst on a data set that is small enough to fit into the computer’s memory. We can retrieve these small datasets from individual files stored in human readable 9e.g., CSV) or binary (e.g., XLS) formats.
				</p>

				<p>
					To be able to tackle the larger data sets, however, we need to be able to connect R with either remote databases or remote computational resources or both. A variety of packages is available to connect R to mature database technologies such as MySQL. In fact, we demonstrated the use of MySQL by installing it on a local machine and then using the RMySQL package to create a table and query it. The more cutting edge technology of Hadoop is just becoming available for R users. This technology, which provides the potential for both massive storage and parallel computational power, promises to make very large datasets available for processing and analysis in R.
				</p>

				<p>
					<term>Chapter Challenge</term>
				</p>

				<p>
					Hadoop is a software framework designed for use with Apache, which is first and foremost a Linux server application. Yet there are development versions of Hadoop available for Windows and Mac as well. These are what are called single node instances, that is they use a single computer to simulate the existence of a large cluster of computers. See if you can install the appropriate version of Hadoop for your computer’s operating system.
				</p>

				<p>
					As a bonus activity, if you are successful in installing Hadoop, then get a copy of the RHadoop package from RevolutionAnalytics and install that. If you are successful with both, you should be able to run the MapReduce code presented in this chapter.
				</p>

			</subsubsection>

			<subsubsection xml:id="sources-9">
				<title>Sources </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://cran.r-project.org/doc/manuals/R-data.html">http://cran.r-project.org/doc/manuals/R-data.html</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://cran.r-project.org/doc/manuals/R-data.pdf">http://cran.r-project.org/doc/manuals/R-data.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://cran.r-project.org/web/packages/gdata/gdata.pdf">http://cran.r-project.org/web/packages/gdata/gdata.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://dev.mysql.com/downloads/">http://dev.mysql.com/downloads/</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Comparison_of_relational_databas">http://en.wikipedia.org/wiki/Comparison_of_relational_databas e_management_systems</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Mapreduce">http://en.wikipedia.org/wiki/Mapreduce</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial">https://github.com/RevolutionAnalytics/RHadoop/wiki/Tutorial</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

			</subsubsection>

			<subsubsection xml:id="section-2">
				<title></title>

			</subsubsection>

			<subsubsection xml:id="r-functions-used-in-this-chapter-3">
				<title><!-- linebreak -->R Functions Used in this Chapter </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										as.numeric() Convert another data type to a number
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										dbConnect() Connect to an SQL database
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										dbGetQuery() Run an SQL query and return the results
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										dbListTables() Show the tables available in a connection
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										dbWriteTable() Send a data table to an SQL systems
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										install.packages() Get the code for an R package
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										lapply() Apply a function to elements of a list
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										library() Make an R package available for use
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										Numberize() A custom function created in this chapter
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										read.xls() Import data from a binary R file; part of the gdata package
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										return() Used in functions to designate the data returned by the function
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										str_replace() Replace a character string with another
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										str_replace_all() Replace multiple instances of a character string with another
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					
				</p>

				<p>
					CHAPTER 15<!-- linebreak -->Map MashUp
				</p>

				<p>
					<image source='media/image67.png'/>Much of what we have accomplished so far has focused on the standard rectangular dataset: one neat table with rows and columns well defined. Yet much of the power of data science comes from bringing together different sources of data in complementary ways. In this chapter we combine different sources of data to make a unique product that transcends any one source.
				</p>

				<p>
					MThe term mashup originated in the music business decades ago related to the practice of overlaying one music recording on top of another one. The term has entered general usage to mean anything that brings together disparate influences or elements. In the application development area, mashup often refers to bringing together various sources of data to create a new product with unique value. There’s even a non-profit group called the Open Mashup Alliance that develops standards and methods for creating new mashups.
				</p>

				<p>
					One example of a mashup is <url href="http://www.housingmaps.com">HousingMaps</url>, a web application that grabs apartment rental listings from the classified advertising service Craigslist and plots them on an interactive map that shows the location of each listing. If you have ever used Craigslist you know that it provides a very basic text-based interface and that the capability to find listings on a map would be welcome.
				</p>

				<p>
					In this chapter we tackle a similar problem. Using some address data from government records, we call the Google geocoding API over the web to find the latitude and longitude of each address. Then we plot these latitudes and longitudes on a map of the U.S. This activities reuses skills we learned in the previous chapter for reading in data files, adds some new skills related to calling web APIs, and introduces us to a new type of data, namely the shapefiles that provide the basis for electronic maps.
				</p>

				<p>
					Let’s look at the new stuff first. The Internet is full of shapefiles that contain mapping data. Shapefiles are a partially proprietary, partially open format supported by a California software company called ESRI. Shapefile is actually an umbrella term that covers several different file types. Because the R community has provided some packages to help deal with shapefiles, we don’t need too much information about the details. The most important thing to know is that shapefiles contain points, polygons, and "polylines." Everyone knows what a point and a polygon are, but a polyline is a term used by computer scientist to refer to a multi-segment line. In many graphics applications it is much easier to approximate a curved line with many tiny connected straight lines than it is to draw a truly curved line. If you think of a road or a river on a map, you will have a good idea of a polyline.
				</p>

				<p>
					The U.S. Census bureau publishes shapefiles at various levels of detail for every part of the country. Search for the term "shapefile" at "site:census.gov" and you will find several pages with listings of different shapefiles. For this exercise, we are using a relatively low detail map of the whole U.S. from <url href="https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html">Cartographic Boundary Files - Shapefile</url>. We downloaded a "zip" file. Unzip this (usually just by double-clicking on it) and you will find several files inside it. The file ending in "shp" is the main shapefile. Another file that will be useful to us ends in "dbf" which contains labels and other information.
				</p>

				<p>
					To get started, we will need two new R packages called PBSmapping and maptools. PBSmapping refers not to public broadcasting, but rather to the Pacific Biology Station, whose researchers and technologists kindly bundled up a wide range of the R processing tools that they use to manage map data. The maptools package was developed by Nicholas J. Lewin-Koh (University of Singapore) and others to provide additional tools and some "wrappers" to make PBSmapping functions easier to use. In this chapter we only scratch the surface of the available tools: there could be a whole book just dedicated to R mapping functions alone.
				</p>

				<p>
					Before we read in the data we grabbed from the Census Bureau, let’s set the working directory in R-Studio so that we don’t have to type it out on the command line. Click on the Tools menu and then choose "Set Working Directory." Use the dialog box to designate the folder where you have unzipped the shape data. After that, these commands will load the shape data into R and show us what we have:
				</p>

				<p>
					&gt; usShape &lt;- importShapefile( +
				</p>

				<p>
					"gz_2010_us_040_00_500k",readDBF=TRUE)
				</p>

				<p>
					&gt; summary(usShape)
				</p>

				<p>
					PolySet
				</p>

				<p>
					Records : 90696
				</p>

				<p>
					Contours : 574
				</p>

				<p>
					Holes : 0
				</p>

				<p>
					Events : NA
				</p>

				<p>
					On boundary : NA
				</p>

				<p>
					Ranges
				</p>

				<p>
					X : [-179.14734, 179.77847]
				</p>

				<p>
					Y : [17.884813, 71.3525606439998]
				</p>

				<p>
					Attributes
				</p>

				<p>
					Projection : LL
				</p>

				<p>
					Zone : NULL
				</p>

				<p>
					Extra columns :
				</p>

				<p>
					&gt; plotPolys(usShape)
				</p>

				<p>
					This last command gives us a simple plot of the 90,696 shapes that our shapefile contains. Here is the plot:
				</p>

				<figure>
	<image source="media/image1.png"/>
					<caption></caption>
</figure>

				<p>
					This is funny looking! The ranges output from the summary() command gives us a hint as to why. The longitude of the elements in our map range from -179 to nearly 180: this covers pretty much the whole of the planet. The reason is that the map contains shapes for Hawaii and Alaska. Both states have far western longitudes, but the Aleutian peninsula in Alaska extends so far that it crosses over the longitude line where -180 and 180 meet in the Pacific Ocean. As a result, the continental U.S. is super squashed. We can specify a more limited area of the map to consider by using the xlim and ylim parameters. The following command:
				</p>

				<p>
					&gt; plotPolys(usShape,+
				</p>

				<p>
					xlim=c(-130,-60),ylim=c(20,50))
				</p>

				<p>
					...gives a plot that shows the continental U.S. more in its typical proportions.
				</p>

				<p>
					<image source='media/image50.png'/><!-- linebreak -->So now we have some map data stored away and ready to use. The PBSmapping package gives us the capability of adding points to an existing map. For now, let’s demonstrate this with a made up point somewhere in Texas:
				</p>

				<p>
					&gt; X &lt;-100
				</p>

				<p>
					&gt; Y &lt;- 30
				</p>

				<p>
					&gt; EID &lt;- 1
				</p>

				<p>
					&gt; pointData &lt;- data.frame(EID,X,Y)
				</p>

				<p>
					&gt; eventData &lt;-as.EventData( +
				</p>

				<p>
					pointData,projection=NA)
				</p>

				<p>
					&gt; addPoints(eventData,col="red",cex=.5)
				</p>

				<figure>
	<image source="media/image24.png"/>
					<caption>continental U.S. with red dot in Texas</caption>
</figure>

				<p>
					You have to look carefully, but in southern Texas there is now a little red dot. We began by manually creating a single point specified by X (longitude), Y (latitude), and EID (an identification number) and sticking it into a dataframe. Then we converted the data in that dataframe into an EventData object. This is a custom class of objects specified by the PBSmapping package. The final command above adds the EventData to the plot.
				</p>

				<p>
					The idea of EventData is a little confusing, but if you remember that this package was developed by biologists at the Pacific Biology Station to map sightings of fish and other natural phenomena it makes more sense. In their lingo, an event was some observation of interest that occurred at a particular day, time, and location. The "event id" or EID &lt;1 that we stuck in the data frame was just saying that this was the first point in our list that we wanted to plot. For us it is not an event so much as a location of something we wanted to see on the map.
				</p>

				<p>
					Also note that the "projection=NA" parameter in the as.EventData() coercion is just letting the mapping software know that we don’t want our point to be transformed according to a mapping projection. If you remember from your Geography class, a projection is a mapping technique to make a curved object like the Earth seem sensible on a flat map. In this example, we’ve already flattened out the U.S., so there is no need to transform the points.
				</p>

				<p>
					Next, we need a source of points to add to our map. This could be anything that we’re interested in: the locations of restaurants, crime scenes, colleges, etc. In Google a search for filetype:xls or filetype;csv with appropriate additional search terms can provide interesting data sources. You may also have mailing lists of customers or clients. The most important thing is that we will need street address, city, and state in order to geocode the addresses. For this example, we searched for "housing street address list filetype:csv", and this turned up a data set of small businesses that have contracts with the U.S. Department of Health and Human services. Let’s read this in using read.csv():
				</p>

				<p>
					&gt; dhhsAddrs &lt;read.csv("DHHS_Contracts.csv")
				</p>

				<p>
					&gt; str(dhhsAddrs)
				</p>

				<p>
					'data.frame': 599 obs. of 10 variables:
				</p>

				<p>
					$ Contract.Number : Factor w/ 285 levels "26301D0054","500000049",..: 125 125 125 279 164 247 19 242 275 70 ...
				</p>

				<p>
					$ Contractor.Name : Factor w/ 245 levels "2020 COMPANY LIMITED LIABILITY COMPANY",..: 1 1 1 2 2 3 4 6 5 7 ...
				</p>

				<p>
					$ Contractor.Address : Factor w/ 244 levels "1 CHASE SQUARE 10TH FLR, ROCHESTER, NY ",..: 116 116 116 117 117 136 230 194 64 164 ...
				</p>

				<p>
					$ Description.Of.Requirement: Factor w/ 468 levels "9TH SOW DEFINTIZE THE LETTER CONTRACT",..: 55 55 55 292 172 354 308 157 221 340 ...
				</p>

				<p>
					$ Dollars.Obligated : Factor w/ 586 levels " $1,000,000.00 ",..: 342 561 335 314 294 2 250 275 421 21 ...
				</p>

				<p>
					$ NAICS.Code : Factor w/ 55 levels "323119","334310",..: 26 26 26 25 10 38 33 29 27 35 ...
				</p>

				<p>
					$ Ultimate.Completion.Date : Factor w/ 206 levels "1-Aug-2011","1-Feb-2013",..: 149 149 149 10 175 161 124 37 150 91 ...
				</p>

				<p>
					$ Contract.Specialist : Factor w/ 95 levels "ALAN FREDERICKS",..: 14 14 60 54 16 90 55 25 58 57 ...
				</p>

				<p>
					$ Contract.Specialist.Email : Factor w/ 102 levels "410-786-8622",..: 16 16 64 59 40 98 60 29 62 62 ... <sub>$ X : logi NA NA NA NA NA NA ...</sub>
				</p>

				<p>
					It sounds like a crazy 60s song again! Anyhow, we read in a comma separated data set with 599 rows and 10 variables. The most important field we have there is Contractor.Address. This contains the street addresses that we need to geocode. We note, however, that the data type for these is Factor rather than character string. So we need to convert that:
				</p>

				<p>
					&gt; dhhsAddrs$strAddr &lt;+
				</p>

				<p>
					as.character(dhhsAddrs$Contractor.Address)
				</p>

				<p>
					&gt; mode(dhhsAddrs$strAddr)
				</p>

				<p>
					[1] "character"
				</p>

				<p>
					&gt; tail(dhhsAddrs$strAddr,4)
				</p>

				<p>
					[1] "1717 W BROADWAY, MADISON, WI "
				</p>

				<p>
					[2] "1717 W BROADWAY, MADISON, WI "
				</p>

				<p>
					[3] "1717 W BROADWAY, MADISON, WI "
				</p>

				<p>
					[4] "789 HOWARD AVE, NEW HAVEN, CT, "
				</p>

				<p>
					This looks pretty good: Our new column, called dhhsAddrs, is character string data, converted from the factor labels of the original Contractor.Address column. Now we need to geocode these.
				</p>

				<p>
					We will use the Google geocoding application programming interface (API) which is pretty easy to use, does not require an account or application ID, and allows about 2500 address conversions per day. The API can be accessed over the web, using what is called an HTTP GET request. Note that the Terms of Service for the Google geocoding API are very specific about how the interface can be used most notably on the point that the geocodes must be used on Google maps. Make sure you read the Terms of Service before you create any software applications that use the geocoding service. See the link in the bibliography at the end of the chapter. The bibliography has a link to an article with dozens of other geocoding APIs if you disagree with Google’s Terms of Service.
				</p>

				<p>
					These acronyms probably look familiar. HTTP is the Hyper Text Transfer Protocol, and it is the standard method for requesting and receiving web page data. A GET request consists of information that is included in the URL string to specify some details about the information we are hoping to get back from the request. Here is an example GET request to the Google geocoding API:
				</p>

				<p>
					<url href="http://maps.googleapis.com/maps/api/geocode/json?address=1">http://maps.googleapis.com/maps/api/geocode/json?address=1 600+Pennsylvania+Avenue,+Washington,+DC&amp;sensor=false</url>
				</p>

				<p>
					The first part of this should look familiar: The <url href="http://maps.googleapis.com">http://maps.googleapis.com</url> part of the URL specifies the domain name just like a regular web page. The next part of the URL, "/maps/api/geocode" tells Google which API we want to use. Then the "json" indicates that we would like to receive our result in "Java Script Object Notation" (JSON) which is a structured, but human readable way of sending back some data. The address appears next, and we are apparently looking for the White House at 1600 Pennsylvania Avenue in Washington, DC. Finally, sensor=false is a required parameter indicating that we are not sending our request from a mobile phone. You can type that whole URL into the address field of any web browser and you should get a sensible result back. The JSON notation is not beautiful, but you will see that it makes sense and provides the names of individual data items along with their values. Here’s a small excerpt that shows the key parts of the data object that we are trying to get our hands on:
				</p>

				<p>
					{
				</p>

				<p>
					"results" : [
				</p>

				<p>
					{
				</p>

				<p>
					"address_components" : [
				</p>

				<p>
					"geometry" : {
				</p>

				<p>
					"location" : {
				</p>

				<p>
					"lat" : 38.89788009999999,
				</p>

				<p>
					"lng" : -77.03664780
				</p>

				<p>
					},
				</p>

				<p>
					"status" : "OK"
				</p>

				<p>
					}
				</p>

				<p>
					There’s tons more data in the JSON object that Google returned, and fortunately there is an R package, called JSONIO, that will extract the data we need from the structure without having to parse it ourselves. In order to get R to send the HTTP GET requests to google, we will also need to use the RCurl package. This will give us a single command to send the request and receive the result back, essentially doing all of the quirky steps that a web browser takes care of automatically for us. To get started, install.packages() and library() the two packages we will need RCurl and JSONIO.
				</p>

				<p>
					Next, we will create a new helper function to take the address field and turn it into the URL that we need:
				</p>

				<p>
					# Format an URL for the Google Geocode API
				</p>

				<p>
					MakeGeoURL &lt;- function(address)
				</p>

				<p>
					{
				</p>

				<p>
					root &lt;- "http://maps.google.com/maps/api/geocode/"
				</p>

				<p>
					url &lt;- paste(root, "json?address=", +
				</p>

				<p>
					address, "&amp;sensor=false", sep = "")
				</p>

				<p>
					return(URLencode(url))
				</p>

				<p>
					}
				</p>

				<p>
					There are three simple steps here. The first line initializes the beginning part of the URL into a string called root. Then we use paste() to glue together the separate parts of the strong (note the sep="" so we don’t get spaces between the parts). This creates a string that looks almost like the one in the White House example two pages ago. The final step converts the string to a legal URL using a utility function called URLencode() that RCurl provides. Let’s try it:
				</p>

				<p>
					&gt; MakeGeoURL( + "1600 Pennsylvania Avenue, Washington, DC")
				</p>

				<p>
					[1] "http://maps.google.com/maps/api/geocode/json?add ress=1600%20Pennsylvania%20Avenue,%20Washington,% 20DC&amp;sensor=false"
				</p>

				<p>
					Looks good! Just slightly different than the original example (%20 instead of the plus character) but hopefully that won’t make a difference. Remember that you can type this function at the command line or you can create it in the script editing window in the upper left hand pane of R-Studio. The latter is the better way to go and if you click the "Source on Save" checkmark, R-Studio will make sure to update R’s stored version of your function every time you save the script file.
				</p>

				<p>
					Now we are ready to use our new function, MakeGeoURL(), in another function that will actually request the data from the Google API:
				</p>

				<p>
					Addr2latlng &lt;- function(address)
				</p>

				<p>
					{
				</p>

				<p>
					url &lt;- MakeGeoURL(address)
				</p>

				<p>
					apiResult &lt;- getURL(url)
				</p>

				<p>
					geoStruct &lt;- fromJSON(apiResult, +
				</p>

				<p>
					simplify = FALSE)
				</p>

				<p>
					lat &lt;- NA
				</p>

				<p>
					lng &lt;- NA
				</p>

				<p>
					try(lat &lt;- +
				</p>

				<p>
					geoStruct$results[[1]]$geometry$location$lat)
				</p>

				<p>
					try(lng &lt;- +
				</p>

				<p>
					geoStruct$results[[1]]$geometry$location$lng)
				</p>

				<p>
					return(c(lat, lng))
				</p>

				<p>
					}
				</p>

				<p>
					We have defined this function to receive an address string as its only argument. The first thing it does is to pass the URL string to MakeGeoURL() to develop the formatted URL. Then the function passes the URL to getURL(), which actually does the work of sending the request out onto the Internet. The getURL() function is part of the RCurl package. This step is just like typing a URL into the address box of your browser.
				</p>

				<p>
					We capture the result in an object called "apiResult". If we were to stop and look inside this, we would find the JSON structure that appeared a couple of pages ago. We can pass this structure to the function fromJSON() we put the result in an object called geoStruct. This is a regular R data frame such that we can access any individual element using regular $ notation and the array index [[1]]. In other instances, a JSON object may contain a whole list of data structures, but in this case there is just one. If you compare the variable names "geometry", "location", "lat" and "lng" to the JSON example a few pages ago you will find that they match perfectly. The fromJSON() function in the JSONIO package has done all the heavy lifting of breaking the JSON structure into its component pieces.
				</p>

				<p>
					Note that this is the first time we have encountered the try() function. When programmers expect the possibility of an error, they will frequently use methods that are tolerant of errors or that catch errors before they disrupt the code. If our call to getURL() returns something unusual that we aren’t expecting, then the JSON structure may not contain the fields that we want. By surrounding our command to assign the lat and lng variables with a try() function, we can avoid stopping the flow of the code if there is an error. Because we initialized lat and lng to NA above, this function will return a two item list with both items being NA if an error occurs in accessing the JSON structure. There are more elegant ways to accomplish this same goal. For example, the Google API puts an error code in the JSON structure and we could choose to interpret that instead. We will leave that to the chapter challenge!
				</p>

				<p>
					In the last step, our new Addr2latlng() function returns a two item list containing the latitude and longitude. We can test it out right now:
				</p>

				<p>
					&gt; testData &lt;- Addr2latlng( +
				</p>

				<p>
					"1600 Pennsylvania Avenue, Washington, DC")
				</p>

				<p>
					&gt; str(testData)
				</p>

				<p>
					num [1:2] 38.9 -77
				</p>

				<p>
					Perfect! we called our new function Addr2latlng() with the address of the Whitehouse and got back a list with two numeric items containing the latitude and longitude associated with that address. With just a few lines of R code we have harnessed the power of Google’s extensive geocoding capability to convert a brief text street address into mapping coordinates.
				</p>

				<p>
					At this point there isn’t too much left to do. We have to create a looping mechanism so that we can process the whole list of addresses in our DHHS data set. We have some small design choices here. It would be possible to attach new fields to the existing dataframe. Instead, the following code keeps everything pretty simple by receiving a list of addresses and returning a new data frame with X, Y, and EID ready to feed into our mapping software:
				</p>

				<p>
					# Process a whole list of addresses
				</p>

				<p>
					ProcessAddrList &lt;- function(addrList)
				</p>

				<p>
					{
				</p>

				<p>
					resultDF &lt;- data.frame(atext=character(), +
				</p>

				<p>
					X=numeric(),Y=numeric(),EID=numeric())
				</p>

				<p>
					i &lt;- 1
				</p>

				<p>
					for (addr in addrList)
				</p>

				<p>
					{
				</p>

				<p>
					latlng = Addr2latlng(addr)
				</p>

				<p>
					resultDF &lt;- rbind(resultDF,+
				</p>

				<p>
					data.frame(atext=addr, + X=latlng[[2]],Y=latlng[[1]], EID=i))
				</p>

				<p>
					i &lt;- i + 1
				</p>

				<p>
					}
				</p>

				<p>
					return(resultDF)
				</p>

				<p>
					}
				</p>

				<p>
					<sub>This new function takes one argument, the list of addresses, which</sub> should be character strings. In the first step we make an empty dataframe for use in the loop. In the second step we initialize a scalar index variable called i to the number 1. We will increment this in the loop and use it as our EID.
				</p>

				<p>
					Then we have the for loop. We are using a neat construct here called "in". The expression "addr in addrList" creates a new variable called addr. Every time that R goes through the loop it assigns to addr the next item in addrList. When addrList runs out of items, the for loop ends. Very handy!
				</p>

				<p>
					Inside the for loop the first thing we do is to call the function that we developed earlier: Addr2latlng(). This performs one conversion of an address to a geocode (latitude and longitude) as described earlier. We pass addr to it as add contains the text address for this time around the loop. We put the result in a new variable called latlng. Remember that this is a two item list.
				</p>

				<p>
					The next statement, starting with "resultDF &lt;- rbind" is the heart of this function. Recall that rbind() sticks a new row on the end of a dataframe. So in the arguments to rbind() we supply our earlier version of resultDF (which starts empty and grows from there) plus a new row of data. The new row of data includes the text address (not strictly necessary but handy for diagnostic purposes), the "X" that our mapping software expects (this is the longitude), the "Y" that the mapping software expects, and the event ID, EID, that the mapping software expects.
				</p>

				<p>
					At the end of the for loop, we increment the index i so that we will have the next number next time around for EID. Once the for loop is done we simply return the dataframe object, resultDF. Piece of cake!
				</p>

				<p>
					Now let’s try it and plot our points:
				</p>

				<p>
					&gt; dhhsPoints &lt;- ProcessAddrList(dhhsAddrs$strAddr)
				</p>

				<p>
					&gt; dhhsPoints &lt;- dhhsPoints[!is.na(dhhsPoints$X),]
				</p>

				<p>
					&gt; eventData &lt;- as.EventData(dhhsPoints,projection=NA)
				</p>

				<p>
					&gt; addPoints(eventData,col="red",cex=.5)
				</p>

				<p>
					The second command above is the only one of the four that may seem unfamiliar. The as.EventData() coercion is picky and will not process the dataframe if there are any fields that are NA. To get rid of those rows that do not have complete latitude and longitude data, we use is.na() to test whether the X value on a given row is NA. We use the ! (not) operator to reverse the sense of this test. So the only rows that will survive this step are those where X is not NA. The plot below shows the results.
				</p>

				<figure>
	<image source="media/image61.png"/>
					<caption>US map with geolocation</caption>
</figure>

				<p>
					If you like conspiracy theories, there is some support in this graph: The vast majority of the companies that have contracts with DHHS are in the Washington, DC area, with a little trickle of additional companies heading up the eastern seaboard as far as Boston and southern New Hampshire. Elsewhere in the country there are a few companies here and there, particularly near the large cities of the east and south east.
				</p>

				<p>
					Using some of the parameters on plotPolys() you could adjust the zoom level and look at different parts of the country in more detail. If you remember that the original DHHS data also had the monetary size of the contract in it, it would also be interesting to change the size or color of the dots depending on the amount of money in
				</p>

				<p>
					the Dollars.Obligated field. This would require running addPoints() in a loop and setting the col= or cex= parameters for each new point.
				</p>

				<p>
					If you get that far and are still itching for another challenge, try improving the map. One idea for this was mentioned above: you could change the size or color of the dots based on the size of the contract received. An even better (and much harder) idea would be to sum the total dollar amount being given within each state and then color each state according to how much DHHS money it receives. This would require delving into the shapefile data quite substantially so that you could understand how to find the outline of a state and fill it in with a color.
				</p>

			</subsubsection>

			<subsubsection xml:id="r-functions-used-in-this-chapter-4">
				<title>R Functions Used in This Chapter </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										Addr2latlng() A custom function built for this chapter
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										addPoints() Place more points on an existing plot
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										as.character() Coerces data into a character string
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										as.EventData() Coerce a regular dataframe into an EventData object for use with PBSmapping routines.
									</p>
									</blockquote>
					</li>

				</ul></p>

			</subsubsection>

			<subsubsection xml:id="chapter-challenges">
				<title>Chapter Challenge(s) </title>

				<p>
					Improve the Addr2latlng() function so that it checks for errors returned from the Google API. This will require going to the Google API site, looking more closely at the JSON structure, and reviewing the error codes that Google mentions in the documentation. You may also learn enough that you can repair some of the addresses that failed.
				</p>

			</subsubsection>

			<subsubsection xml:id="r-functions-used-in-this-chapter-5">
				<title>R Functions Used in This Chapter </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										data.frame() Takes individual variables and ties them together
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										for() Runs a loop, iterating a certain number of times depending upon the expression provided in parentheses
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										fromJSON() Takes JSON data as input and provides a regular R dataframe as output
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										function() Creates a new function
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										importShapeFile() Gets shapefile data from a set of ESRI compatible polygon data files
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										MakeGeoURL() Custom helper function built for this chapter
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										paste() Glues strings together
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										plotPolys() Plots a map from shape data
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										rbind() Binds new rows on a dataframe
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										return() Specifies the object that should be returned from a function
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										URLencode() Formats a character string so it can be used in an HTTP request
									</p>
									</blockquote>
					</li>

				</ul></p>

			</subsubsection>

			<subsubsection xml:id="sources-10">
				<title><!-- linebreak -->Sources </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://blog.programmableweb.com/2012/06/21/7-free-geocodin">http://blog.programmableweb.com/2012/06/21/7-free-geocodin g-apis-google-bing-yahoo-and-mapquest/</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="https://developers.google.com/maps/terms">https://developers.google.com/maps/terms</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Open_Mashup_Alliance">http://en.wikipedia.org/wiki/Open_Mashup_Alliance</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Shapefile">http://en.wikipedia.org/wiki/Shapefile</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.housingmaps.com/">http://www.housingmaps.com/</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.census.gov/geo/www/cob/cbf_state.html">http://www.census.gov/geo/www/cob/cbf_state.html</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					CHAPTER 16<!-- linebreak -->Line Up, Please <term>heepdog demonstration, Lone Pine Sanctuary, Brisbane, QLD. Photo credit: Jeff Stanton</term>
				</p>

				<p>
					<image source='media/image64.png'/><!-- linebreak --><term>Sheepdog demonstration, Lone Pine Sanctuary, Brisbane, QLD. Photo credit: Jeff Stanton</term><!-- linebreak -->Data users are often interested in questions about relationships and prediction. For example, those interested in athletics might want to know how the size of the fanbase of a team is connected with attendance on game day. In this chapter, our Australian colleague, Robert de Graaf, introduces the techniques of linear regression, a very important data science tool.
				</p>

				<p>
					<term>Using R to Find Relationships between Sets of Data via Multiple Regression, by Robert W. de Graaf</term>
				</p>

				<p>
					Finding relationships between sets of data is one of the key aims of data science. The question of ‘does x influence y’ is of prime concern for data analysts – are house prices influenced by incomes, is the growth rate of crops improved by fertilizer, do taller sprinters run faster?
				</p>

				<p>
					The workhorse method used by statisticians to interpret data is linear modeling, which is a term covering a wide variety of methods, from the relatively simple to very sophisticated. You can get an idea of how many different methods there are by looking at the Regression Analysis page in Wikipedia and checking out the number of entries listed under ‘Models’ on the right hand sidebar (and, by the way, the list is not exhaustive).
				</p>

				<p>
					The basis of all these methods is the idea that it is possible to fit a line to a set of data points which represents the effect an "independent" variable is having on a "dependent" variable. It is easy to visualize how this works with one variable changing in step with another variable. Figure one shows a line fitted to a series of points, using the so called "least squares" method (a relatively simple mathematical method of finding a best fitting line). Note that although the line fits the points fairly well, with an even split (as even as it can be for five points!) of points on either side of the line, none of the points are precisely on the line – the data do not fit the line precisely. As we discuss the concepts in regression analysis further, we will see that understanding these discrepancies is just as important as understanding the line itself.<!-- linebreak --><image source='media/image63.png'/>
				</p>

				<blockquote>
									<p>
					Figure: A line fitted to some points
				</p>
				</blockquote>

				<p>
					The graph in the figure above shows how the relationship between an input variable – on the horizontal x axis – relates to the output values on the y axis.
				</p>

				<p>
					The original ideas behind linear regression were developed by some of the usual suspects behind many of the ideas we’ve seen already, such as Laplace, Gauss, Galton, and Pearson. The biggest individual contribution was probably by Gauss, who used the procedure to predict movements of the other planets in the solar system when they were hidden from view, and hence correctly predict when and where they would appear in view again.
				</p>

				<p>
					The mathematical idea that allows us to fit lines of best fit to a set of data points like this is that we can find a position for the line that will minimize the distance the line is from all the points. While the mathematics behind these techniques can be handled by someone with college freshman mathematics the reality is that with even only a few data points, the process of fitting with manual calculations becomes very tedious, very quickly. For this reason, we will not discuss the specifics of how these calculations are done, but move quickly to how it can be done for us, using R.
				</p>

				<p>
					<term>Football or Rugby?</term>
				</p>

				<p>
					We can use an example to show how to use linear regression on real data. The example concerns attendances at Australian Rules Football matches. For all of you in the U.S., Australian Rules Football is closer to what you think of as rugby and not so much like American football. The data in this example concerns matches/ games at the largest stadium for the sport, the Melbourne Cricket Ground (note: Australian Rules football is a winter sport, cricket is a summer sport). The Melbourne Cricket Ground or MCG is also considered the most prestigious ground to play a football match, due to its long association with Australian Rules Football.
				</p>

				<p>
					Australian Rules Football is the most popular sport to have been developed in Australia. The object is to kick the ball through the larger goal posts, scoring six points. If the ball is touched before it crosses the line under the large posts, or instead passes through the
				</p>

				<p>
					smaller posts on either side of the large goal posts, a single point is scored. The rules ensure that possession of the ball is always changing. There is full body tackling and possession is turned over frequently. This leads to continuous and exciting on-field action.
				</p>

				<p>
					The main stronghold of Australian Rules Football is in the state of Victoria (south eastern Australia), where the original league, the VFL, became the AFL after its league of mostly Melbourne suburban teams added teams to represent areas outside Victoria, such as West Coast (Perth, the capital of the state of Western Australia) and Adelaide (capital of the state of South Australia). Note that Melbourne is the capital city of Victoria. Much of the popularity of the VFL was based on the rivalries between neighboring suburbs, and teams with long histories, like the Collingwood Football Club based in one of Melbourne’s most working class suburbs – have large and loyal fan bases.
				</p>

				<p>
					While it isn’t necessary to know anything about how the sport is played to understand the example, it is useful to know that the Australian Football League, the premiere organization playing Australian Rules Football, was formerly the Victorian Football League, and although teams from outside the Australian state of Victoria have joined, more than half the teams in the league are Victorian, even though Victoria is only one of six Australian states.
				</p>

				<p>
					<term>Getting the Data</term>
				</p>

				<p>
					The data are available from OzDasl, a website which provides public domain data sets for analysis, and the MCG attendance data has its own page at <url href="http://www.statsci.org/data/oz/afl.html">http://www.statsci.org/data/oz/afl.html</url>.
				</p>

				<p>
					The variable of interest is MCG attendance, and named ‘MCG’ in the dataset. Most statisticians would refer to this variable as the dependent variable, because it is the variable that we are most interested in predicting: It is the "outcome" of the situation we are trying to understand. Potential explanatory, or independent, variables include club membership, weather on match day, date of match, etc. There is a detailed description of each of the variables available on the website. You can use the data set to test your own theories of what makes football fans decide whether or not to go to a game, but to learn some of the skills we will test a couple of those factors together.
				</p>

				<p>
					Before we can start, we need R to be able to find the data. Make sure that you download the data to the spot on your computer that R considers the "working" directory. Use this command to find out what the current working directory is:
				</p>

				<p>
					&gt; getwd()
				</p>

				<p>
					After downloading the data set from OzDasl into your R working directory, read the data set into R as follows:
				</p>

				<p>
					&gt; attend&lt;-read.delim("afl.txt", header=TRUE)
				</p>

				<p>
					&gt; attach(attend)
				</p>

				<p>
					We include the optional ‘header = TRUE’ to designate the first row as the column names, and the ‘attach’ commands turns each of the named columns into a single column vector.
				</p>

				<p>
					Once we’ve read the data into R, we can examine some plots of the data. With many techniques in data science, it can be quite valuable to visualize the data before undertaking a more detailed analysis. One of the variables we might consider is the combined mem
				</p>

				<p>
					bership of the two teams playing, a proxy for the popularity of the teams playing.
				</p>

				<p>
					&gt; plot(MCG ~ Members, xlab = "Combined membership of teams playing", ylab = "MCG Match Day Attendance" )
				</p>

				<figure>
	<image source="media/image58.png"/>
					<caption>Figure: Scatterplot of membership versus attendance </caption>
</figure>

				<p>
					<term>Figure: Scatterplot of membership versus attendance</term>
				</p>

				<p>
					We see evidence of a trend in the points on the left hand side of the graph, and a small group of points representing games with very high combined membership but that don’t seem to fit the trend applying to the rest of data. If it wasn’t for the four "outliers" on the right hand side of the plot, we would be left with a plot showing a very strong relationship.
				</p>

				<p>
					As a next step we can use R to create a linear model for MCG attendance using the combined membership as the single explanatory variable using the following R code:
				</p>

				<p>
					&gt; model1 &lt;lm(MCG ~ Members-1)
				</p>

				<p>
					&gt; summary(model1)
				</p>

				<p>
					There are two steps here because in the first step the lm() command creates a nice big data structure full of output, and we want to hang onto that in the variable called "model1." In the second command we request an overview of the contents of model1.
				</p>

				<p>
					It is important to note that in this model, and in the others that follow, we have added a ‘-1’ term to the specification, which forces the line of best fit to pass through zero on the y axis at zero on the x axis (more technically speaking, the y-intercept is forced to be at the origin). In the present model that is essentially saying that if the two teams are so unpopular they don’t have any members, no one will go to see their matches, and vice versa. This technique is appropriate in this particular example, because both of the measures have sensible zero points, and we can logically reason that zero on X implies zero on Y. In most other models, particularly for survey data that may not have a sensible zero point (think about rating scales ranging from 1 to 5), it would not be appropriate to force the best fitting line through the origin.
				</p>

				<p>
					The second command above, summary(model1), provides the following output:
				</p>

				<p>
					Call:
				</p>

				<p>
					lm(formula = MCG ~ Members 1)
				</p>

				<p>
					Residuals:
				</p>

				<p>
					Min 1Q Median 3Q Max
				</p>

				<p>
					-44.961 -6.550 2.954 9.931 29.252
				</p>

				<p>
					Coefficients:
				</p>

				<p>
					Estimate Std. Error t value Pr(&gt;|t|)
				</p>

				<p>
					Members 1.53610 0.08768 17.52 &lt;2e-16 ***
				</p>

				<p>
					--
				</p>

				<p>
					Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
				</p>

				<p>
					Residual standard error: 15.65 on 40 degrees of freedom
				</p>

				<p>
					Multiple R-squared: 0.8847,
				</p>

				<p>
					Adjusted R-squared: 0.8818
				</p>

				<p>
					F-statistic: 306.9 on 1 and 40 DF, p-value: &lt; 2.2e-16
				</p>

				<p>
					Wow! That’s a lot of information that R has provided for us, and we need to use this information to decide whether we are happy with this model. Being ‘happy’ with the model can involve many factors, and there is no simple way of deciding. To start with, we will look at the r-squared value, also known as the coefficient of determination.
				</p>

				<p>
					The r squared value – the coefficient of determination – represents the proportion of the variation which is accounted for in the dependent variable by the whole set of independent variables (in this case just one independent variable). An r-squared value of 1.0 would mean that the X variable(s), the independent variable(s), perfectly predicted the y, or dependent variable. An r-squared value of zero would indicated that the x variable(s) did not predict the y variable at all. R-squared cannot be negative. The r-squared of .8847 in this example means that the Combined Members variable account for 88.47% of the MCG attendance variable, an excellent result. Note that there is no absolute rule for what makes an rsquared good. Much depends on the purpose of the analysis. In the analysis of human behavior, which is notoriously unpredictable, an r-squared of .20 or .30 may be very good.
				</p>

				<p>
					In the figure below, we have added a line of best fit based on the model to the x-y plot of MCG attendance against total team membership with this command:
				</p>

				<p>
					&gt; abline(model1)
				</p>

				<p>
					While the line of best fit seems to fit the points in the middle, the points on the lower right hand side and also some points towards the top of the graph, appear to be a long way from the line of best fit.<!-- linebreak --><image source='media/image36.png'/>
				</p>

				<p>
					<term>Adding Another Independent Variable</term>
				</p>

				<p>
					We discussed at the beginning of this chapter the origin of Australian Rules Football in Victoria, where the MCG is located. While most of the teams in the AFL are also Victoria teams, and therefore have a supporter base which can easily access the MCG, a number of the teams are from other states, and their supporters would need to make a significant interstate journey to see their team play at the MCG. For example, the journey from Sydney to Melbourne is around eight hours by car or two by plane, whereas from Perth, where most West Coast’s supporter base is located, is close to five hours by air – and two time zones away. Australia is a really huge country.
				</p>

				<p>
					The dataset doesn’t have a variable for interstate teams but fortunately there are only four teams that are interstate: Brisbane, Sydney, Adelaide, and West Coast, abbreviated respectively as "Bris", "Syd", "Adel", and "WC". We can make a binary coded variable to indicate these interstate teams with a simple command:
				</p>

				<p>
					&gt; away.inter &lt;ifelse(Away=="WC" |
				</p>

				<p>
					Away=="Adel"| Away=="Syd"| Away=="Bris",1,0)
				</p>

				<p>
					The code above checks the values in the column labeled ‘Away’, and if it finds an exact match with one of the names of an interstate team, it stores a value of 1. Otherwise it stores a value of 0. Note that we use a double equals sign for the exact comparison in R, and the vertical bar is used to represent the logical ‘OR’ operator. These symbols are similar, although not precisely the same, as symbols used to represent logical operators in programming languages such as C and Java. Having created the new ‘Away team is interstate’ variable, we can use this variable to create a new linear regression model that includes two independent variables.
				</p>

				<p>
					&gt; model2&lt;-lm(MCG~Members+away.inter-1)
				</p>

				<p>
					&gt; summary(model2)
				</p>

				<p>
					Call:
				</p>

				<p>
					lm(formula = MCG ~ Members + away.inter 1)
				</p>

				<p>
					Residuals:
				</p>

				<p>
					Min 1Q Median 3Q Max
				</p>

				<p>
					-30.2003 -8.5061 0.0862 8.5411 23.5687
				</p>

				<p>
					Coefficients:
				</p>

				<p>
					Estimate Std. Error t value Pr(&gt;|t|)
				</p>

				<p>
					Members 1.69255 0.07962 21.257 &lt; 2e-16 ***
				</p>

				<p>
					away.inter -22.84122 5.02583 -4.545 5.2e-05 ***
				</p>

				<p>
					--
				</p>

				<p>
					Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
				</p>

				<p>
					Residual standard error: 12.82 on 39 degrees of freedom
				</p>

				<p>
					Multiple R-squared: 0.9246, Adjusted R-squared: 0.9208
				</p>

				<p>
					F-statistic: 239.2 on 2 and 39 DF, p-value: &lt; 2.2e-16
				</p>

				<p>
					Note that the r-squared value is now 0.9246, which is quite a bit higher than the 0.8847 that we observed in the previous model. In this new model, the two independent variables working together account for 92.46% of the dependent variable. So together, the total fan base and the status as an away team are doing a really great job of predicting attendance. This result is also intuitive – we would expect that football fans, regardless of how devoted they are to their team, are more likely to come to games if they’re a moderate car ride away, compared to a plane journey.
				</p>

				<p>
					Because we have two independent variables now, we have to look beyond the r-squared value to understand the situation better. In particular, about one third of the way into the output for the lm() command there is a heading that says "Estimate." Right below that are slope values for Members and for away.inter. Notice that the slope (sometimes called a "B-weight") on Members is positive: This makes sense because the more fans the team has the higher the attendance. The slope on away.inter is negative because when this variable is 1 (in the case of interstate teams) the attendance is lower) whereas when this variable is 0 (for local teams), attendance is higher.
				</p>

				<p>
					How can you tell if these slopes or B-weights are actually important contributors to the prediction? You can divide the unstandardized B-weight by its standard error to create a "t value". The lm() command has done this for you and it is reported in the output above. This "t" is the Student’s t-test, described in a previous chapter. As a rule of thumb, if this t value has an absolute value (i.e., ignoring the minus sign if there is one) that is larger than about 2, you can be assured that the independent/predictor variable we are talking about is contributing to the prediction of the dependent
				</p>

				<p>
					variable. In this example we can see that Members has a humongous t value of 21.257, showing that it is very important in the prediction. The away.inter variable has a somewhat more modest, but still important value of -4.545 (again, don’t worry about the minus sign when judging the magnitude of the t value).
				</p>

				<p>
					We can keep on adding variables that we think make a difference. How many variables we end up using depends, apart from our ability to think of new variables to measure, somewhat on what we want to use the model for.
				</p>

				<p>
					The model we have developed now has two explanatory variables – one which can be any positive number, and one which is two levels. We now have what could be considered a very respectable r-squared value, so we could easily leave well enough alone. That is not to say our model is perfect, however – the graphs we have prepared suggest that the ‘Members’ effect is actually different if the away team is from interstate rather than from Victoria – the crowd does not increase with additional combined membership as quickly with an away team, which is in line with what we might expect intuitively.
				</p>

				<p>
					One thing we didn’t mention was the actual prediction equation that one might construct from the output of lm(). It is actually very simple and just uses the estimates/B-weights from the output:
				</p>

				<p>
					MCG = (21.257 * Members) ( 4.545 * away.inter)
				</p>

				<p>
					This equation would let us predict the attendance of any game with a good degree of accuracy, assuming that we knew the combined fan base and whether the team was interstate. Interestingly, statisticians are rarely interested in using prediction equations like the one above: They are generally more interested in just knowing that a predictor is important or unimportant. Also, one must be careful with using the slopes/B-weights obtained from a linear regression of a single sample, because they are likely to change if another sample is analyzed just because of the forces of randomness.
				</p>

				<p>
					<term>Conclusion</term>
				</p>

				<p>
					The material we have covered is really only a taste of multiple regression and linear modeling. On the one hand, there are a number of additional factors that may be considered before deciding on a final model. On the other hand, there are a great number of techniques that may be used in specialized circumstances. For example, in trying to model attendance at the MCG, we have seen that the standard model fits the data some of the time but not others, depending on the selection of the explanatory variables.
				</p>

				<p>
					In general, a simple model is a good model, and will keep us from thinking that we are better than we really are. However, there are times when we will want to find as many dependent variables as possible. Contrast the needs of a manager trying to forecast sales to set inventory with an engineer or scientist trying to select parameters for further experimentation. In the first case, the manager needs to avoid a falsely precise estimate which could lead her to be overconfident in the forecast, and either order too much stock or too little. The manager wants to be conservative about deciding that particular variables make a difference to prediction variable. On the other hand the experimenter wants to find as many variables as possible for future research, so is prepared to be optimistic about whether different parameters affect the variables of interest.
				</p>

				<p>
					<term>Chapter Challenge</term>
				</p>

				<p>
					We intentionally ignored some of the output of these regression models, for the sake of simplicity. It would be quite valuable for you to understand those missing parts, however. In particular, we ignored the "p-values" associated with the t-tests on the slope/Bweight estimates and we also ignored the overall F-statistic reported at the very bottom of the output. There are tons of great resources on the web for explaining what these are.
				</p>

				<p>
					For a super bonus, you could also investigate the meaning of the "Adjusted" r-squared that appears in the output.
				</p>

			</subsubsection>

			<subsubsection xml:id="sources-11">
				<title>Sources </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Australian_rules_football">http://en.wikipedia.org/wiki/Australian_rules_football</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm.">http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm. html</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.ddiez.com/teac/r/linear_models.php">http://www.ddiez.com/teac/r/linear_models.php</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

			</subsubsection>

			<subsubsection xml:id="r-functions-used-in-this-chapter-6">
				<title>R Functions Used in This Chapter </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										abline() plots a best fitting line on top of a scatterplot
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										attach() makes a data structure the "focus" of attention
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										getwd() show the current working directory for R
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										ifelse() a conditional test that provides one of two possible outputs
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										lm() "linear models" and for this chapter, multiple regression
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										plot() general purpose graphing function, many uses in R summary produces an overview of an output structure
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					
				</p>

				<p>
					CHAPTER 17<!-- linebreak -->Hi Ho, Hi Ho<!-- linebreak -->Data Mining We Go<!-- linebreak --><image source='media/image47.png'/><!-- linebreak -->Data Mining is an area of research and practice that is focused on discovering novel patterns in data. As usual, R has lots of possibilities for data mining. In this chapter we will begin experimentation with essential data mining techniques by trying out one of the easiest methods to understand: association rules mining. More beer and diapers please!
				</p>

				<p>
					Data mining is a term that refers to the use of algorithms and computers to discover novel and interesting patterns within data. One famous example that gets mentioned quite frequently is the supermarket that analyzed patterns of purchasing behavior and found that diapers and beer were often purchased together. The supermarket manager decided to put a beer display close to the diaper aisle and supposedly sold more of both products as a result. Another familiar example comes from online merchant sites that say things like, "people who bought that book were also interested in this book." By using an algorithm to look at purchasing patterns, vendors are able to create automatic systems that make these kinds of recommendations.
				</p>

				<p>
					Over recent decades, statisticians and computer scientists have developed many different algorithms that can search for patterns in different kinds of data. As computers get faster and the researchers do additional work on making these algorithms more efficient it becomes possible to look through larger and larger data sets looking for promising patterns. Today we have software that can search through massive data haystacks looking for lots of interesting and usable needles.
				</p>

				<p>
					Some people refer to this area of research as machine learning. Machine learning focuses on creating computer algorithms that can use pre-existing inputs to refine and improve their own capabilities for dealing with future inputs. MAchine learning is very different from human learning. When we think of human learning, like learning the alphabet or learning a foreign language, humans can develop flexible and adaptable skills and knowledge that are applicable to a range of different contexts and problems. Machine learning is more about figuring out patterns of incoming information
				</p>

				<p>
					that correspond to a specific result. For example, given lots of examples like this input: 3, 5, 10; output: 150 a machine learning algorithm could figure out on its own that multiplying the input values together produces the output value.
				</p>

				<p>
					Machine learning is not exactly the same thing as data mining and vice versa. Not all data mining techniques rely on what researchers would consider machine learning. Likewise, machine learning is used in areas like robotics that we don’t commonly think of when we are thinking of data mining as such.
				</p>

				<p>
					Data mining typically consists of four processes: 1) data preparation, 2) exploratory data analysis, 3) model development, and 4) interpretation of results. Although this sounds like a neat, linear set of steps, there is often a lot of back and forth through these processes, and especially among the first three. The other point that is interesting about these four steps is that Steps 3 and 4 seem like the most fun, but Step 1 usually takes the most amount of time. Step 1 involves making sure that the data are organized in the right way, that missing data fields are filled in, that inaccurate data are located and repaired or deleted, and that data are "recoded" as necessary to make them amenable to the kind of analysis we have in mind.
				</p>

				<p>
					Step 2 is very similar to activities we have done in prior chapters of this book: getting to know the data using histograms and other visualization tools, and looking for preliminary hints that will guide our model choice. The exploration process also involves figuring out the right values for key parameters. We will see some of that activity in this chapter.
				</p>

				<p>
					Step 3 choosing and developing a model is by far the most complex and most interesting of the activities of a data miner. It is here where you test out a selection of the most appropriate data mining techniques. Depending upon the structure of a dataset, there may be dozens of options, and choosing the most promising one has as much art in it as science.
				</p>

				<p>
					For the current chapter we are going to focus on just one data mining technique, albeit one that is quite powerful and applicable to a range of very practical problems. So we will not really have to do Step 3, because we will not have two or more different mining techniques to compare. The technique we will use in this chapter is called "association rules mining" and it is the strategy that was used to find the diapers and beer association described earlier.
				</p>

				<p>
					Step 4 the interpretation of results focuses on making sense out of what the data mining algorithm has produced. This is the most important step from the perspective of the data user, because this is where an actionable conclusion is formed. When we discussed the example of beer and diapers, the interpretation of the association rules that were derived from the grocery purchasing data is what led to the discover of the beer-diapers rule and the use of that rule in reconfiguring the displays in the store.
				</p>

				<p>
					Let’s begin by talking a little bit about association rules. Take a look at the figure below with all of the boxes and arrows:
				</p>

				<figure>
	<image source="media/image44.png"/>
					<caption>Map of customers to products</caption>
</figure>

				<p>
					From the figure you can see that each supermarket customer has a grocery cart that contains several items from the larger set of items that the grocery store stocks. The association rules algorithm (also sometimes called affinity analysis) tries out many different propositions, such as "if diapers are purchased, then beer is also purchased." The algorithm uses a dataset of transactions (in the example above, these are the individual carts) to evaluate a long list of these rules for a value called "support." Support is the proportion of times that a particular pairing occurs across all shopping carts. The algorithm also evaluates another quantity called "confidence," which is how frequently a particular pair occurs among all the times when the first item is present. If you look back at the figure again, we had support of 0.67 (the diapers-beer association occurred in two out of the three carts) and confidence of 1.0 ("beer" occurred 100% of the time with "diapers"). In practice, both support and confidence are generally much lower than in this example, but even a rule with low support and smallish confidence might reveal purchasing patterns that grocery store managers could use to guide pricing, coupon offers, or advertising strategies.
				</p>

				<p>
					We can get started with association rules mining very easily using the R package known as "arules." In R-Studio, you can get the arules package ready using the following commands:
				</p>

				<p>
					&gt; install.packages("arules")
				</p>

				<p>
					&gt; library("arules")
				</p>

				<p>
					We will begin our exploration of association rules mining using a dataset that is built into the arules package. For the sake of familiarity, we will use the Groceries dataset. Note that by using the Groceries data set, we have relieved ourselves of the burden of data preparation, as the authors of the arules package have generously made sure that Groceries is ready to be analyzed. So we are skipping right to Step 2 in our four step process exploratory data analysis. You can make the Groceries data set ready with this command:
				</p>

				<p>
					&gt; data(Groceries)
				</p>

				<p>
					Next, lets run the summary() function on Groceries so that we can see what is in there:
				</p>

				<p>
					&gt; summary(Groceries)
				</p>

				<p>
					transactions as itemMatrix in sparse format with
				</p>

				<p>
					9835 rows (elements/itemsets/transactions) and
				</p>

				<p>
					169 columns (items) and a density of 0.02609146
				</p>

				<p>
					most frequent items:
				</p>

				<p>
					whole milk other vegetables rolls/buns soda
				</p>

				<p>
					2513 1903 1809 1715
				</p>

				<p>
					yogurt (Other)
				</p>

				<p>
					1372 34055
				</p>

				<p>
					element (itemset/transaction) length distribution:
				</p>

				<p>
					sizes
				</p>

				<p>
					1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
				</p>

				<p>
					2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 46
				</p>

				<p>
					17 18 19 20 21 22 23 24 26 27 28 29 32
				</p>

				<p>
					29 14 14 9 11 4 6 1 1 1 1 3 1
				</p>

				<p>
					Min. 1st Qu. Median Mean 3rd Qu. Max.
				</p>

				<p>
					1.000 2.000 3.000 4.409 6.000 32.000 includes extended item information examples:
				</p>

				<p>
					labels level2 level1
				</p>

				<p>
					1 frankfurter sausage meet and sausage
				</p>

				<p>
					2 sausage sausage meet and sausage
				</p>

				<p>
					3 liver loaf sausage meet and sausage
				</p>

				<p>
					Right after the summary command line we see that Groceries is an itemMatrix object in sparse format. So what we have is a nice, rectangular data structure with 9835 rows and 169 columns, where each row is a list of items that might appear in a grocery cart. The word "matrix" in this case, is just referring to this rectangular data structure. The columns are the individual items. A little later in the output we see that there are 169 columns, which means that there are 169 items. The reason the matrix is called "sparse" is that very few of these items exist in any given grocery basket. By the way, when an item appears in a basket, its cell contains a one, while if an item is not in a basket, its cell contains a zero. So in any given row, most of the cells are zero and very few are one and this is what is meant by sparse. We can see from the Min, Median, Mean, and Max output that every cart has at least one item, half the carts have more than three items, the average number of items in a cart is 4.4 and the maximum number of items in a cart is 32.
				</p>

				<p>
					The output also shows us which items occur in grocery baskets most frequently. If you like working with spreadsheets, you could imagine going to the very bottom of the column that is marked "whole milk" and putting in a formula to sum up all of the ones in that column. You would come up with 2513, indicating that there
				</p>

				<p>
					are 2513 grocery baskets that contain whole milk. Remember that every row/basket that has a one in the whole milk column has whole milk in that basket, whereas a zero would appear if the basket did not contain whole milk. You might wonder what the data field would look like if a grocery cart contained two gallons of whole milk. For the present data mining exercise we can ignore that problem by assuming that any non-zero amount of whole milk is represented by a one. Other data mining techniques could take advantage of knowing the exact amount of a product, but association rules does not need to know that amount just whether the product is present or absent.
				</p>

				<p>
					Another way of inspecting our sparse matrix is with the itemFrequencyPlot() function. This produces a bar graph that is similar in concept to a histogram: it shows the relative frequency of occurrence of different items in the matrix. When using the itemFrequencyPlot() function, you must specify the minimum level of "support" needed to include an item in the plot. Remember the mention of support earlier in the chapter in this case it simply refers to the relative frequency of occurrence of something. We can make a guess as to what level of support to choose based on the results of the summary() function we ran earlier in the chapter. For example, the item "yogurt" appeared in 1372 out of 9835 rows or about 14% of cases. So we can set the support parameter to somewhere around 10%-15% in order to get a manageable number of items:
				</p>

				<p>
					&gt; itemFrequencyPlot(Groceries,support=0.1)
				</p>

				<p>
					This command produces the following plot:<!-- linebreak --><image source='media/image6.png'/>
				</p>

				<p>
					We can see that yogurt is right around 14% as expected and we also see a few other items not mentioned in the summary such as bottled water and tropical fruit.
				</p>

				<p>
					You should experiment with using different levels of support, just so that you can get a sense of the other common items in the data set. If you show more than about ten items, you will find that the labels on the X-axis start to overlap and obscure one another. Use the "cex.names" parameter to turn down the font size on the labels.
				</p>

				<p>
					This will keep the labels from overlapping at the expense of making the font size much smaller. Here’s an example:
				</p>

				<p>
					&gt; itemFrequencyPlot(Groceries,support=0.05,cex.names=0.5)
				</p>

				<p>
					This command yields about 25 items on the X-axis. Without worrying too much about the labels, you can also experiment with lower values of support, just to get a feel for how many items appear at the lower frequencies. We need to guess at a minimum level of support that will give us quite a substantial number of items that can potentially be part of a rule. Nonetheless, it should also be obvious that an item that occurs only very rarely in the grocery baskets is unlikely to be of much use to us in terms of creating meaningful rules. Let’s pretend, for example, that the item "Venezuelan Beaver Cheese" only occurred once in the whole set of 9835 carts. Even if we did end up with a rule about this item, it won’t apply very often, and is therefore unlikely to be useful to store managers or others. So we want to focus our attention on items that occur with some meaningful frequency in the dataset. Whether this is one percent or half a percent, or something somewhat larger or smaller will depend on the size of the data set and the intended application of the rules.
				</p>

				<p>
					Now we can prepare to generate some rules with the apriori() command. The term "apriori" refers to the specific algorithm that R will use to scan the data set for appropriate rules. Apriori is a very commonly used algorithm and it is quite efficient at finding rules in transaction data. Rules are in the form of "if LHS then RHS." The acronym LHS means "left hand side" and, naturally, RHS means "right hand side." So each rule states that when the thing or things on the left hand side of the equation occur(s), the thing on the right hand side occurs a certain percentage of the time. To reiterate a definition provided earlier in the chapter, support for a rule refers to the frequency of co-occurrence of both members of the pair, i.e., LHS and RHS together. The confidence of a rule refers to the proportion of the time that LHS and RHS occur together versus the total number of appearances of LHS. For example, if Milk and Butter occur together in 10% of the grocery carts (that is "support"), and Milk (by itself, ignoring Butter) occurs in 25% of the carts, then the confidence of the Milk/Butter rule is 0.10/0.25 = 0.40.
				</p>

				<p>
					There are a couple of other measures that can help us zero in on good association rules such as "lift"and "conviction" but we will put off discussing these until a little later.
				</p>

				<p>
					One last note before we start using apriori(): For most of the work the data miners do with association rules, the RHS part of the equation contains just one item, like "Butter." On the other hand, LHS can and will contain multiple items. A simple rule might just have Milk in LHS and Butter in RHS, but a more complex rule might have Milk and Bread together in LHS with Butter in RHS.
				</p>

				<p>
					In the spirit of experimentation, we can try out some different parameter values for using the apriori() command, just to see what we will get:
				</p>

				<p>
					&gt; apriori(Groceries,parameter=list(support=0.005,+ confidence=0.5))
				</p>

				<p>
					parameter specification:
				</p>

				<p>
					confidence minval smax arem aval
				</p>

				<p>
					0.5 0.1 1 none FALSE
				</p>

				<p>
					originalSupport support minlen maxlen target
				</p>

				<p>
					TRUE 0.005 1 10 rules
				</p>

				<p>
					ext <sub>FALSE</sub>
				</p>

				<p>
					algorithmic control:
				</p>

				<p>
					filter tree heap memopt load sort verbose
				</p>

				<p>
					0.1 TRUE TRUE FALSE TRUE 2 TRUE
				</p>

				<p>
					apriori find association rules with the apriori algorithm
				</p>

				<p>
					version 4.21 (2004.05.09) (c) 1996-2004 Christian Borgelt
				</p>

				<p>
					set item appearances ...[0 item(s)] done [0.00s].
				</p>

				<p>
					set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].
				</p>

				<p>
					sorting and recoding items ... [120 item(s)] done [0.00s].
				</p>

				<p>
					creating transaction tree ... done [0.01s].
				</p>

				<p>
					checking subsets of size 1 2 3 4 done [0.01s].
				</p>

				<p>
					writing ... [120 rule(s)] done [0.00s].
				</p>

				<p>
					creating S4 object ... done [0.00s].
				</p>

				<p>
					set of 120 rules
				</p>

				<p>
					We set up the apriori() command to use a support of 0.005 (half a percent) and confidence of 0.5 (50%) as the minimums. These values are confirmed in the first few lines of output. Some other confirmations, such as the value of "minval" and "smax" are not relevant to us right now they have sensible defaults provided by the apriori() implementation. The "minlen" and "maxlen" parameters also have sensible defaults: these refer to the minimum and maximum length of item set that will be considered in generating rules. Obviously you can’t generate a rule unless you have at least one item in an item set, and setting maxlen to 10 ensures that we will not have any rules that contain more than 10 items. If you recall from earlier in the chapter, the average cart only has 4.4 items, so we are not likely to produce rules involving more than 10 items.
				</p>

				<p>
					In fact, a little later in the apriori() output above, we see that the apriori() algorithm only had to examine "subsets of size" one, two three, and four. Apparently no rule in this output contains more than four items. At the very end of the output we see that 120 rules were generated. Later on we will examine ways of making sense out of a large number of rules, but for now let’s agree that 120 is too many rules to examine. Let’s move our support to one percent and rerun apriori(). This time we will store the resulting rules in a data structure called ruleset:
				</p>

				<p>
					&gt; ruleset &lt;apriori(Groceries,+ parameter=list(support=0.01,confidence=0.5))
				</p>

				<p>
					If you examine the output from this command, you should find that we have slimmed down to 15 rules, quite a manageable number to examine one by one. We can get a preliminary look at the rules using the summary function, like this:
				</p>

				<p>
					&gt; summary(ruleset)
				</p>

				<p>
					set of 15 rules
				</p>

				<p>
					rule length distribution (lhs + rhs):sizes
				</p>

				<p>
					3 <sub>15</sub>
				</p>

				<p>
					Min. 1st Qu. Median Mean 3rd Qu. Max.
				</p>

				<p>
					3 3 3 3 3 3
				</p>

				<p>
					summary of quality measures:
				</p>

				<p>
					support confidence lift
				</p>

				<p>
					Min. :0.01007 Min. :0.5000 Min. :1.984
				</p>

				<p>
					1st Qu.:0.01174 1st Qu.:0.5151 1st Qu.:2.036
				</p>

				<p>
					Median :0.01230 Median :0.5245 Median :2.203
				</p>

				<p>
					Mean :0.01316 Mean :0.5411 Mean :2.299
				</p>

				<p>
					3rd Qu.:0.01403 3rd Qu.:0.5718 3rd Qu.:2.432
				</p>

				<p>
					mining info:
				</p>

				<p>
					data transactions support confidence
				</p>

				<p>
					Groceries 9835 0.01 0.5
				</p>

				<p>
					Max. :0.02227 Max. :0.5862 Max. :3.030
				</p>

				<p>
					Looking through this output, we can see that there are 15 rules in total. Under "rule length distribution" it shows that all 15 of the rules have exactly three elements (counting both the LHS and the RHS). Then, under "summary of quality measures," we have an overview of the distributions of support, confidence, and a new parameter called "lift."
				</p>

				<p>
					Researchers have done a lot of work trying to come up with ways of measuring how "interesting" a rule is. A more interesting rule may be a more useful rule because it is more novel or unexpected. Lift is one such measure. Without getting into the math, lift takes into account the support for a rule, but also gives more weight to rules where the LHS and/or the RHS occur less frequently. In other words, lift favors situations where LHS and RHS are not abundant but where the relatively few occurrences always happen together. The larger the value of lift, the more "interesting" the rule may be.
				</p>

				<p>
					Now we are ready to take a closer look at the rules we generated. The inspect() command gives us the detailed contents of the dta object generated by apriori():
				</p>

				<p>
					&gt; inspect(ruleset)
				</p>

				<p>
					lhs rhs support confidence lift
				</p>

				<p>
					1 {curd,
				</p>

				<p>
					yogurt} =&gt; {whole milk} 0.01006609 0.5823529 2.279125
				</p>

				<p>
					2 {other vegetables,
				</p>

				<p>
					butter} =&gt; {whole milk} 0.01148958 0.5736041 2.244885
				</p>

				<p>
					3 {other vegetables,
				</p>

				<p>
					domestic eggs} =&gt; {whole milk} 0.01230300 0.5525114 2.162336
				</p>

				<p>
					4 {yogurt,
				</p>

				<p>
					whipped/sour cream} =&gt; {whole milk} 0.01087951 0.5245098 2.052747
				</p>

				<p>
					5 {other vegetables,
				</p>

				<p>
					whipped/sour cream} =&gt; {whole milk} 0.01464159 0.5070423 1.984385
				</p>

				<p>
					6 {pip fruit,
				</p>

				<p>
					other vegetables} =&gt; {whole milk} 0.01352313 0.5175097 2.025351
				</p>

				<p>
					7 {citrus fruit,
				</p>

				<p>
					root vegetables} =&gt; {other vegetables} 0.01037112 0.5862069 3.029608
				</p>

				<p>
					8 {tropical fruit,
				</p>

				<p>
					root vegetables} =&gt; {other vegetables} 0.01230300 0.5845411 3.020999
				</p>

				<p>
					9 {tropical fruit,
				</p>

				<p>
					root vegetables} =&gt; {whole milk} 0.01199797 0.5700483 2.230969
				</p>

				<p>
					10 {tropical fruit,
				</p>

				<p>
					yogurt} =&gt; {whole milk} 0.01514997 0.5173611 2.024770
				</p>

				<p>
					11 {root vegetables,
				</p>

				<p>
					yogurt} =&gt; {other vegetables} 0.01291307 0.5000000 2.584078
				</p>

				<p>
					12 {root vegetables,
				</p>

				<p>
					yogurt} =&gt; {whole milk} 0.01453991 0.5629921 2.203354
				</p>

				<p>
					13 {root vegetables,
				</p>

				<p>
					rolls/buns} =&gt; {other vegetables} 0.01220132 0.5020921 2.594890
				</p>

				<p>
					14 {root vegetables,
				</p>

				<p>
					rolls/buns} =&gt; {whole milk} 0.01270971 0.5230126 2.046888
				</p>

				<p>
					15 {other vegetables,
				</p>

				<p>
					yogurt} =&gt; {whole milk} 0.02226741 0.5128806 2.007235
				</p>

				<p>
					With apologies for the tiny font size, you can see that each of the 15 rules shows the LHS, the RHS, the support, the confidence, and the lift. Rules 7 and 8 have the highest level of lift: the fruits and vegetables involved in these two rules have a relatively low frequency of occurrence, but their support and confidence are both relatively high. Contrast these two rules with Rule 1, which also has high confidence, but which has low support. The reason for this is that milk is a frequently occurring item, so there is not much novelty to that rule. On the other hand, the combination of fruits, root vegetables, and other vegetables suggest a need to find out more about customers whose carts may contain only vegetarian or vegan items.
				</p>

				<p>
					Now it is possible that we have set our parameters for confidence and support too stringently and as a result have missed some truly novel combinations that might lead us to better insights. We can use a data visualization package to help explore this possibility. The R package called arulesViz has methods of visualizing the rule sets generated by apriori() that can help us examine a larger set of rules. First, install and library the arulesViz package:
				</p>

				<p>
					&gt; install.packages("arulesViz")
				</p>

				<p>
					&gt; library(arulesViz)
				</p>

				<p>
					These commands will give the usual raft of status and progress messages. When you run the second command you may find that three or four data objects are "masked." As before, these warnings generally will not compromise the operation of the package.
				</p>

				<p>
					Now lets return to our apriori() command, but we will be much more lenient this time in our minimum support and confidence parameters:
				</p>

				<p>
					&gt; ruleset &lt;apriori(Groceries,+ parameter=list(support=0.005,confidence=0.35))
				</p>

				<p>
					We brought support back to half of one percent and confidence down to 35%. When you run this command you should find that you now generate 357 rules. That is way too many rules to examine manually, so let’s use the arulesViz package to see what we have. We will use the plot() command that we have also used earlier in the book. You may ask yourself why we needed to library the arulesViz package if we are simply going to use an old command. The answer to this conundrum is that arulesViz has put some plumbing into place so that when plot runs across a data object of type "rules" (as generated by apriori) it will use some of the code that is built into arulesViz to do the work. So by installing arulesViz we have put some custom visualization code in place that can be used by the generic plot() command. The command is very simple:
				</p>

				<p>
					&gt; plot(ruleset)
				</p>

				<p>
					The figure below contains the result:
				</p>

				<figure>
	<image source="media/image11.png"/>
					<caption>Scatterplot for 357 rules</caption>
</figure>

				<p>
					Even though we see a two dimensional plot, we actually have three variables represented here. Support is on the X-axis and confidence is on the Y-axis. All else being equal we would like to have rules that have high support and high confidence. We know, however, that lift serves as a measure of interestingness, and we are also interested in the rules with the highest lift. On this plot, the lift is shown by the darkness of a dot that appears on the plot. The darker the dot, the close the lift of that rule is to 4.0, which appears to be the highest lift value among these 357 rules.
				</p>

				<p>
					The other thing we can see from this plot is that while the support of rules ranges from somewhere below 1% all the way up above 7%, all of the rules with high lift seem to have support below 1%. On the other hand, there are rules with high lift and high confidence, which sounds quite positive.
				</p>

				<p>
					Based on this evidence, lets focus on a smaller set of rules that only have the very highest levels of lift. The following command makes a subset of the larger set of rules by choosing only those rules that have lift higher than 3.5:
				</p>

				<p>
					&gt; goodrules &lt;ruleset[quality(ruleset)$lift &gt; 3.5]
				</p>

				<p>
					Note that the use of the square braces with our data structure ruleset allows us to index only those elements of the data object that meet our criteria. In this case, we use the expression quality(ruleset)$lift to tap into the lift parameter for each rule. The inequality test &gt; 3.5 gives us just those rules with the highest lift. When you run this line of code you should find that goodrules contains just nine rules. Let’s inspect those nine rules:
				</p>

				<p>
					&gt; inspect(goodrules)
				</p>

				<p>
					lhs rhs support confidence lift
				</p>

				<p>
					1 {herbs} =&gt; {root vegetables} 0.007015760 0.4312500 3.956477
				</p>

				<p>
					2 {onions,
				</p>

				<p>
					other vegetables} =&gt; {root vegetables} 0.005693950 0.4000000 3.669776
				</p>

				<p>
					3 {beef,
				</p>

				<p>
					other vegetables} =&gt; {root vegetables} 0.007930859 0.4020619 3.688692
				</p>

				<p>
					4 {tropical fruit,
				</p>

				<p>
					curd} =&gt; {yogurt} 0.005287239 0.5148515 3.690645
				</p>

				<p>
					5 {citrus fruit,
				</p>

				<p>
					pip fruit} =&gt; {tropical fruit} 0.005592272 0.4044118 3.854060
				</p>

				<p>
					6 {pip fruit,
				</p>

				<p>
					other vegetables,
				</p>

				<p>
					whole milk} =&gt; {root vegetables} 0.005490595 0.4060150 3.724961
				</p>

				<p>
					7 {citrus fruit,
				</p>

				<p>
					other vegetables,
				</p>

				<p>
					whole milk} =&gt; {root vegetables} 0.005795628 0.4453125 4.085493
				</p>

				<p>
					8 {root vegetables,
				</p>

				<p>
					whole milk,
				</p>

				<p>
					yogurt} =&gt; {tropical fruit} 0.005693950 0.3916084 3.732043
				</p>

				<p>
					9 {tropical fruit,
				</p>

				<p>
					other vegetables,
				</p>

				<p>
					whole milk} =&gt; {root vegetables} 0.007015760 0.4107143 3.768074
				</p>

				<p>
					There we go again with the microscopic font size. When you look over these rules, it seems evidence that shoppers are purchasing particular combinations of items that go together in recipes. The first three rules really seem like soup! Rules four and five seem like a fruit platter with dip. The other four rules may also connect to a recipe, although it is not quite so obvious what.
				</p>

				<p>
					The key takeaway point here is that using a good visualization tool to examine the results of a data mining activity can enhance the process of sorting through the evidence and making sense of it. If we were to present these results to a store manager (and we would certainly do a little more digging before formulating our final conclusions) we might recommend that recipes could be published along with coupons and popular recipes, such as for homemade soup, might want to have all of the ingredients group together in the store along with signs saying, "Mmmm, homemade soup!"
				</p>

				<p>
					<term>Chapter Challenge</term>
				</p>

				<p>
					The arules package contains other data sets, such as the Epub dataset with 3975 transactions from the electronic publication platform of the Vienna University of Economics. Load up that data set, generate some rules, visualize the rules, and choose some interesting ones for further discussion.
				</p>

				<p>
					<term>Data Mining with Rattle</term>
				</p>

				<p>
					A company called Togaware has created a graphical user interface (GUI) for R called Rattle. At this writing (working with R version 3.0.0), one of Rattle’s components has gotten out of date and will not work with the latest version of R, particularly on the Mac. It is likely, however, that those involved with the Rattle project will soon update it to be compatible again. Using Rattle simplifies many of the processes described earlier in the chapter. Try going to the Togaware site and following the instructions there for installing Rattle for your particular operating system.
				</p>

			</subsubsection>

			<subsubsection xml:id="sources-12">
				<title>Sources </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Association_rule_learning">http://en.wikipedia.org/wiki/Association_rule_learning</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11">http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11 a.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://journal.r-project.org/archive/2009-2/RJournal_2009-2_Will">http://journal.r-project.org/archive/2009-2/RJournal_2009-2_Will iams.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.r-bloggers.com/examples-and-resources-on-associati">http://www.r-bloggers.com/examples-and-resources-on-associati on-rule-mining-with-r/</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://rattle.togaware.com">http://rattle.togaware.com</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.statsoft.com/textbook/association-rules/">http://www.statsoft.com/textbook/association-rules/</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

			</subsubsection>

			<subsubsection xml:id="reference">
				<title>Reference </title>

				<p>
					Michael Hahsler, Kurt Hornik, and Thomas Reutterer (2006) Implications of probabilistic data modeling for mining association rules. In M. Spiliopoulou, R. Kruse, C. Borgelt, A. Nuernberger, and W. Gaul, editors, From Data and Information Analysis to Knowledge Engineering, Studies in Classification, Data Analysis, and Knowledge Organization, pages 598–605. Springer-Verlag.
				</p>

			</subsubsection>

			<subsubsection xml:id="r-functions-used-in-this-chapter-7">
				<title>R Functions Used in This Chapter </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										apriori() Uses the algorithm of the same name to analyze a transaction data set and generate rules.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										itemFrequencyPlot() Shows the relative frequency of commonly occurring items in the spare occurrence matrix.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										inspect() Shows the contents of the data object generated by apriori() that generates the association rules
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										install.packages() Loads package from the CRAN repository
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										summary() Provides an overview of the contents of a data structure.
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					
				</p>

				<p>
					CHAPTER 18<!-- linebreak -->What’s Your Vector, Victor?<!-- linebreak --><image source='media/image49.png'/>In the previous chapter, we explored an unsupervised learning technique known as association rules mining. In this chapter, we will examine a set of supervised learning approaches known as support vector machines (SVMs). SVMs are flexible algorithms that excel at addressing classification problems.
				</p>

				<p>
					From the previous chapter you may remember that data mining techniques fall into two large categories: supervised learning techniques and unsupervised learning techniques. The association rules mining examined in the previous chapter was an unsupervised technique. This means that there was no particular criterion that we were trying to predict, rather we were just looking for patterns that would emerge from the data naturally.
				</p>

				<p>
					In the present chapter we will examine a supervised learning technique called "support vector machines." Why the technique is called this we will examine shortly. The reason this is considered a supervised learning technique is that we "train" the algorithm on an initial set of data (the "supervised" phase) and then we test it out on a brand new set of data. If the training we accomplished worked well, then the algorithm should be able to predict the right outcome most of the time in the test data.
				</p>

				<p>
					Take the weather as a simple example. Some days are cloudy, some are sunny. The barometer rises some days and fall others. The wind may be strong or weak and it may come from various directions. If we collect data on a bunch of days and use those data to train a machine learning algorithm, the algorithm may find that cloudy days with a falling barometer and the wind from the east may signal that it is likely to rain. Next, we can collect more data on some other days and see how well our algorithm does at predicting rain on those days. The algorithm will make mistakes. The percentage of mistakes is the error rate, and we would like the error rate to be as low as possible.
				</p>

				<p>
					This is the basic strategy of supervised machine learning: Have a substantial number of training cases that the algorithm can use to
				</p>

				<p>
					discover and mimic the underlying pattern and then use the results of that process on a test data set in order to find out how well the algorithm and parameters perform in a "cross validation." Cross validation, in this instance, refers to the process of verifying that the trained algorithm can carry out its prediction or classification task accurately on novel data.
				</p>

				<p>
					In this chapter, we will develop a "support vector machine" (SVM) to classify emails into spam or not spam. An SVM maps a low dimensional problem into a higher dimensional space with the goal of being able to describe geometric boundaries between different regions. The input data (the independent variables) from a given case are processed through a "mapping" algorithm called a kernel (the kernel is simply a formula that is run on each case’s vector of input data), and the resulting kernel output determines the position of that case in multidimensional space.
				</p>

				<p>
					A simple 2D-3D mapping example illustrates how this works: Imagine looking at a photograph of a snow-capped mountain photographed from high above the earth such that the mountain looks like a small, white circle completely surrounded by a region of green trees. Using a pair of scissors, there is no way of cutting the photo on a straight line so that all of the white snow is on one side of the cut and all of the green trees are on the other. In other words there is no simple linear separation function that could correctly separate or classify the white and green points given their 2D position on the photograph.
				</p>

				<p>
					Next, instead of a piece of paper, think about a realistic three dimensional clay model of the mountain. Now all the white points occupy a cone at the peak of the mountain and all of the green points lie at the base of the mountain. Imagine inserting a sheet of cardboard through the clay model in a way that divides the snow capped peak from the green-tree-covered base. It is much easier to do now because the white points are sticking up into the high altitude and the green points are all on the base of the mountain.
				</p>

				<p>
					The position of that piece of cardboard is the planar separation function that divides white points from green points. A support vector machine analysis of this scenario would take the original two dimensional point data and search for a projection into three dimensions that would maximize the spacing between green points and white points. The result of the analysis would be a mathematical description of the position and orientation of the cardboard plane. Given inputs describing a novel data point, the SVM could then map the data into the higher dimensional space and then report whether the point was above the cardboard (a white point) or below the cardboard (a green point). The so called support vectors contain the coefficients that map the input data for each case into the high dimensional space.
				</p>

				<p>
					To get started with support vector machines, we can load one of the R packages that supports this technique. We will use the "kernlab" package. Use the commands below:
				</p>

				<p>
					&gt; install.packages("kernlab")
				</p>

				<p>
					&gt; library(kernlab)
				</p>

				<p>
					I found that it was important to use the double quotes in the first command, but not in the second command. The data set that we want to use is built into this package. The data comes from a study of spam emails received by employees at the Hewlett-Packard company. Load the data with the following command:
				</p>

				<p>
					&gt; data(spam)
				</p>

				<p>
					This command does not produce any output. We can now inspect the "spam" dataset with the str() command:
				</p>

				<p>
					&gt; str(spam)
				</p>

				<p>
					'data.frame': 4601 obs. of 58 variables:
				</p>

				<p>
					$ make : num 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
				</p>

				<p>
					$ address : num 0.64 0.28 0 0 0 0 0 0 0 0.12 ...
				</p>

				<p>
					$ all : num 0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
				</p>

				<p>
					$ num3d : num 0 0 0 0 0 0 0 0 0 0 ...
				</p>

				<p>
					.<sub>.. $ charDollar : num 0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...</sub>
				</p>

				<p>
					$ charHash : num 0 0.048 0.01 0 0 0 0 0 0.022 0 ...
				</p>

				<p>
					$ capitalAve : num 3.76 5.11 9.82 3.54 3.54 ...
				</p>

				<p>
					$ capitalLong : num 61 101 485 40 40 15 4 11 445 43 ...
				</p>

				<p>
					$ capitalTotal : num 278 1028 2259 191 191 ...
				</p>

				<p>
					$ type : Factor w/ 2 levels "nonspam","spam": 2 2 2 2 2 2 2 2 2 2 ...
				</p>

				<p>
					Some of the lines of output have been elided from the material above. You can also use the dim() function to get a quick overview of the data structure:
				</p>

				<p>
					&gt; dim(spam)
				</p>

				<p>
					[1] 4601 58
				</p>

				<p>
					The dim() function shows the "dimensions" of the data structure. The output of this dim() function shows that the spam data structure has 4601 rows and 58 columns. If you inspect a few of the column names that emerged from the str() command, you may see that each email is coded with respect to its contents. There is lots of information available about the data set here:
				</p>

				<p>
					<url href="http://archive.ics.uci.edu/ml/datasets/Spambase">http://archive.ics.uci.edu/ml/datasets/Spambase</url>
				</p>

				<p>
					For example, just before "type" at the end of the output of the str() command on the previous page, we see a variable called "capitalTotal." This is the total number of capital letters in the whole email. Right after that is the criterion variable, "type," that indicates whether an email was classified as spam by human experts. Let’s explore this variable a bit more:
				</p>

				<p>
					&gt; table(spam$type)
				</p>

				<p>
					nonspam spam
				</p>

				<p>
					2788 1813
				</p>

				<p>
					We use the table function because type is a factor rather than a numeric variable. The output shows us that there are 2788 messages that were classified by human experts as not spam, and 1813 messages that were classified as spam. What a great dataset!
				</p>

				<p>
					To make the analysis work we need to divide the dataset into a training set and a test set. There is no universal way to do this, but as a rule of thumb, you can use two thirds of the data set to train and the remainder to test. Let’s first generate a randomized index that will let us choose cases for our training and test sets. In the following command, we create a new list/vector variable that samples at random from a list of numbers ranging from 1 to the final element index of the spam data (4601).
				</p>

				<p>
					&gt; randIndex &lt;sample(1:dim(spam)[1])
				</p>

				<p>
					&gt; summary(randIndex)
				</p>

				<p>
					Min. 1st Qu. Median Mean 3rd Qu. Max.
				</p>

				<p>
					1 1151 2301 2301 3451 4601
				</p>

				<p>
					&gt; length(randIndex)
				</p>

				<p>
					[1] 4601
				</p>

				<p>
					The output of the summary() and length() commands above show that we have successfully created a list of indices ranging from 1 to 4601 and that the total length of our index list is the same as the number of rows in the spam dataset: 4601. We can confirm that the indices are randomized by looking at the first few cases:
				</p>

				<p>
					&gt; head(randIndex)
				</p>

				<p>
					[1] 2073 769 4565 955 3541 3357
				</p>

				<p>
					It is important to randomize your selection of cases for the training and test sets in order to ensure that there is no systematic bias in the selection of cases. We have no way of knowing how the original dataset was sorted (if at all) in case it was sorted on some variable of interest we do not just want to take the first 2/3rds of the cases as the training set.
				</p>

				<p>
					Next, let’s calculate the "cut point" that would divide the spam dataset into a two thirds training set and a one third test set:
				</p>

				<p>
					&gt; cutPoint2_3 &lt;floor(2 * dim(spam)[1]/3)
				</p>

				<p>
					&gt; cutPoint2_3
				</p>

				<p>
					[1] 3067
				</p>

				<p>
					The first command in this group calculates the two-thirds cut point based on the number of rows in spam (the expression dim(spam)[1] gives the number of rows in the spam dataset). The second command reveals that that cut point is 3067 rows into the data set, which seems very sensible given that there are 4601 rows in total. Note that the floor() function chops off any decimal part of the calculation. We want to get rid of any decimal because an index variable needs to be an integer.
				</p>

				<p>
					Now we are ready to generate our test and training sets from the original spam dataset. First we will build our training set from the first 3067 rows:
				</p>

				<p>
					&gt; trainData &lt;spam[randIndex[1:cutPoint2_3],]
				</p>

				<p>
					We make the new data set, called trainData, using the randomized set of indices that we created in the randIndex list, but only using the first 3067 elements of randIndex (The inner expression in square brackets, 1:cutPoint2_3, does the job of selecting the first 3067 elements. From here you should be able to imagine the command for creating the test set:
				</p>

				<p>
					&gt; testData &lt;spam[randIndex[(cutPoint2_3+1):dim(spam)[1]],]
				</p>

				<p>
					The inner expression now selects the rows from 3068 all the way up to 4601 for a total of 1534 rows. So now we have two separate data sets, representing a two-thirds training and one third test breakdown of the original data. We are now in good shape to train our support vector model. The following command generates a model based on the training data set:
				</p>

				<p>
					&gt; svmOutput &lt;ksvm(type ~ ., data=trainData, kernel="rbfdot",kpar="automatic",C=5,cross=3, prob.model=TRUE)
				</p>

				<p>
					Using automatic sigma estimation (sigest) for RBF or laplace kernel
				</p>

				<p>
					Let’s examine this command in some detail. The first argument, "type ~ .", specifies the model we want to test. Using the word "type" in this expression means that we want to have the "type" variable (i.e., whether the message is spam or non-spam) as the outcome variable that our model predicts. The tilde character ("~") in an R expression simply separates the left hand side of the expression from the right hand side. Finally, the dot character (".") is a shorthand that tell R to us all of the other variables in the dataframe to try to predict "type."
				</p>

				<p>
					The "data" parameter let’s us specify which dataframe to use in the analysis, In this case, we have specified that the procedure should use the trainData training set that we developed.
				</p>

				<p>
					The next parameter is an interesting one: kernel="rbfdot". You will remember from the earlier discussion that the kernel is the customizable part of the SVM algorithm that lets us project the low dimensional problem into higher dimensional space. In this case, the rbfdot designation refers to the "radial basis function." One simple way of thinking about the radial basis function is that if you think of a point on a regular x,y coordinate system the distance from the origin to the point is like a radius of a circle. The "dot" in the name refers to the mathematical idea of a "dot product," which is a way of multiplying vectors together to come up with a single number such as a distance value. In simplified terms, the radial basis function kernel takes the set of inputs from each row in a dataset and calculates a distance value based on the combination of the many variables in the row. The weighting of the different variables in the row is adjusted by the algorithm in order to get the maximum separation of distance between the spam cases and the non-spam cases.
				</p>

				<p>
					The "kpar" argument refers to a variety of parameters that can be used to control the operation of the radial basis function kernel. In this case we are depending on the good will of the designers of this algorithm by specifying "automatic." The designers came up with some "heuristics" (guesses) to establish the appropriate parameters without user intervention.
				</p>

				<p>
					The C argument refers to the so called "cost of constraints." Remember back to our example of the the white top on the green mountain? When we put the piece of cardboard (the planar separation function) through the mountain, what if we happen to get one green point on the white side or one white point on the green side? This is a kind of mistake that influences how the algorithm places the piece of cardboard. We can force these mistakes to be more or less "costly," and thus to have more influence on the position of our piece of cardboard and the margin of separation that it defines. We can get a large margin of separation but possibly with a few mistakes by specifying a small value of C. If we specify a large value of C we may possibly get fewer mistakes, but on at the cost of having the cardboard cut a very close margin between the green and white points the cardboard might get stuck into the mountain at a very weird angle just to make sure that all of the green points and white points are separated. On the other hand if we have a low value of C we will get a generalizable model, but one that makes more classification mistakes.
				</p>

				<p>
					In the next argument, we have specified "cross=3." Cross refers to the cross validation model that the algorithm uses. In this case, our choice of the final parameter, "prob.model=TRUE," dictates that we use a so called three-fold cross validation in order to generate the probabilities associate with whether a message is or isn’t a spam message. Cross validation is important for avoiding the problem of overfitting. In theory, many of the algorithms used in data mining can be pushed to the point where they essentially memorize the input data and can perfectly replicate the outcome data in the training set. The only problem with this is that the model base don the memorization of the training data will almost never generalize to other data sets. In effect, if we push the algorithm too hard, it will become too specialized to the training data and we won’t be able to use it anywhere else. By using k-fold (in this case three fold) crossvalidation, we can rein in the fitting function so that it does not work so hard and so that it does creat a model that is more likely to generalize to other data.
				</p>

				<p>
					Let’s have a look at what our output structure contains:
				</p>

				<p>
					&gt; svmOutput
				</p>

				<p>
					Support Vector Machine object of class "ksvm"
				</p>

				<p>
					SV type: C-svc (classification)
				</p>

				<p>
					parameter : cost C = 5
				</p>

				<p>
					Gaussian Radial Basis kernel function.
				</p>

				<p>
					Hyperparameter : sigma = 0.0287825580201687
				</p>

				<p>
					Number of Support Vectors : 953
				</p>

				<p>
					Objective Function Value : -1750.51
				</p>

				<p>
					Training error : 0.027388
				</p>

				<p>
					Cross validation error : 0.076296
				</p>

				<p>
					Probability model included.
				</p>

				<p>
					Most of this is technical detail that will not necessarily affect how we use the SVM output, but a few things are worth pointing out. First, the sigma parameter mentioned was estimated for us by the algorithm because we used the "automatic" option. Thank goodness for that as it would have taken a lot of experimentation to choose a reasonable value without the help of the algorithm. Next, note the large number of support vectors. These are the lists of weights that help to project the variables in each row into a higher dimensional space. The "training error" at about 2.7% is quite low. Naturally, the cross-validation error is higher, as a set of parameters never perform as well on subsequent data sets as they do with the original training set. Even so, a 7.6% cross validation error rate is pretty good for a variety of prediction purposes.
				</p>

				<p>
					We can take a closer look at these support vectors with the following command:
				</p>

				<p>
					&gt; hist(alpha(svmOutput)[[1]])
				</p>

				<p>
					The alpha() accessor reveals the values of the support vectors. Note that these are stored in a nested list, hence the need for the [[1]] expression to access the first list in the list of lists. Because the particular dataset we are using only has two classes (spam or not spam), we only need one set of support vectors. If the "type" criterion variable had more than two levels (e.g., spam, not sure, and not spam), we would need additional support vectors to be able to classify the cases into more than two groups. The histogram output reveals the range of the support vectors from 0 to 5:
				</p>

				<figure>
	<image source="media/image35.png"/>
					<caption>Histogram of alpha(svmOutput)[[1]]</caption>
</figure>

				<p>
					The maximum value of the support vector is equal to the cost parameter that we discussed earlier. We can see that about half of the support vectors are at this maximum value while the rest cluster around zero. Those support vectors at the maximum represent the most difficult cases to classify. WIth respect to our mountain metaphor, these are the white points that are below the piece of cardboard and the green points that are above it.
				</p>

				<p>
					If we increase the cost parameter we can get fewer of these problem points, but at only at the cost of increasing our cross validation error:
				</p>

				<p>
					&gt; svmOutput &lt;ksvm(type ~ ., data=trainData, kernel="rbfdot",kpar="automatic",<term>C=50</term>,cross=3,pro b.model=TRUE)
				</p>

				<p>
					&gt; svmOutput
				</p>

				<p>
					Support Vector Machine object of class "ksvm"
				</p>

				<p>
					SV type: C-svc (classification)
				</p>

				<p>
					parameter : cost C = 50
				</p>

				<p>
					Gaussian Radial Basis kernel function.
				</p>

				<p>
					Hyperparameter : sigma = 0.0299992970259353
				</p>

				<p>
					Number of Support Vectors : 850
				</p>

				<p>
					Objective Function Value : -6894.635
				</p>

				<p>
					Training error : 0.008803<!-- linebreak -->Cross validation error : 0.085424
				</p>

				<p>
					Probability model included.
				</p>

				<p>
					In the first command above, the C=50 is bolded to show what we changed from the earlier command. The output here shows that our training error went way down, to 0.88%, but that our crossvalidation error went up from 7.6% in our earlier model to 8.5% in this model. We can again get a histogram of the support vectors to show what has happened:<!-- linebreak --><image source='media/image8.png'/>
				</p>

				<p>
					Now there are only about 100 cases where the support vector is at the maxed out value (in this case 50, because we set C=50 in the svm command). Again, these are the hard cases that the model could not get to be on the right side of the cardboard (or that were right on the cardboard). Meanwhile, the many cases with the support vector value near zero represent the combinations of parameters that make a case lie very far from the piece of cardboard. These cases were so easy to classify that they really made no contribution to "positioning" the hyperplane that separates the spam cases from the non-spam cases.
				</p>

				<p>
					We can poke our way into this a little more deeply by looking at a couple of instructive cases. First, let’s find the index numbers of a few of the support vectors that were near zero:
				</p>

				<p>
					&gt; alphaindex(svmOutput)[[1]][alpha(svmOutput)[[1]]&lt;0.05]
				</p>

				<p>
					[1] 90 98 289 497 634 745 1055 1479 1530 1544 1646 1830 1948 2520 2754
				</p>

				<p>
					This monster of a command is not as bad as it looks. We are tapping into a new part of the svmOutput object, this time using the alphaindex() accessor function. Remember that we have 850 support vectors in this output. Now imagine two lists of 850 right next to each other: the first is the list of support vectors themselves, we get at that list with the alpha() accessor function. The second list, lined right up next to the first list, is a set of indices into the original training dataset, trainData. The left hand part of the expression in the command above let’s us access these indices. The right hand part of the expression, where it says alpha(svmOutput)[[1]]&lt;0.05, is a conditional expression that let’s us pick from the index list just those cases with a very small support vector value. You can see the output above, just underneat the command: about 15 indices were returned. Just pick off the first one, 90, and take a look at the individual case it refers to:
				</p>

				<p>
					&gt; trainData[90,]
				</p>

				<p>
					make address all num3d our over remove
				</p>

				<p>
					0 0 0 0 0 0 0
				</p>

				<p>
					internet order mail receive will
				</p>

				<p>
					0 0 0 0 0
				</p>

				<p>
					.<sub>..charExclamation charDollar charHash capitalAve</sub>
				</p>

				<p>
					1.123 0 0 2.6
				</p>

				<p>
					capitalLong capitalTotal
				</p>

				<p>
					16 26
				</p>

				<p>
					type
				</p>

				<p>
					nonspam
				</p>

				<p>
					The command requested row 90 from the trainData training set. A few of the lines of the output were left off for ease of reading and almost all of the variables thus left out were zero. Note the very last two lines of the output, where this record is identified as a non-spam email. So this was a very easy case to classify because it has virtually none of the markers that a spam email usually has (for example, as shown above, no mention of internet, order, or mail). You can contrast this case with one of the hard cases by running this command:
				</p>

				<p>
					&gt; alphaindex(svmOutput)[[1]][alpha(svmOutput)[[1]]==50]
				</p>

				<p>
					You will get a list of the 92 indices of cases where the support vector was "maxed out" to the level of the cost function (remember C=50 from the latest run of the svm() command). Pick any of those cases and display it, like this:
				</p>

				<p>
					&gt; trainData[11,]
				</p>

				<p>
					This particular record did not have many suspicious keywords, but it did have long strings of capital letters that made it hard to classify (it was a non-spam case, by the way). You can check out a few of them to see if you can spot why each case may have been difficult for the classifier to place.
				</p>

				<p>
					The real acid test for our support vector model, however, will be to use the support vectors we generated through this training process to predict the outcomes in a novel data set. Fortunately, because we prepared carefully, we have the testData training set ready to go. The following commands with give us some output known as a "confusion matrix:"
				</p>

				<p>
					&gt; svmPred &lt;predict(svmOutput, testData, type="votes")
				</p>

				<p>
					&gt; compTable &lt;data.frame(testData[,58],svmPred[1,])
				</p>

				<p>
					&gt; table(compTable)
				</p>

				<p>
					svmPred.1...
				</p>

				<p>
					testData...58. 0 1
				</p>

				<p>
					nonspam 38 854
				</p>

				<p>
					spam 574 68
				</p>

				<p>
					The first command in the block above uses our model output from before, namely svmOutput, as the parameters for prediction. It uses the "testData," which the support vectors have never seen before, to generate predictions, and it requests "votes" from the prediction process. We could also look at probabilities and other types of model output, but for a simple analysis of whether the svm is generating good predictions, votes will make our lives easier.
				</p>

				<p>
					The output from the predict() command is a two dimensional list. You should use the str() command to examine its structure. basically there are two lists of "vote" values side by side. Both lists are 1534 elements long, corresponding to the 1534 cases in our testData object. The lefthand list has one for a non-spam vote and zero for a spam vote. Because this is a two-class problem, the other list has just the opposite. We can use either one because they are mirror images of each other.
				</p>

				<p>
					In the second command above, we make a little dataframe, called compTable, with two variables in it: The first variable is the 58th column in the test data, which is the last column containing the "type" variable (a factor indicating spam or non-spam). Remember that this type variable is the human judgments from the original dataset , so it is our ground truth. The second variable is the first column in our votes data structure (svmPred), so it contains ones for non-spam predictions and zeros for spam predictions.
				</p>

				<p>
					Finally, applying the table() command to our new dataframe (compTable) gives us the confusion matrix as output. Along the main diagonal we see the erroneous classifications 38 cases that were not spam, but were classified as spam by the support vector matrix and 68 cases that were spam, but were classified as non spam by the support vector matrix. On the counter-diagonal, we see 854 cases that were correctly classified as non-spam and 574 cases that were correctly classified as spam.
				</p>

				<p>
					Overall, it looks like we did a pretty good job. There are a bunch of different ways of calculating the accuracy of the prediction, depending upon what you are most interested in. The simplest way is to sum the 68 + 38 = 106 error cases and divided by the 1534 total cases for an total error rate of about 6.9%. Interestingly, that is a tad better than the 8.5% error rate we got from the k-fold crossvalidation in the run of svm() that created the model we are testing. Keep in mind, though, that we may be more interested in certain kinds of error than other kinds. For example, consider which is worse, an email that gets mistakenly quarantined because it is not really spam, or a spam email that gets through to someone’s inbox? It really depends on the situation, but you can see that you might want to give more consideration to either the 68 misclassification errors or the other set of 38 misclassification errors.
				</p>

			</subsubsection>

			<subsubsection xml:id="chapter-challenge-5">
				<title>Chapter Challenge </title>

				<p>
					Look up the term "confusion matrix" and then follow-up on some other terms such as Type I error, Type II error, sensitivity, and specificity. Think about how the support vector machine model could be modified to do better at either sensitivity or specificity.
				</p>

				<p>
					For a super challenge, try using another dataset with the kernlab svm technology. There is a dataset called promotergene that is built into the kernlab package. You could also load up your own data set and try creating an svm model from that.
				</p>

			</subsubsection>

			<subsubsection xml:id="sources-13">
				<title>Sources </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/svmbasic/sv">http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/svmbasic/sv mbasic_notes.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://cran.r-project.org/web/packages/kernlab/kernlab.pdf">http://cran.r-project.org/web/packages/kernlab/kernlab.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Confusion_matrix">http://en.wikipedia.org/wiki/Confusion_matrix</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://stackoverflow.com/questions/9480605/what-is-the-relatio">http://stackoverflow.com/questions/9480605/what-is-the-relatio n-between-the-number-of-support-vectors-and-training-data-and</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.louisaslett.com/Courses/Data_Mining/ST4003-Lab7">http://www.louisaslett.com/Courses/Data_Mining/ST4003-Lab7 -Introduction_to_Support_Vector_Machines.pdf</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.jstatsoft.org/v11/i09/paper">http://www.jstatsoft.org/v11/i09/paper</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					APPENDIX A<!-- linebreak -->Installing R and R-Studio
				</p>

				<p>
					As mentioned in previous chapters, R is an open source program, meaning that the source code that is used to create a copy of R to run on a Mac, Windows, or Linux computer is available for all to inspect and modify. If your computer has the Windows®, Mac-OSX® or a Linux operating system, there is a version of R waiting for you at <url href="http://cran.r-project.org">http://cran.r-project.org</url>/. Download and install your own copy. If you sometimes have difficulties with installing new software and you need some help, there is a wonderful little book by Thomas P. Hogan called, <em>Bare Bones R: A Brief Introductory Guide</em> that you might want to buy or borrow from your library. There are lots of sites online that also give help with installing R, although many of them are not oriented towards the inexperienced user. I searched online using the term "help installing R" and I got a few good hits. One site that was quite informative for installing R on Windows was at "readthedocs.org," and you can try to access it at this TinyUrl: <url href="http://tinyurl.com/872ngt">http://tinyurl.com/872ngt</url>t. For Mac users there is a video by Jeremy Taylor at Vimeo.com, <url href="http://vimeo.com/36697971">http://vimeo.com/36697971</url>, that outlines both the initial installation on a Mac and a number of other optional steps for getting started. YouTube also has videos that provide brief tutorials for installing R. Try searching for "install R" in the YouTube search box. The rest of this chapter assumes that you have installed R and can run it on your computer as shown in the screenshot above. (Note that this screenshot is from the Mac version of R: if you are running Windows or Linux your R screen may appear slightly different from this.) Just for fun, one of the first things you can do when you have R running is to click on the color wheel and customize the appearance of R. This screenshot uses Syracuse orange as a background color. The screenshot also shows a simple command to type that shows the most basic method of interaction with R.
				</p>

				<p>
					Perhaps the most challenging aspect of installing R-Studio is having to install R first, but if you’ve already done that, then R-Studio should be a piece of cake. Make sure that you have the latest version of R installed before you begin with the installation of R-studio. There is ample documentation on the R-studio website, <url href="http://www.rstudio.org/">http://www.rstudio.org/</url>, so if you follow the instructions there, you should have minimal difficulty. If you reach a page where you are asked to choose between installing Rstudio server and installing R-studio as a desktop application on your computer, choose the latter. For now you want the desktop/single user version. If you run into any difficulties or you just want some additional guidance about R-studio, you may want to have a look at the book entitled, <em>Getting Started with R-studio</em>, by John Verzani (2011, Sebastopol, CA: O’Reilly Media). The first chapter of that book has a general orientation to R and R-studio as well as a guide to installing and updating R-studio. There is also a YouTube video that introduces R-studio here: <url href="http://www.youtube.com/watch?v=7sAmqkZ3Be8">http://www.youtube.com/watch?v=7sAmqkZ3Be8</url> Be aware if you search for other YouTube videos that there is a disk recovery program as well a music group that share the R-Studio name: You will get a number of these videos if you search on "R-Studio" without any other search terms.
				</p>
			</subsubsection>

		</subsection>

	</subsubsection>



</article>
</pretext>
